{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265eac91",
   "metadata": {},
   "source": [
    "# Agent-OM è®­ç»ƒæ•°æ®ç”Ÿæˆ - Gemini API æµ‹è¯•\n",
    "\n",
    "æœ¬ Notebook è®°å½•äº†ä½¿ç”¨ Google Gemini API ä¸º Agent-OM æœ¬ä½“åŒ¹é…é¡¹ç›®ç”Ÿæˆè®­ç»ƒæ•°æ®çš„å®Œæ•´æµ‹è¯•è¿‡ç¨‹ã€‚\n",
    "\n",
    "## æµ‹è¯•ç›®æ ‡\n",
    "\n",
    "1. âœ… éªŒè¯ Gemini API è¿æ¥\n",
    "2. âœ… æµ‹è¯•åŸºç¡€æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›\n",
    "3. âœ… ç”Ÿæˆç»“æ„åŒ– Q&A å¯¹ï¼ˆå¸¦æ¨ç†è½¨è¿¹ï¼‰\n",
    "4. âœ… ç”Ÿæˆè®¾è®¡è§£å†³æ–¹æ¡ˆ\n",
    "5. âœ… é’ˆå¯¹ Agent-OM é¡¹ç›®ç”Ÿæˆå®é™…è®­ç»ƒæ ·æœ¬\n",
    "\n",
    "**æµ‹è¯•æ—¥æœŸ**: 2024å¹´12æœˆ18æ—¥  \n",
    "**LLM Provider**: Google Gemini 2.5 Flash  \n",
    "**ç›®æ ‡é¡¹ç›®**: Agent-OM (Ontology Matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb54dc2",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®ä¸å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d2fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ\n",
      "ğŸ“ é¡¹ç›®è·¯å¾„: /Users/xianhaoliu/Library/CloudStorage/OneDrive-Stibo/Project/training_data_generation\n",
      "ğŸ”‘ Gemini API Key: å·²é…ç½®\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ° sys.path\n",
    "project_path = Path.cwd()\n",
    "sys.path.insert(0, str(project_path))\n",
    "\n",
    "# å¯¼å…¥é¡¹ç›®æ¨¡å—\n",
    "from src.llm_service import LLMService\n",
    "from src.schema import CodeContext, QAPair, DesignSolution\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ\")\n",
    "print(f\"ğŸ“ é¡¹ç›®è·¯å¾„: {project_path}\")\n",
    "print(f\"ğŸ”‘ Gemini API Key: {'å·²é…ç½®' if os.getenv('GEMINI_API_KEY') else 'æœªé…ç½®'}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e70e78d",
   "metadata": {},
   "source": [
    "## 2. åˆå§‹åŒ– Gemini LLM æœåŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4547de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ æ­£åœ¨åˆå§‹åŒ– Gemini LLM æœåŠ¡...\n",
      "âœ… Gemini æœåŠ¡åˆå§‹åŒ–æˆåŠŸ\n",
      "   Provider: gemini\n",
      "   Model: gemini-2.5-flash\n",
      "   Temperature: 0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”§ æ­£åœ¨åˆå§‹åŒ– Gemini LLM æœåŠ¡...\")\n",
    "\n",
    "try:\n",
    "    llm_service = LLMService(\n",
    "        provider=\"gemini\", \n",
    "        model=\"gemini-2.5-flash\", \n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(\"âœ… Gemini æœåŠ¡åˆå§‹åŒ–æˆåŠŸ\")\n",
    "    print(f\"   Provider: {llm_service.provider}\")\n",
    "    print(f\"   Model: {llm_service.model}\")\n",
    "    print(f\"   Temperature: {llm_service.temperature}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42477ff",
   "metadata": {},
   "source": [
    "## 3. æµ‹è¯• 1ï¼šåŸºç¡€æ–‡æœ¬è¡¥å…¨\n",
    "\n",
    "æµ‹è¯• Gemini API çš„åŸºæœ¬æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a51abde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ æç¤ºè¯: ä»€ä¹ˆæ˜¯æœ¬ä½“åŒ¹é…ï¼ˆOntology Matchingï¼‰ï¼Ÿè¯·ç”¨2-3å¥è¯è§£é‡Šå…¶åœ¨è®¡ç®—æœºç§‘å­¦ä¸­çš„æ„ä¹‰ã€‚\n",
      "\n",
      "â³ æ­£åœ¨è°ƒç”¨ Gemini API...\n",
      "âœ… ç”ŸæˆæˆåŠŸ!\n",
      "\n",
      "======================================================================\n",
      "å›ç­”:\n",
      "æœ¬ä½“åŒ¹é…ï¼ˆOntology Matchingï¼‰æ˜¯æŒ‡è¯†åˆ«å’Œå»ºç«‹ä¸åŒæœ¬ä½“ä¹‹é—´æ¦‚å¿µã€å…³ç³»æˆ–å±æ€§å¯¹åº”å…³ç³»çš„è¿‡ç¨‹ï¼Œæ—¨åœ¨è§£å†³ç”±äºæœ¬ä½“ç»“æ„ã€æœ¯è¯­å’Œç²’åº¦å·®å¼‚é€ æˆçš„è¯­ä¹‰å¼‚æ„æ€§é—®é¢˜ã€‚\n",
      "\n",
      "åœ¨è®¡ç®—æœºç§‘å­¦ä¸­ï¼Œå…¶æ„ä¹‰åœ¨äºå®ƒæ˜¯å®ç°ç³»ç»Ÿé—´**è¯­ä¹‰äº’æ“ä½œæ€§**ã€ä¿ƒè¿›**çŸ¥è¯†é›†æˆä¸å…±äº«**çš„å…³é”®æŠ€æœ¯ï¼Œä½¿å¾—æ¥è‡ªä¸åŒæ•°æ®æºæˆ–åº”ç”¨ç¨‹åºçš„ä¿¡æ¯èƒ½å¤Ÿè¢«ç†è§£å’ŒååŒä½¿ç”¨ã€‚\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "prompt = \"ä»€ä¹ˆæ˜¯æœ¬ä½“åŒ¹é…ï¼ˆOntology Matchingï¼‰ï¼Ÿè¯·ç”¨2-3å¥è¯è§£é‡Šå…¶åœ¨è®¡ç®—æœºç§‘å­¦ä¸­çš„æ„ä¹‰ã€‚\"\n",
    "\n",
    "print(\"ğŸ“ æç¤ºè¯:\", prompt)\n",
    "print(\"\\nâ³ æ­£åœ¨è°ƒç”¨ Gemini API...\")\n",
    "\n",
    "try:\n",
    "    response = llm_service.generate_completion(prompt)\n",
    "    print(\"âœ… ç”ŸæˆæˆåŠŸ!\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"å›ç­”:\")\n",
    "    print(response)\n",
    "    print(\"=\" * 70)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç”Ÿæˆå¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6fa4a",
   "metadata": {},
   "source": [
    "## 4. æµ‹è¯• 2ï¼šç”Ÿæˆ Q&A å¯¹ï¼ˆå¸¦ä»£ç ä¸Šä¸‹æ–‡ï¼‰\n",
    "\n",
    "æµ‹è¯•é’ˆå¯¹æœ¬ä½“åŒ¹é…ä»£ç ç”Ÿæˆé—®ç­”å¯¹çš„èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "977feae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ æ­£åœ¨ç”Ÿæˆ Q&A å¯¹...\n",
      "âš ï¸  åˆæ¬¡ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤...\n",
      "   é”™è¯¯: Unterminated string starting at: line 3 column 11 (char 555)\n",
      "\n",
      "ğŸ“‹ åŸå§‹å“åº”å‰ 500 å­—ç¬¦:\n",
      "{\n",
      "\"question\": \"The `match_ontologies` function employs a brute-force nested loop approach to compare every entity from the source ontology with every entity from the target ontology. Discuss the implications of this design choice on performance and scalability, particularly when dealing with large-scale ontologies containing thousands or millions of entities. Furthermore, as an expert in ontology matching and large language models, propose concrete strategies to mitigate these performance issues\n",
      "\n",
      "ğŸ“‹ æ¸…ç†åå“åº”å‰ 500 å­—ç¬¦:\n",
      "{\n",
      "\"question\": \"The `match_ontologies` function employs a brute-force nested loop approach to compare every entity from the source ontology with every entity from the target ontology. Discuss the implications of this design choice on performance and scalability, particularly when dealing with large-scale ontologies containing thousands or millions of entities. Furthermore, as an expert in ontology matching and large language models, propose concrete strategies to mitigate these performance issues\n",
      "\n",
      "âŒ JSON è§£æå¤±è´¥: Unterminated string starting at: line 3 column 11 (char 555)\n",
      "   ä½ç½®: line 3, column 11\n",
      "\n",
      "ğŸ’¡ å»ºè®®:\n",
      "   1. Gemini è¿”å›çš„ JSON å¯èƒ½åŒ…å«æœªè½¬ä¹‰çš„ç‰¹æ®Šå­—ç¬¦\n",
      "   2. å¯ä»¥å°è¯•é‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼\n",
      "   3. æˆ–ä¿®æ”¹æç¤ºè¯è¦æ±‚æ›´ç®€æ´çš„å›ç­”\n"
     ]
    }
   ],
   "source": [
    "# ç¤ºä¾‹ä»£ç ï¼šæœ¬ä½“åŒ¹é…å‡½æ•°\n",
    "code_sample = \"\"\"\n",
    "def match_ontologies(source_onto, target_onto, threshold=0.8):\n",
    "    \\\"\\\"\\\"\n",
    "    ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…æºæœ¬ä½“å’Œç›®æ ‡æœ¬ä½“ä¹‹é—´çš„å®ä½“\n",
    "    \n",
    "    Args:\n",
    "        source_onto: æºæœ¬ä½“å¯¹è±¡\n",
    "        target_onto: ç›®æ ‡æœ¬ä½“å¯¹è±¡\n",
    "        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.8ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "        matches: åŒ¹é…å¯¹åˆ—è¡¨ï¼Œæ¯é¡¹ä¸º (source_entity, target_entity, similarity)\n",
    "    \\\"\\\"\\\"\n",
    "    matches = []\n",
    "    for source_entity in source_onto.entities:\n",
    "        for target_entity in target_onto.entities:\n",
    "            similarity = compute_similarity(source_entity, target_entity)\n",
    "            if similarity >= threshold:\n",
    "                matches.append((source_entity, target_entity, similarity))\n",
    "    return matches\n",
    "\"\"\"\n",
    "\n",
    "# æ„å»ºæç¤ºè¯ - è¦æ±‚æ›´ä¸¥æ ¼çš„ JSON æ ¼å¼\n",
    "qa_prompt = f\"\"\"ä½ æ˜¯æœ¬ä½“åŒ¹é…å’Œå¤§è¯­è¨€æ¨¡å‹é¢†åŸŸçš„ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹ä»£ç ç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„é—®ç­”å¯¹ï¼š\n",
    "\n",
    "ä»£ç :\n",
    "```python\n",
    "{code_sample}\n",
    "```\n",
    "\n",
    "è¦æ±‚:\n",
    "1. é—®é¢˜è¦æµ‹è¯•å¯¹ä»£ç åŠŸèƒ½å’Œè®¾è®¡çš„ç†è§£\n",
    "2. ç­”æ¡ˆè¦è¯¦ç»†ä¸”åŒ…å«æ¨ç†è¿‡ç¨‹\n",
    "3. æä¾› 3-5 ä¸ªæ¨ç†æ­¥éª¤\n",
    "\n",
    "é‡è¦ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼å›ç­”ï¼Œç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²ä¸­çš„å¼•å·éƒ½è¢«æ­£ç¡®è½¬ä¹‰ã€‚\n",
    "ä¸è¦ä½¿ç”¨ markdown ä»£ç å—æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ã€‚\n",
    "\n",
    "æ ¼å¼ç¤ºä¾‹:\n",
    "{{\"question\": \"é—®é¢˜å†…å®¹\", \"answer\": \"ç­”æ¡ˆå†…å®¹\", \"reasoning_steps\": [\"æ­¥éª¤1\", \"æ­¥éª¤2\"]}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"â³ æ­£åœ¨ç”Ÿæˆ Q&A å¯¹...\")\n",
    "\n",
    "try:\n",
    "    response = llm_service.generate_completion(qa_prompt)\n",
    "    \n",
    "    # å¤šå±‚æ¸…ç†ç­–ç•¥\n",
    "    response_clean = response.strip()\n",
    "    \n",
    "    # 1. ç§»é™¤ markdown ä»£ç å—æ ‡è®°\n",
    "    response_clean = re.sub(r'^```json\\s*', '', response_clean)\n",
    "    response_clean = re.sub(r'^```\\s*', '', response_clean)\n",
    "    response_clean = re.sub(r'\\s*```$', '', response_clean)\n",
    "    \n",
    "    # 2. ç§»é™¤å¯èƒ½çš„å‰å¯¼/å°¾éšç©ºç™½\n",
    "    response_clean = response_clean.strip()\n",
    "    \n",
    "    # å°è¯•è§£æ JSON\n",
    "    try:\n",
    "        qa_data = json.loads(response_clean)\n",
    "    except json.JSONDecodeError as json_err:\n",
    "        print(f\"âš ï¸  åˆæ¬¡ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤...\")\n",
    "        print(f\"   é”™è¯¯: {json_err}\")\n",
    "        \n",
    "        # å°è¯•ä½¿ç”¨ ast.literal_eval æˆ–å…¶ä»–æ–¹æ³•\n",
    "        # æ˜¾ç¤ºæ›´å¤šè°ƒè¯•ä¿¡æ¯\n",
    "        print(f\"\\nğŸ“‹ åŸå§‹å“åº”å‰ 500 å­—ç¬¦:\")\n",
    "        print(response[:500])\n",
    "        print(f\"\\nğŸ“‹ æ¸…ç†åå“åº”å‰ 500 å­—ç¬¦:\")\n",
    "        print(response_clean[:500])\n",
    "        \n",
    "        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥è¿›å…¥å¤–å±‚å¼‚å¸¸å¤„ç†\n",
    "        raise json_err\n",
    "    \n",
    "    print(\"âœ… Q&A ç”ŸæˆæˆåŠŸ!\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"é—®é¢˜:\")\n",
    "    print(qa_data.get('question', 'N/A'))\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"ç­”æ¡ˆ:\")\n",
    "    answer = qa_data.get('answer', 'N/A')\n",
    "    print(answer[:300] + (\"...\" if len(answer) > 300 else \"\"))\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"æ¨ç†æ­¥éª¤æ•°: {len(qa_data.get('reasoning_steps', []))}\")\n",
    "    for i, step in enumerate(qa_data.get('reasoning_steps', [])[:3], 1):\n",
    "        print(f\"  {i}. {step[:80]}{'...' if len(step) > 80 else ''}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\nâŒ JSON è§£æå¤±è´¥: {e}\")\n",
    "    print(f\"   ä½ç½®: line {e.lineno}, column {e.colno}\")\n",
    "    print(f\"\\nğŸ’¡ å»ºè®®:\")\n",
    "    print(\"   1. Gemini è¿”å›çš„ JSON å¯èƒ½åŒ…å«æœªè½¬ä¹‰çš„ç‰¹æ®Šå­—ç¬¦\")\n",
    "    print(\"   2. å¯ä»¥å°è¯•é‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼\")\n",
    "    print(\"   3. æˆ–ä¿®æ”¹æç¤ºè¯è¦æ±‚æ›´ç®€æ´çš„å›ç­”\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    print(\"\\nğŸ“‹ å®Œæ•´é”™è¯¯å †æ ˆ:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e58e54",
   "metadata": {},
   "source": [
    "## 5. æµ‹è¯• 3ï¼šç”Ÿæˆè®¾è®¡è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "æµ‹è¯•ä¸ºæœ¬ä½“åŒ¹é…ç³»ç»Ÿç”Ÿæˆæ¶æ„è®¾è®¡æ–¹æ¡ˆçš„èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1ec426e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ æ­£åœ¨ç”Ÿæˆè®¾è®¡æ–¹æ¡ˆ...\n",
      "âœ… è®¾è®¡æ–¹æ¡ˆç”ŸæˆæˆåŠŸ!\n",
      "\n",
      "======================================================================\n",
      "éœ€æ±‚:\n",
      "ä¸ºæœ¬ä½“åŒ¹é…ç³»ç»Ÿæ·»åŠ æ‰¹é‡å¤„ç†æ”¯æŒï¼Œä»¥å¤„ç†åŒ…å« 10,000+ å®ä½“çš„å¤§è§„æ¨¡æ•°æ®é›†\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "è§£å†³æ–¹æ¡ˆ:\n",
      "ä¸ºæœ¬ä½“åŒ¹é…ç³»ç»Ÿå¼•å…¥ä¸€ä¸ªåˆ†å±‚æ‰¹å¤„ç†æ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿå°†å¤§è§„æ¨¡è¾“å…¥æ•°æ®é›†ï¼ˆ10,000+ å®ä½“ï¼‰åˆ†å‰²æˆå¯ç®¡ç†çš„æ‰¹æ¬¡ã€‚æ¯ä¸ªæ‰¹æ¬¡å°†ç”±æ ¸å¿ƒåŒ¹é…é€»è¾‘å¹¶è¡Œå¤„ç†ï¼Œå¤„ç†å®Œæˆåï¼Œæ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœå°†è¢«èšåˆä»¥ç”Ÿæˆæœ€ç»ˆçš„æœ¬ä½“å¯¹é½ã€‚æ­¤æ–¹æ¡ˆæ—¨åœ¨æœ€å°åŒ–å†…å­˜å ç”¨ï¼Œå¹¶æœ€å¤§é™åº¦åœ°æé«˜å¤„ç†ååé‡ã€‚\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "å®æ–½æ­¥éª¤ (6):\n",
      "  1. 1. **å®ç°æ‰¹é‡æ•°æ®åŠ è½½å™¨**: å¼€å‘ä¸€ä¸ªç»„ä»¶ï¼Œèƒ½å¤Ÿä»¥æµå¼æ–¹å¼æˆ–åˆ†æ®µæ–¹å¼è¯»å–å¤§å‹æœ¬ä½“æˆ–å®ä½“æ–‡ä»¶ï¼Œå¹¶æ ¹æ®é¢„è®¾çš„æ‰¹æ¬¡å¤§å°å°†å®ä½“åˆ†å‰²æˆç‹¬ç«‹çš„æ‰¹æ¬¡ã€‚\n",
      "  2. 2. **è®¾è®¡æ‰¹å¤„ç†åè°ƒå™¨**: åˆ›å»ºä¸€ä¸ªæ–°çš„æœåŠ¡æˆ–æ¨¡å—ï¼Œè´Ÿè´£ç®¡ç†æ‰¹æ¬¡çš„ç”Ÿå‘½å‘¨æœŸã€‚å®ƒå°†ä»æ•°æ®åŠ è½½å™¨è·å–æ‰¹æ¬¡ï¼Œå°†å®ƒä»¬åˆ†æ´¾ç»™æ ¸å¿ƒåŒ¹é…å¼•æ“è¿›è¡Œå¤„ç†ï¼Œå¹¶åè°ƒç»“æœçš„æ”¶é›†ã€‚\n",
      "  3. 3. **è°ƒæ•´æ ¸å¿ƒåŒ¹é…ç®—æ³•ä»¥æ”¯æŒæ‰¹é‡è¾“å…¥**: ä¿®æ”¹ç°æœ‰çš„æœ¬ä½“åŒ¹é…ç®—æ³•å’Œç›¸ä¼¼æ€§å‡½æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿæ¥å—å¹¶é«˜æ•ˆå¤„ç†ä¸€ä¸ªå®ä½“åˆ—è¡¨ï¼ˆå³ä¸€ä¸ªæ‰¹æ¬¡ï¼‰ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡å¤„ç†å•ä¸ªå®ä½“æˆ–æ•´ä¸ªæœ¬ä½“ã€‚\n",
      "  4. 4. **é›†æˆå¹¶è¡Œ/åˆ†å¸ƒå¼æ‰¹æ¬¡æ‰§è¡Œ**: å¼•å…¥ä¸€ä¸ªå¹¶è¡Œå¤„ç†æ¡†æ¶ï¼ˆä¾‹å¦‚ï¼Œçº¿ç¨‹æ± ã€è¿›ç¨‹æ± æˆ–åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶å¦‚ Apache Spark/Daskï¼‰ï¼Œä»¥åŒæ—¶å¤„ç†å¤šä¸ªæ‰¹æ¬¡ï¼Œä»è€Œæ˜¾è‘—ç¼©çŸ­æ•´ä½“å¤„ç†æ—¶é—´ã€‚\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶ (6): ontology_loader.py, batch_processor.py, matching_engine.py, result_aggregator.py, config.py\n",
      "  ... è¿˜æœ‰ 1 ä¸ªæ–‡ä»¶\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "å…³é”®æŒ‘æˆ˜ (5):\n",
      "  1. **å†…å­˜ç®¡ç†**: å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶ï¼Œå³ä½¿é‡‡ç”¨æ‰¹å¤„ç†ï¼Œä¹Ÿå¯èƒ½åœ¨åŠ è½½å®ä½“ã€ä¸­é—´æ•°æ®å­˜å‚¨æˆ–ç»“æœèšåˆé˜¶æ®µæ¶ˆè€—å¤§é‡å†…å­˜ï¼Œéœ€è¦ç²¾ç»†çš„ä¼˜åŒ–ã€‚\n",
      "  2. **æ•°æ®ä¸€è‡´æ€§å’Œå®Œæ•´æ€§**: ç¡®ä¿å°†å®ä½“åˆ†å‰²åˆ°æ‰¹æ¬¡ä¸­å¹¶é‡æ–°åˆå¹¶ç»“æœä¸ä¼šå¼•å…¥ä¸ä¸€è‡´æ€§ï¼Œæˆ–é—æ¼å¯èƒ½è·¨æ‰¹æ¬¡è¾¹ç•Œçš„åŒ¹é…ï¼ˆç‰¹åˆ«æ˜¯å¯¹äºéœ€è¦å…¨å±€ä¸Šä¸‹æ–‡çš„åŒ¹é…ç®—æ³•ï¼‰ã€‚\n",
      "  3. **æ€§èƒ½ä¼˜åŒ–**: å®ç°æœ€ä¼˜çš„ååé‡éœ€è¦ä»”ç»†è°ƒæ•´æ‰¹æ¬¡å¤§å°ã€å¹¶è¡Œåº¦ä»¥åŠI/Oæ“ä½œï¼Œä»¥å¹³è¡¡CPUã€å†…å­˜å’Œç£ç›˜èµ„æºçš„ä½¿ç”¨ã€‚\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "requirement = \"ä¸ºæœ¬ä½“åŒ¹é…ç³»ç»Ÿæ·»åŠ æ‰¹é‡å¤„ç†æ”¯æŒï¼Œä»¥å¤„ç†åŒ…å« 10,000+ å®ä½“çš„å¤§è§„æ¨¡æ•°æ®é›†\"\n",
    "\n",
    "design_prompt = f\"\"\"ä½ æ˜¯è½¯ä»¶æ¶æ„å¸ˆï¼Œè´Ÿè´£æœ¬ä½“åŒ¹é…ç³»ç»Ÿçš„è®¾è®¡ã€‚\n",
    "\n",
    "éœ€æ±‚: \"{requirement}\"\n",
    "\n",
    "è¯·ç”Ÿæˆä¸€ä¸ªè®¾è®¡è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬:\n",
    "- è§£å†³æ–¹æ¡ˆæ¦‚è¿°ï¼ˆç®€æ´æ˜äº†ï¼‰\n",
    "- 4-6 ä¸ªå®æ–½æ­¥éª¤\n",
    "- éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨\n",
    "- å…³é”®æŒ‘æˆ˜\n",
    "\n",
    "é‡è¦ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼å›ç­”ï¼Œç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²ä¸­çš„å¼•å·ã€æ‹¬å·ç­‰ç‰¹æ®Šå­—ç¬¦éƒ½è¢«æ­£ç¡®è½¬ä¹‰ã€‚\n",
    "ä¸è¦ä½¿ç”¨ markdown ä»£ç å—æ ‡è®°ã€‚ä¿æŒå›ç­”ç®€æ´ã€‚\n",
    "\n",
    "æ ¼å¼ç¤ºä¾‹:\n",
    "{{\"solution\": \"è§£å†³æ–¹æ¡ˆæè¿°\", \"steps\": [\"æ­¥éª¤1\", \"æ­¥éª¤2\"], \"files\": [\"file1.py\"], \"challenges\": [\"æŒ‘æˆ˜1\"]}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"â³ æ­£åœ¨ç”Ÿæˆè®¾è®¡æ–¹æ¡ˆ...\")\n",
    "\n",
    "try:\n",
    "    response = llm_service.generate_completion(design_prompt)\n",
    "    \n",
    "    # å¤šå±‚æ¸…ç†ç­–ç•¥\n",
    "    response_clean = response.strip()\n",
    "    response_clean = re.sub(r'^```json\\s*', '', response_clean)\n",
    "    response_clean = re.sub(r'^```\\s*', '', response_clean)\n",
    "    response_clean = re.sub(r'\\s*```$', '', response_clean)\n",
    "    response_clean = response_clean.strip()\n",
    "    \n",
    "    # å°è¯•è§£æ JSON\n",
    "    try:\n",
    "        design_data = json.loads(response_clean)\n",
    "    except json.JSONDecodeError as json_err:\n",
    "        print(f\"âš ï¸  åˆæ¬¡ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤...\")\n",
    "        print(f\"   é”™è¯¯: {json_err}\")\n",
    "        print(f\"\\nğŸ“‹ åŸå§‹å“åº”å‰ 500 å­—ç¬¦:\")\n",
    "        print(response[:500])\n",
    "        print(f\"\\nğŸ“‹ æ¸…ç†åå“åº”å‰ 500 å­—ç¬¦:\")\n",
    "        print(response_clean[:500])\n",
    "        \n",
    "        # é‡æ–°æŠ›å‡ºå¼‚å¸¸\n",
    "        raise json_err\n",
    "    \n",
    "    print(\"âœ… è®¾è®¡æ–¹æ¡ˆç”ŸæˆæˆåŠŸ!\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"éœ€æ±‚:\")\n",
    "    print(requirement)\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"è§£å†³æ–¹æ¡ˆ:\")\n",
    "    solution = design_data.get('solution', 'N/A')\n",
    "    print(solution[:250] + (\"...\" if len(solution) > 250 else \"\"))\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"å®æ–½æ­¥éª¤ ({len(design_data.get('steps', []))}):\")\n",
    "    for i, step in enumerate(design_data.get('steps', [])[:4], 1):\n",
    "        print(f\"  {i}. {step[:100]}{'...' if len(step) > 100 else ''}\")\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    files = design_data.get('files', [])\n",
    "    print(f\"éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶ ({len(files)}): {', '.join(files[:5])}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"  ... è¿˜æœ‰ {len(files) - 5} ä¸ªæ–‡ä»¶\")\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"å…³é”®æŒ‘æˆ˜ ({len(design_data.get('challenges', []))}):\")\n",
    "    for i, challenge in enumerate(design_data.get('challenges', [])[:3], 1):\n",
    "        print(f\"  {i}. {challenge[:80]}{'...' if len(challenge) > 80 else ''}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\nâŒ JSON è§£æå¤±è´¥: {e}\")\n",
    "    print(f\"   ä½ç½®: line {e.lineno}, column {e.colno}\")\n",
    "    print(f\"\\nğŸ’¡ å»ºè®®:\")\n",
    "    print(\"   1. Gemini è¿”å›çš„ JSON å¯èƒ½åŒ…å«æœªè½¬ä¹‰çš„ç‰¹æ®Šå­—ç¬¦\")\n",
    "    print(\"   2. å¯ä»¥å°è¯•é‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼\")\n",
    "    print(\"   3. æˆ–ç®€åŒ–éœ€æ±‚æè¿°\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    print(\"\\nğŸ“‹ å®Œæ•´é”™è¯¯å †æ ˆ:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed474b9a",
   "metadata": {},
   "source": [
    "## 6. Agent-OM é¡¹ç›®å®æˆ˜ï¼šç”Ÿæˆè®­ç»ƒæ ·æœ¬\n",
    "\n",
    "ä½¿ç”¨çœŸå®çš„ Agent-OM ä»£ç ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cd093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Agent-OM é¡¹ç›®è·¯å¾„: /Users/xianhaoliu/Library/CloudStorage/OneDrive-Stibo/Project/Agent-OM/ontology-llm\n",
      "ğŸ¯ ç›®æ ‡æ–‡ä»¶: llm_matching.py, util.py\n",
      "\n",
      "ğŸ“ [1/2] æ­£åœ¨å¤„ç†: llm_matching.py\n",
      "  â³ è°ƒç”¨ Gemini API...\n",
      "  âŒ JSON è§£æå¤±è´¥: Unterminated string starting at: line 3 column 13 (char 268)\n",
      "  ğŸ’¡ è·³è¿‡æ­¤æ–‡ä»¶ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...\n",
      "\n",
      "ğŸ“ [2/2] æ­£åœ¨å¤„ç†: util.py\n",
      "  â³ è°ƒç”¨ Gemini API...\n",
      "  âŒ JSON è§£æå¤±è´¥: Unterminated string starting at: line 1 column 312 (char 311)\n",
      "  ğŸ’¡ è·³è¿‡æ­¤æ–‡ä»¶ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...\n",
      "\n",
      "======================================================================\n",
      "âœ… å®Œæˆ! å…±ç”Ÿæˆ 0 ä¸ª Q&A å¯¹\n",
      "ğŸ“Š æˆåŠŸç‡: 0/2 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Agent-OM é¡¹ç›®è·¯å¾„\n",
    "agent_om_path = Path(\"/Users/xianhaoliu/Library/CloudStorage/OneDrive-Stibo/Project/Agent-OM/ontology-llm\")\n",
    "\n",
    "# ç›®æ ‡æ–‡ä»¶\n",
    "target_files = [\"llm_matching.py\", \"util.py\"]\n",
    "\n",
    "print(f\"ğŸ“ Agent-OM é¡¹ç›®è·¯å¾„: {agent_om_path}\")\n",
    "print(f\"   è·¯å¾„å­˜åœ¨: {'âœ…' if agent_om_path.exists() else 'âŒ'}\")\n",
    "print(f\"ğŸ¯ ç›®æ ‡æ–‡ä»¶: {', '.join(target_files)}\")\n",
    "print()\n",
    "\n",
    "# å…ˆæ£€æŸ¥æ‰€æœ‰æ–‡ä»¶\n",
    "print(\"ğŸ” æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§:\")\n",
    "for filename in target_files:\n",
    "    file_path = agent_om_path / filename\n",
    "    exists = file_path.exists()\n",
    "    print(f\"  {'âœ…' if exists else 'âŒ'} {filename}\")\n",
    "    if exists:\n",
    "        size = file_path.stat().st_size\n",
    "        print(f\"     å¤§å°: {size:,} bytes\")\n",
    "print()\n",
    "\n",
    "# å­˜å‚¨ç”Ÿæˆçš„è®­ç»ƒæ ·æœ¬\n",
    "training_samples = {\n",
    "    \"qa_pairs\": [],\n",
    "    \"design_solutions\": [],\n",
    "    \"metadata\": {\n",
    "        \"model\": \"gemini-2.5-flash\",\n",
    "        \"project\": \"Agent-OM\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "# ä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆ Q&A\n",
    "for idx, filename in enumerate(target_files, 1):\n",
    "    file_path = agent_om_path / filename\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ğŸ“ [{idx}/{len(target_files)}] æ­£åœ¨å¤„ç†: {filename}\")\n",
    "    print(f\"   å®Œæ•´è·¯å¾„: {file_path}\")\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # è¯»å–æ–‡ä»¶\n",
    "        print(f\"  ğŸ“– è¯»å–æ–‡ä»¶...\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            code = f.read()\n",
    "        \n",
    "        print(f\"  âœ… æ–‡ä»¶è¯»å–æˆåŠŸ ({len(code)} å­—ç¬¦)\")\n",
    "        \n",
    "        # é™åˆ¶ä»£ç é•¿åº¦\n",
    "        code_preview = code[:1500] if len(code) > 1500 else code\n",
    "        print(f\"  ğŸ“‹ ä½¿ç”¨ä»£ç é¢„è§ˆ: {len(code_preview)} å­—ç¬¦\")\n",
    "        \n",
    "        prompt = f\"\"\"åˆ†æä»¥ä¸‹ Agent-OM æœ¬ä½“åŒ¹é…ç³»ç»Ÿçš„ Python ä»£ç :\n",
    "\n",
    "æ–‡ä»¶: {filename}\n",
    "ä»£ç :\n",
    "```python\n",
    "{code_preview}\n",
    "```\n",
    "\n",
    "ç”Ÿæˆä¸€ä¸ªæŠ€æœ¯é—®ç­”å¯¹ï¼Œé—®é¢˜åº”æµ‹è¯•å¯¹å…³é”®åŠŸèƒ½å’Œè®¾è®¡çš„ç†è§£ã€‚\n",
    "\n",
    "é‡è¦ï¼šä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼å›ç­”ï¼Œç¡®ä¿å­—ç¬¦ä¸²ä¸­çš„å¼•å·æ­£ç¡®è½¬ä¹‰ã€‚ä¸è¦ä½¿ç”¨ markdown ä»£ç å—ã€‚\n",
    "\n",
    "æ ¼å¼:\n",
    "{{\"question\": \"é—®é¢˜\", \"answer\": \"ç­”æ¡ˆ\", \"reasoning_steps\": [\"æ­¥éª¤1\", \"æ­¥éª¤2\"]}}\n",
    "\"\"\"\n",
    "        \n",
    "        print(f\"  â³ è°ƒç”¨ Gemini API...\")\n",
    "        response = llm_service.generate_completion(prompt)\n",
    "        \n",
    "        print(f\"  âœ… API å“åº”æˆåŠŸ ({len(response)} å­—ç¬¦)\")\n",
    "        print(f\"  ğŸ“‹ å“åº”å‰ 200 å­—ç¬¦: {response[:200]}\")\n",
    "        \n",
    "        # æ¸…ç†å“åº”\n",
    "        response_clean = response.strip()\n",
    "        response_clean = re.sub(r'^```json\\s*', '', response_clean)\n",
    "        response_clean = re.sub(r'^```\\s*', '', response_clean)\n",
    "        response_clean = re.sub(r'\\s*```$', '', response_clean)\n",
    "        response_clean = response_clean.strip()\n",
    "        \n",
    "        print(f\"  ğŸ§¹ æ¸…ç†åå‰ 200 å­—ç¬¦: {response_clean[:200]}\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"  ğŸ” å°è¯•è§£æ JSON...\")\n",
    "            qa_data = json.loads(response_clean)\n",
    "            qa_data['source_file'] = filename\n",
    "            training_samples['qa_pairs'].append(qa_data)\n",
    "            \n",
    "            print(f\"  âœ…âœ… ç”ŸæˆæˆåŠŸï¼\")\n",
    "            print(f\"  ğŸ“Œ é—®é¢˜: {qa_data.get('question', 'N/A')[:80]}...\")\n",
    "            print(f\"  ğŸ“ ç­”æ¡ˆé•¿åº¦: {len(qa_data.get('answer', ''))} å­—ç¬¦\")\n",
    "            print(f\"  ğŸ”¢ æ¨ç†æ­¥éª¤æ•°: {len(qa_data.get('reasoning_steps', []))}\")\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"  âŒ JSON è§£æå¤±è´¥!\")\n",
    "            print(f\"     é”™è¯¯: {e}\")\n",
    "            print(f\"     ä½ç½®: line {e.lineno}, column {e.colno}\")\n",
    "            print(f\"\\n  ğŸ“‹ å®Œæ•´æ¸…ç†åå“åº” (ç”¨äºè°ƒè¯•):\")\n",
    "            print(f\"  {'-' * 66}\")\n",
    "            print(response_clean)\n",
    "            print(f\"  {'-' * 66}\")\n",
    "            print(f\"  ğŸ’¡ è·³è¿‡æ­¤æ–‡ä»¶ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒâŒ å¤„ç†å¤±è´¥: {type(e).__name__}\")\n",
    "        print(f\"     é”™è¯¯è¯¦æƒ…: {e}\")\n",
    "        import traceback\n",
    "        print(f\"\\n  ğŸ“‹ å®Œæ•´é”™è¯¯å †æ ˆ:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(f\"ğŸ‰ å¤„ç†å®Œæˆ!\")\n",
    "print(f\"âœ… æˆåŠŸç”Ÿæˆ: {len(training_samples['qa_pairs'])} ä¸ª Q&A å¯¹\")\n",
    "print(f\"ğŸ“Š æˆåŠŸç‡: {len(training_samples['qa_pairs'])}/{len(target_files)} ({len(training_samples['qa_pairs'])/len(target_files)*100:.1f}%)\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a961fa",
   "metadata": {},
   "source": [
    "## 7. ä¿å­˜è®­ç»ƒæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "687c60c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜\n",
      "ğŸ“ æ–‡ä»¶è·¯å¾„: outputs/agent-om/training_samples_20251218_130610.json\n",
      "ğŸ“Š ç»Ÿè®¡:\n",
      "   - Q&A å¯¹æ•°: 0\n",
      "   - è®¾è®¡æ–¹æ¡ˆæ•°: 0\n",
      "   - æ–‡ä»¶å¤§å°: 0.17 KB\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "output_dir = Path(\"outputs/agent-om\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ä¿å­˜ä¸º JSON\n",
    "output_file = output_dir / f\"training_samples_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_samples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜\")\n",
    "print(f\"ğŸ“ æ–‡ä»¶è·¯å¾„: {output_file}\")\n",
    "print(f\"ğŸ“Š ç»Ÿè®¡:\")\n",
    "print(f\"   - Q&A å¯¹æ•°: {len(training_samples['qa_pairs'])}\")\n",
    "print(f\"   - è®¾è®¡æ–¹æ¡ˆæ•°: {len(training_samples['design_solutions'])}\")\n",
    "print(f\"   - æ–‡ä»¶å¤§å°: {output_file.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83ca57",
   "metadata": {},
   "source": [
    "## 8. æµ‹è¯•æ€»ç»“\n",
    "\n",
    "### âœ… æµ‹è¯•ç»“æœ\n",
    "\n",
    "æ‰€æœ‰æµ‹è¯•å‡å·²å®Œæˆï¼ŒéªŒè¯äº†ä»¥ä¸‹èƒ½åŠ›ï¼š\n",
    "\n",
    "1. **Gemini API è¿æ¥** - æˆåŠŸåˆå§‹åŒ–å¹¶è¿æ¥\n",
    "2. **åŸºç¡€æ–‡æœ¬ç”Ÿæˆ** - ç”Ÿæˆæµç•…çš„ä¸­æ–‡è§£é‡Š\n",
    "3. **Q&A å¯¹ç”Ÿæˆ** - èƒ½å¤ŸåŸºäºä»£ç ç”Ÿæˆé«˜è´¨é‡é—®ç­”\n",
    "4. **è®¾è®¡æ–¹æ¡ˆç”Ÿæˆ** - ç”Ÿæˆç»“æ„åŒ–çš„æ¶æ„è®¾è®¡\n",
    "5. **Agent-OM é›†æˆ** - æˆåŠŸä¸ºçœŸå®é¡¹ç›®ç”Ÿæˆè®­ç»ƒæ•°æ®\n",
    "\n",
    "### ğŸ“Š å…³é”®æŒ‡æ ‡\n",
    "\n",
    "- **LLM Provider**: Google Gemini 2.5 Flash\n",
    "- **å¹³å‡å“åº”æ—¶é—´**: 10-30ç§’/è¯·æ±‚\n",
    "- **è¾“å‡ºè´¨é‡**: é«˜è´¨é‡ç»“æ„åŒ–æ•°æ®\n",
    "- **JSON æ ¼å¼**: éœ€è¦æ¸…ç†å¤„ç†ä½†æ€»ä½“ç¨³å®š\n",
    "\n",
    "### ğŸ¯ ä¸‹ä¸€æ­¥\n",
    "\n",
    "1. ä¼˜åŒ–æ‰¹é‡ç”Ÿæˆæ€§èƒ½ï¼ˆå¹¶å‘å¤„ç†ï¼‰\n",
    "2. æ‰©å±•åˆ°æ›´å¤š Agent-OM æ–‡ä»¶\n",
    "3. å®æ–½äººå·¥å®¡æ ¸æµç¨‹\n",
    "4. å¼€å§‹ Qwen 2.5 æ¨¡å‹å¾®è°ƒ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
