{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265eac91",
   "metadata": {},
   "source": [
    "# Agent-OM è®­ç»ƒæ•°æ®ç”Ÿæˆ - Gemini API æµ‹è¯•\n",
    "\n",
    "æœ¬ Notebook è®°å½•äº†ä½¿ç”¨ Google Gemini API ä¸º Agent-OM æœ¬ä½“åŒ¹é…é¡¹ç›®ç”Ÿæˆè®­ç»ƒæ•°æ®çš„å®Œæ•´æµ‹è¯•è¿‡ç¨‹ã€‚\n",
    "\n",
    "## æµ‹è¯•ç›®æ ‡\n",
    "\n",
    "1. âœ… éªŒè¯ Gemini API è¿æ¥\n",
    "2. âœ… æµ‹è¯•åŸºç¡€æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›\n",
    "3. âœ… ç”Ÿæˆç»“æ„åŒ– Q&A å¯¹ï¼ˆå¸¦æ¨ç†è½¨è¿¹ï¼‰\n",
    "4. âœ… ç”Ÿæˆè®¾è®¡è§£å†³æ–¹æ¡ˆ\n",
    "5. âœ… é’ˆå¯¹ Agent-OM é¡¹ç›®ç”Ÿæˆå®é™…è®­ç»ƒæ ·æœ¬\n",
    "\n",
    "**æµ‹è¯•æ—¥æœŸ**: 2025å¹´12æœˆ18æ—¥  \n",
    "**LLM Provider**: Google Gemini 2.5 Flash  \n",
    "**ç›®æ ‡é¡¹ç›®**: Agent-OM (Ontology Matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56d2fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ\n",
      "ğŸ”‘ Gemini API Key: å·²é…ç½®\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ° sys.path\n",
    "project_path = Path.cwd()\n",
    "sys.path.insert(0, str(project_path))\n",
    "\n",
    "# å¯¼å…¥é¡¹ç›®æ¨¡å—\n",
    "from src.llm_service import LLMService\n",
    "from src.schema import CodeContext, QAPair, DesignSolution\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ\")\n",
    "print(f\"ğŸ”‘ Gemini API Key: {'å·²é…ç½®' if os.getenv('GEMINI_API_KEY') else 'æœªé…ç½®'}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4547de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ æ­£åœ¨åˆå§‹åŒ– Gemini LLM æœåŠ¡...\n",
      "âœ… Gemini æœåŠ¡åˆå§‹åŒ–æˆåŠŸ\n",
      "   Provider: gemini\n",
      "   Model: gemini-2.5-flash\n",
      "   Temperature: 0.3\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”§ æ­£åœ¨åˆå§‹åŒ– Gemini LLM æœåŠ¡...\")\n",
    "\n",
    "try:\n",
    "    # é™ä½ temperature ä»¥è·å¾—æ›´ç¨³å®šçš„ JSON è¾“å‡º\n",
    "    llm_service = LLMService(\n",
    "        provider=\"gemini\", \n",
    "        model=\"gemini-2.5-flash\", \n",
    "        temperature=0.3  # ä» 0.7 é™ä½åˆ° 0.3ï¼Œæé«˜æ ¼å¼ç¨³å®šæ€§\n",
    "    )\n",
    "    print(\"âœ… Gemini æœåŠ¡åˆå§‹åŒ–æˆåŠŸ\")\n",
    "    print(f\"   Provider: {llm_service.provider}\")\n",
    "    print(f\"   Model: {llm_service.model}\")\n",
    "    print(f\"   Temperature: {llm_service.temperature}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42477ff",
   "metadata": {},
   "source": [
    "## 3. æµ‹è¯• 1ï¼šåŸºç¡€æ–‡æœ¬è¡¥å…¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a51abde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ æç¤ºè¯: ä»€ä¹ˆæ˜¯æœ¬ä½“åŒ¹é…ï¼ˆOntology Matchingï¼‰ï¼Ÿè¯·ç”¨2-3å¥è¯è§£é‡Šå…¶åœ¨è®¡ç®—æœºç§‘å­¦ä¸­çš„æ„ä¹‰ã€‚\n",
      "\n",
      "â³ æ­£åœ¨è°ƒç”¨ Gemini API...\n",
      "âœ… ç”ŸæˆæˆåŠŸ!\n",
      "\n",
      "======================================================================\n",
      "å›ç­”:\n",
      "æœ¬ä½“åŒ¹é…ï¼ˆOntology Matchingï¼‰æ˜¯æŒ‡è¯†åˆ«å’Œå»ºç«‹ä¸åŒæœ¬ä½“ä¹‹é—´è¯­ä¹‰å¯¹åº”å…³ç³»ï¼ˆå¦‚ç­‰ä»·ã€åŒ…å«ç­‰ï¼‰çš„è¿‡ç¨‹ï¼Œå³ä½¿å®ƒä»¬ä½¿ç”¨ä¸åŒçš„æœ¯è¯­æˆ–ç»“æ„æ¥æè¿°ç›¸ä¼¼çš„æ¦‚å¿µã€‚\n",
      "\n",
      "åœ¨è®¡ç®—æœºç§‘å­¦ä¸­ï¼Œå…¶æ„ä¹‰åœ¨äºå®ç°å¼‚æ„æ•°æ®æºçš„äº’æ“ä½œæ€§å’ŒçŸ¥è¯†é›†æˆï¼Œè¿›è€Œæ¨åŠ¨è¯­ä¹‰ç½‘ã€å¤§æ•°æ®é›†æˆå’Œäººå·¥æ™ºèƒ½ç­‰é¢†åŸŸçš„ä¿¡æ¯å…±äº«ä¸æ™ºèƒ½å¤„ç†ã€‚\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "prompt = \"ä»€ä¹ˆæ˜¯æœ¬ä½“åŒ¹é…ï¼ˆOntology Matchingï¼‰ï¼Ÿè¯·ç”¨2-3å¥è¯è§£é‡Šå…¶åœ¨è®¡ç®—æœºç§‘å­¦ä¸­çš„æ„ä¹‰ã€‚\"\n",
    "\n",
    "print(\"ğŸ“ æç¤ºè¯:\", prompt)\n",
    "print(\"\\nâ³ æ­£åœ¨è°ƒç”¨ Gemini API...\")\n",
    "\n",
    "try:\n",
    "    response = llm_service.generate_completion(prompt)\n",
    "    print(\"âœ… ç”ŸæˆæˆåŠŸ!\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"å›ç­”:\")\n",
    "    print(response)\n",
    "    print(\"=\" * 70)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç”Ÿæˆå¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6fa4a",
   "metadata": {},
   "source": [
    "## 4. æµ‹è¯• 2ï¼šç”Ÿæˆ Q&A å¯¹ï¼ˆå¸¦ä»£ç ä¸Šä¸‹æ–‡ï¼‰\n",
    "\n",
    "### ğŸ“‹ æµ‹è¯•ç›®çš„\n",
    "\n",
    "éªŒè¯ Gemini èƒ½å¦åŸºäºä»£ç ç‰‡æ®µç”Ÿæˆé«˜è´¨é‡çš„ç»“æ„åŒ–é—®ç­”æ•°æ®ï¼Œè¿™æ˜¯è®­ç»ƒæ•°æ®ç”Ÿæˆçš„æ ¸å¿ƒèƒ½åŠ›ã€‚\n",
    "\n",
    "\n",
    "### ğŸ¯ æµ‹è¯•ä»»åŠ¡- æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶\n",
    "\n",
    "**è¾“å…¥**ï¼šä¸€æ®µæœ¬ä½“åŒ¹é…å‡½æ•°ä»£ç ï¼ˆ`match_ontologies`ï¼‰  æ­¤æµ‹è¯•éªŒè¯äº†ä¸ºä»£ç åº“ç”ŸæˆQ&Aè®­ç»ƒæ•°æ®çš„å¯è¡Œæ€§ï¼ŒæˆåŠŸåå¯æ‰©å±•åˆ°ï¼š\n",
    "\n",
    "**è¾“å‡º**ï¼šJSONæ ¼å¼çš„é—®ç­”å¯¹ï¼ŒåŒ…å«ï¼š### ğŸ’¡ å®é™…åº”ç”¨ä»·å€¼\n",
    "\n",
    "- `question`: å…³äºä»£ç çš„æŠ€æœ¯é—®é¢˜\n",
    "\n",
    "- `answer`: è¯¦ç»†ç­”æ¡ˆ\n",
    "\n",
    "- `reasoning_steps`: 3-5ä¸ªæ¨ç†æ­¥éª¤-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977feae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ æ­£åœ¨ç”Ÿæˆ Q&A å¯¹...\n",
      "âœ… Q&A ç”ŸæˆæˆåŠŸ!\n",
      "\n",
      "======================================================================\n",
      "é—®é¢˜:\n",
      "The `match_ontologies` function employs a straightforward pairwise comparison approach. From an ontology matching and large language model (LLM) perspective, what are the primary characteristics, potential performance bottlenecks, and the critical role of the `compute_similarity` function in this design, especially when dealing with large or complex ontologies?\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ç­”æ¡ˆ:\n",
      "The `match_ontologies` function implements a fundamental, exhaustive, and unsupervised ontology matching strategy. Its primary characteristics include simplicity and transparency, as it attempts to find matches for every possible pair of entities. However, this design introduces significant performa...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "æ¨ç†æ­¥éª¤æ•°: 4\n",
      "  1. 1.  **Analyze Algorithm Characteristics:** Identify the code's core logic as a b...\n",
      "  2. 2.  **Identify Performance Bottleneck:** Determine the time complexity. The nest...\n",
      "  3. 3.  **Elaborate on `compute_similarity`'s Critical Role:** Explain that `compute...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ç¤ºä¾‹ä»£ç ï¼šæœ¬ä½“åŒ¹é…å‡½æ•°\n",
    "code_sample = \"\"\"\n",
    "def match_ontologies(source_onto, target_onto, threshold=0.8):\n",
    "    \\\"\\\"\\\"\n",
    "    ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…æºæœ¬ä½“å’Œç›®æ ‡æœ¬ä½“ä¹‹é—´çš„å®ä½“\n",
    "    \n",
    "    Args:\n",
    "        source_onto: æºæœ¬ä½“å¯¹è±¡\n",
    "        target_onto: ç›®æ ‡æœ¬ä½“å¯¹è±¡\n",
    "        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.8ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "        matches: åŒ¹é…å¯¹åˆ—è¡¨ï¼Œæ¯é¡¹ä¸º (source_entity, target_entity, similarity)\n",
    "    \\\"\\\"\\\"\n",
    "    matches = []\n",
    "    for source_entity in source_onto.entities:\n",
    "        for target_entity in target_onto.entities:\n",
    "            similarity = compute_similarity(source_entity, target_entity)\n",
    "            if similarity >= threshold:\n",
    "                matches.append((source_entity, target_entity, similarity))\n",
    "    return matches\n",
    "\"\"\"\n",
    "\n",
    "# æ„å»ºæç¤ºè¯ \n",
    "qa_prompt = f\"\"\"ä½ æ˜¯æœ¬ä½“åŒ¹é…å’Œå¤§è¯­è¨€æ¨¡å‹é¢†åŸŸçš„ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹ä»£ç ç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„é—®ç­”å¯¹ï¼š\n",
    "\n",
    "ä»£ç :\n",
    "```python\n",
    "{code_sample}\n",
    "```\n",
    "\n",
    "è¦æ±‚:\n",
    "1. é—®é¢˜è¦æµ‹è¯•å¯¹ä»£ç åŠŸèƒ½å’Œè®¾è®¡çš„ç†è§£\n",
    "2. ç­”æ¡ˆè¦è¯¦ç»†ä¸”åŒ…å«æ¨ç†è¿‡ç¨‹\n",
    "3. æä¾› 3-5 ä¸ªæ¨ç†æ­¥éª¤\n",
    "\n",
    "é‡è¦ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼å›ç­”ï¼Œç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²ä¸­çš„å¼•å·éƒ½è¢«æ­£ç¡®è½¬ä¹‰ã€‚\n",
    "ä¸è¦ä½¿ç”¨ markdown ä»£ç å—æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ã€‚\n",
    "\n",
    "æ ¼å¼ç¤ºä¾‹:\n",
    "{{\"question\": \"é—®é¢˜å†…å®¹\", \"answer\": \"ç­”æ¡ˆå†…å®¹\", \"reasoning_steps\": [\"æ­¥éª¤1\", \"æ­¥éª¤2\"]}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"â³ æ­£åœ¨ç”Ÿæˆ Q&A å¯¹...\")\n",
    "\n",
    "try:\n",
    "    response = llm_service.generate_completion(qa_prompt)\n",
    "    \n",
    "    # å¤šå±‚æ¸…ç†ç­–ç•¥\n",
    "    response_clean = response.strip()\n",
    "    \n",
    "    # 1. ç§»é™¤ markdown ä»£ç å—æ ‡è®°\n",
    "    response_clean = re.sub(r'^```json\\s*', '', response_clean)\n",
    "    response_clean = re.sub(r'^```\\s*', '', response_clean)\n",
    "    response_clean = re.sub(r'\\s*```$', '', response_clean)\n",
    "    \n",
    "    # 2. ç§»é™¤å¯èƒ½çš„å‰å¯¼/å°¾éšç©ºç™½\n",
    "    response_clean = response_clean.strip()\n",
    "    \n",
    "    # å°è¯•è§£æ JSON\n",
    "    try:\n",
    "        qa_data = json.loads(response_clean)\n",
    "    except json.JSONDecodeError as json_err:\n",
    "        print(f\"âš ï¸  åˆæ¬¡ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤...\")\n",
    "        print(f\"   é”™è¯¯: {json_err}\")\n",
    "        \n",
    "        # å°è¯•ä½¿ç”¨ ast.literal_eval æˆ–å…¶ä»–æ–¹æ³•\n",
    "        # æ˜¾ç¤ºæ›´å¤šè°ƒè¯•ä¿¡æ¯\n",
    "        print(f\"\\nğŸ“‹ åŸå§‹å“åº”å‰ 500 å­—ç¬¦:\")\n",
    "        print(response[:500])\n",
    "        print(f\"\\nğŸ“‹ æ¸…ç†åå“åº”å‰ 500 å­—ç¬¦:\")\n",
    "        print(response_clean[:500])\n",
    "        \n",
    "        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥è¿›å…¥å¤–å±‚å¼‚å¸¸å¤„ç†\n",
    "        raise json_err\n",
    "    \n",
    "    print(\"âœ… Q&A ç”ŸæˆæˆåŠŸ!\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"é—®é¢˜:\")\n",
    "    print(qa_data.get('question', 'N/A'))\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"ç­”æ¡ˆ:\")\n",
    "    answer = qa_data.get('answer', 'N/A')\n",
    "    print(answer[:300] + (\"...\" if len(answer) > 300 else \"\"))\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"æ¨ç†æ­¥éª¤æ•°: {len(qa_data.get('reasoning_steps', []))}\")\n",
    "    for i, step in enumerate(qa_data.get('reasoning_steps', [])[:3], 1):\n",
    "        print(f\"  {i}. {step[:80]}{'...' if len(step) > 80 else ''}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\nâŒ JSON è§£æå¤±è´¥: {e}\")\n",
    "    print(f\"   ä½ç½®: line {e.lineno}, column {e.colno}\")\n",
    "    print(f\"\\nğŸ’¡ å»ºè®®:\")\n",
    "    print(\"   1. Gemini è¿”å›çš„ JSON å¯èƒ½åŒ…å«æœªè½¬ä¹‰çš„ç‰¹æ®Šå­—ç¬¦\")\n",
    "    print(\"   2. å¯ä»¥å°è¯•é‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼\")\n",
    "    print(\"   3. æˆ–ä¿®æ”¹æç¤ºè¯è¦æ±‚æ›´ç®€æ´çš„å›ç­”\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    print(\"\\nğŸ“‹ å®Œæ•´é”™è¯¯å †æ ˆ:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e58e54",
   "metadata": {},
   "source": [
    "## 5. æµ‹è¯• 3ï¼šç”Ÿæˆè®¾è®¡è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "### ğŸ“‹ æµ‹è¯•ç›®çš„\n",
    "\n",
    "éªŒè¯ Gemini èƒ½å¦åŸºäºéœ€æ±‚æè¿°ç”Ÿæˆç³»ç»Ÿçº§çš„æ¶æ„è®¾è®¡æ–¹æ¡ˆï¼Œæµ‹è¯•å…¶åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸçš„åº”ç”¨èƒ½åŠ›ã€‚\n",
    "\n",
    "\n",
    "### ğŸ¯ æµ‹è¯•ä»»åŠ¡- è¾…åŠ©æ¶æ„è¯„å®¡\n",
    "\n",
    "**è¾“å…¥**ï¼šåŠŸèƒ½éœ€æ±‚æè¿°ï¼ˆä¾‹ï¼š\"ä¸ºæœ¬ä½“åŒ¹é…ç³»ç»Ÿæ·»åŠ æ‰¹é‡å¤„ç†æ”¯æŒï¼Œä»¥å¤„ç†åŒ…å«10,000+å®ä½“çš„å¤§è§„æ¨¡æ•°æ®é›†\"ï¼‰  \n",
    "\n",
    "**è¾“å‡º**ï¼šJSONæ ¼å¼çš„è®¾è®¡æ–¹æ¡ˆï¼ŒåŒ…å«ï¼šæˆåŠŸåå¯ç”¨äºï¼š\n",
    "\n",
    "- `solution`: è§£å†³æ–¹æ¡ˆæ¦‚è¿°### ğŸš€ æ‰©å±•åº”ç”¨\n",
    "\n",
    "- `steps`: 4-6ä¸ªå®æ–½æ­¥éª¤\n",
    "\n",
    "- `files`: éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨- **è¿­ä»£æ”¹è¿›**ï¼šæ ¹æ®è¾“å‡ºè´¨é‡è°ƒæ•´æç¤ºè¯\n",
    "\n",
    "- `challenges`: å…³é”®æŒ‘æˆ˜- **æ¨¡æ¿å¼•å¯¼**ï¼šæä¾›å…·ä½“ç¤ºä¾‹æ ¼å¼\n",
    "\n",
    "- **é•¿åº¦æ§åˆ¶**ï¼šé™åˆ¶æ¯ä¸ªå­—æ®µçš„å­—ç¬¦æ•°ï¼ˆå¦‚solution < 300å­—ç¬¦ï¼‰\n",
    "\n",
    "è®¾è®¡æ–¹æ¡ˆè¯„ä¼°ç»´åº¦- **Promptå·¥ç¨‹**ï¼šæä¾›é¡¹ç›®èƒŒæ™¯å’ŒæŠ€æœ¯æ ˆä¿¡æ¯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1ec426e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ æ­£åœ¨ç”Ÿæˆè®¾è®¡æ–¹æ¡ˆ...\n",
      "âœ… è®¾è®¡æ–¹æ¡ˆç”ŸæˆæˆåŠŸ!\n",
      "\n",
      "======================================================================\n",
      "éœ€æ±‚:\n",
      "ä¸ºæœ¬ä½“åŒ¹é…ç³»ç»Ÿæ·»åŠ æ‰¹é‡å¤„ç†æ”¯æŒï¼Œä»¥å¤„ç†åŒ…å« 10,000+ å®ä½“çš„å¤§è§„æ¨¡æ•°æ®é›†\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "è§£å†³æ–¹æ¡ˆ:\n",
      "å®æ–½ä¸€ä¸ªåˆ†å±‚æ‰¹å¤„ç†ç³»ç»Ÿï¼Œå°†å¤§è§„æ¨¡æ•°æ®é›†åˆ†è§£ä¸ºå¯ç®¡ç†çš„æ‰¹æ¬¡ã€‚è¿™äº›æ‰¹æ¬¡å°†ç”±æœ¬ä½“åŒ¹é…å¼•æ“å¼‚æ­¥å¤„ç†ï¼Œå¯èƒ½é€šè¿‡æ¶ˆæ¯é˜Ÿåˆ—æˆ–ä»»åŠ¡è°ƒåº¦å™¨è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚ä¸€ä¸ªç»“æœèšåˆæœåŠ¡å°†è´Ÿè´£æ”¶é›†å¹¶åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„å¤„ç†ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆçš„åŒ¹é…è¾“å‡ºã€‚\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "å®æ–½æ­¥éª¤ (6):\n",
      "  1. 1. **è®¾è®¡å¹¶å®ç°æ‰¹å¤„ç†è¾“å…¥å±‚ï¼š** åˆ›å»ºä¸€ä¸ªæ–°æ¨¡å—ï¼Œè´Ÿè´£æ¥æ”¶å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ ¹æ®é¢„å®šä¹‰æˆ–åŠ¨æ€é…ç½®çš„æ‰¹æ¬¡å¤§å°å°†å…¶åˆ†å‰²æˆæ›´å°çš„æ‰¹æ¬¡ï¼Œå¹¶å°†è¿™äº›æ‰¹æ¬¡æäº¤åˆ°å¤„ç†é˜Ÿåˆ—ã€‚\n",
      "  2. 2. **é›†æˆå¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—ç³»ç»Ÿï¼š** å¼•å…¥ä¸€ä¸ªæ¶ˆæ¯é˜Ÿåˆ—æˆ–ä»»åŠ¡ç®¡ç†ç³»ç»Ÿï¼ˆä¾‹å¦‚ï¼ŒCeleryã€Kafkaï¼‰æ¥ç®¡ç†æ‰¹æ¬¡çš„å¼‚æ­¥å¤„ç†ã€‚è¯¥ç³»ç»Ÿå°†åè°ƒå·¥ä½œèŠ‚ç‚¹å¤„ç†æ‰¹æ¬¡ï¼Œå¹¶æä¾›ä»»åŠ¡è°ƒåº¦ã€è´Ÿè½½å‡è¡¡å’Œå®¹é”™èƒ½åŠ›ã€‚\n",
      "  3. 3. **è°ƒæ•´æœ¬ä½“åŒ¹é…å¼•æ“ä»¥æ”¯æŒæ‰¹å¤„ç†ï¼š** ä¿®æ”¹ç°æœ‰çš„æœ¬ä½“åŒ¹é…ç®—æ³•å’Œæ ¸å¿ƒé€»è¾‘ï¼Œä½¿å…¶èƒ½å¤Ÿé«˜æ•ˆåœ°æ¥æ”¶å’Œå¤„ç†ä¸€ä¸ªå®ä½“æ‰¹æ¬¡ã€‚è¿™å¯èƒ½åŒ…æ‹¬ä¼˜åŒ–æ‰¹æ¬¡å†…å…±äº«è®¡ç®—ã€ç¼“å­˜å’Œèµ„æºåˆ©ç”¨ã€‚\n",
      "  4. 4. **å¼€å‘ç»“æœèšåˆæœåŠ¡ï¼š** æ„å»ºä¸€ä¸ªæœåŠ¡æ¥æ”¶é›†æ¥è‡ªæ‰€æœ‰å·²å¤„ç†æ‰¹æ¬¡çš„ç»“æœã€‚è¯¥æœåŠ¡éœ€è¦å¤„ç†æ‰¹æ¬¡å®Œæˆçš„ä¹±åºæƒ…å†µï¼Œå¹¶è´Ÿè´£å°†æ‰€æœ‰æ‰¹æ¬¡çš„åŒ¹é…ç»“æœåˆå¹¶æˆä¸€ä¸ªå®Œæ•´ä¸”ä¸€è‡´çš„æœ€ç»ˆè¾“å‡ºã€‚\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶ (7): src/batch/batch_processor.py, src/batch/result_aggregator.py, src/matching_engine/core_matcher.py, src/api/job_submission.py, config/settings.py\n",
      "  ... è¿˜æœ‰ 2 ä¸ªæ–‡ä»¶\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "å…³é”®æŒ‘æˆ˜ (5):\n",
      "  1. **å†…å­˜ä¸èµ„æºç®¡ç†ï¼š** åœ¨å¤„ç†å¤§è§„æ¨¡å®ä½“æ‰¹æ¬¡æ—¶ï¼Œæœ‰æ•ˆç®¡ç†å†…å­˜å’Œè®¡ç®—èµ„æºï¼Œé¿å…å†…å­˜æº¢å‡ºæˆ–æ€§èƒ½ç“¶é¢ˆã€‚\n",
      "  2. **æ€§èƒ½ä¸ååé‡ä¼˜åŒ–ï¼š** ç¡®ä¿æ‰¹å¤„ç†å¼•å…¥çš„å¼€é”€æœ€å°åŒ–ï¼Œå¹¶ä½¿ç³»ç»Ÿèƒ½å¤Ÿä»¥é«˜ååé‡åœ¨å¯æ¥å—çš„æ—¶é—´å†…å¤„ç†10,000+å®ä½“ã€‚\n",
      "  3. **æ•°æ®ä¸€è‡´æ€§ä¸å¹‚ç­‰æ€§ï¼š** ä¿è¯æ‰€æœ‰å®ä½“éƒ½è¢«ç²¾ç¡®å¤„ç†ä¸€æ¬¡ï¼Œå³ä½¿åœ¨é‡è¯•æˆ–å¹¶è¡Œæ‰§è¡Œçš„æƒ…å†µä¸‹ï¼Œç»“æœä¹Ÿä¿æŒä¸€è‡´å’Œæ­£ç¡®ã€‚\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "requirement = \"ä¸ºæœ¬ä½“åŒ¹é…ç³»ç»Ÿæ·»åŠ æ‰¹é‡å¤„ç†æ”¯æŒï¼Œä»¥å¤„ç†åŒ…å« 10,000+ å®ä½“çš„å¤§è§„æ¨¡æ•°æ®é›†\"\n",
    "\n",
    "design_prompt = f\"\"\"ä½ æ˜¯è½¯ä»¶æ¶æ„å¸ˆï¼Œè´Ÿè´£æœ¬ä½“åŒ¹é…ç³»ç»Ÿçš„è®¾è®¡ã€‚\n",
    "\n",
    "éœ€æ±‚: \"{requirement}\"\n",
    "\n",
    "è¯·ç”Ÿæˆä¸€ä¸ªè®¾è®¡è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬:\n",
    "- è§£å†³æ–¹æ¡ˆæ¦‚è¿°ï¼ˆç®€æ´æ˜äº†ï¼‰\n",
    "- 4-6 ä¸ªå®æ–½æ­¥éª¤\n",
    "- éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶åˆ—è¡¨\n",
    "- å…³é”®æŒ‘æˆ˜\n",
    "\n",
    "é‡è¦ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼å›ç­”ï¼Œç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²ä¸­çš„å¼•å·ã€æ‹¬å·ç­‰ç‰¹æ®Šå­—ç¬¦éƒ½è¢«æ­£ç¡®è½¬ä¹‰ã€‚\n",
    "ä¸è¦ä½¿ç”¨ markdown ä»£ç å—æ ‡è®°ã€‚ä¿æŒå›ç­”ç®€æ´ã€‚\n",
    "\n",
    "æ ¼å¼ç¤ºä¾‹:\n",
    "{{\"solution\": \"è§£å†³æ–¹æ¡ˆæè¿°\", \"steps\": [\"æ­¥éª¤1\", \"æ­¥éª¤2\"], \"files\": [\"file1.py\"], \"challenges\": [\"æŒ‘æˆ˜1\"]}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"â³ æ­£åœ¨ç”Ÿæˆè®¾è®¡æ–¹æ¡ˆ...\")\n",
    "\n",
    "try:\n",
    "    response = llm_service.generate_completion(design_prompt)\n",
    "    \n",
    "    # å¤šå±‚æ¸…ç†ç­–ç•¥\n",
    "    response_clean = response.strip()\n",
    "    response_clean = re.sub(r'^```json\\s*', '', response_clean)\n",
    "    response_clean = re.sub(r'^```\\s*', '', response_clean)\n",
    "    response_clean = re.sub(r'\\s*```$', '', response_clean)\n",
    "    response_clean = response_clean.strip()\n",
    "    \n",
    "    # å°è¯•è§£æ JSON\n",
    "    try:\n",
    "        design_data = json.loads(response_clean)\n",
    "    except json.JSONDecodeError as json_err:\n",
    "        print(f\"âš ï¸  åˆæ¬¡ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤...\")\n",
    "        print(f\"   é”™è¯¯: {json_err}\")\n",
    "        print(f\"\\nğŸ“‹ åŸå§‹å“åº”å‰ 500 å­—ç¬¦:\")\n",
    "        print(response[:500])\n",
    "        print(f\"\\nğŸ“‹ æ¸…ç†åå“åº”å‰ 500 å­—ç¬¦:\")\n",
    "        print(response_clean[:500])\n",
    "        \n",
    "        # é‡æ–°æŠ›å‡ºå¼‚å¸¸\n",
    "        raise json_err\n",
    "    \n",
    "    print(\"âœ… è®¾è®¡æ–¹æ¡ˆç”ŸæˆæˆåŠŸ!\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"éœ€æ±‚:\")\n",
    "    print(requirement)\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"è§£å†³æ–¹æ¡ˆ:\")\n",
    "    solution = design_data.get('solution', 'N/A')\n",
    "    print(solution[:250] + (\"...\" if len(solution) > 250 else \"\"))\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"å®æ–½æ­¥éª¤ ({len(design_data.get('steps', []))}):\")\n",
    "    for i, step in enumerate(design_data.get('steps', [])[:4], 1):\n",
    "        print(f\"  {i}. {step[:100]}{'...' if len(step) > 100 else ''}\")\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    files = design_data.get('files', [])\n",
    "    print(f\"éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶ ({len(files)}): {', '.join(files[:5])}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"  ... è¿˜æœ‰ {len(files) - 5} ä¸ªæ–‡ä»¶\")\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"å…³é”®æŒ‘æˆ˜ ({len(design_data.get('challenges', []))}):\")\n",
    "    for i, challenge in enumerate(design_data.get('challenges', [])[:3], 1):\n",
    "        print(f\"  {i}. {challenge[:80]}{'...' if len(challenge) > 80 else ''}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\nâŒ JSON è§£æå¤±è´¥: {e}\")\n",
    "    print(f\"   ä½ç½®: line {e.lineno}, column {e.colno}\")\n",
    "    print(f\"\\nğŸ’¡ å»ºè®®:\")\n",
    "    print(\"   1. Gemini è¿”å›çš„ JSON å¯èƒ½åŒ…å«æœªè½¬ä¹‰çš„ç‰¹æ®Šå­—ç¬¦\")\n",
    "    print(\"   2. å¯ä»¥å°è¯•é‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼\")\n",
    "    print(\"   3. æˆ–ç®€åŒ–éœ€æ±‚æè¿°\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    print(\"\\nğŸ“‹ å®Œæ•´é”™è¯¯å †æ ˆ:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed474b9a",
   "metadata": {},
   "source": [
    "## 6. Agent-OM é¡¹ç›®å®æˆ˜ï¼šç”Ÿæˆè®­ç»ƒæ ·æœ¬\n",
    "\n",
    "\n",
    "ä»ç†è®ºæµ‹è¯•è½¬å‘å®é™…åº”ç”¨ï¼Œä½¿ç”¨çœŸå®çš„ Agent-OM é¡¹ç›®ä»£ç ç”Ÿæˆå¯ç”¨äºæ¨¡å‹å¾®è°ƒçš„è®­ç»ƒæ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d5d1b2",
   "metadata": {},
   "source": [
    "## Q&A ç”Ÿæˆæ–¹æ³•è®º\n",
    "\n",
    "### 1ï¸âƒ£ å¦‚ä½•ç¡®å®šåº”è¯¥æœ‰å“ªäº›é—®é¢˜ï¼Ÿ\n",
    "\n",
    "**é—®é¢˜æ¥æºç­–ç•¥ï¼š**\n",
    "- **ä»£ç è¦†ç›–é©±åŠ¨**ï¼šéå†é¡¹ç›®å…³é”®æ–‡ä»¶ï¼Œä»ä¸åŒä»£ç ç‰‡æ®µç”Ÿæˆé—®é¢˜\n",
    "- **åŠŸèƒ½æ¨¡å—åˆ†ç±»**ï¼šé’ˆå¯¹ä¸åŒæ¨¡å—ï¼ˆå¦‚åŒ¹é…ç®—æ³•ã€å·¥å…·å‡½æ•°ã€é…ç½®ç®¡ç†ï¼‰ç”Ÿæˆå¯¹åº”é—®é¢˜\n",
    "- **éš¾åº¦å±‚æ¬¡åˆ’åˆ†**ï¼š\n",
    "  - åŸºç¡€ç†è§£ï¼ˆä»£ç åŠŸèƒ½ã€å‚æ•°å«ä¹‰ï¼‰\n",
    "  - æ·±åº¦åˆ†æï¼ˆç®—æ³•å¤æ‚åº¦ã€æ€§èƒ½ä¼˜åŒ–ï¼‰\n",
    "  - æ¶æ„è®¾è®¡ï¼ˆæ¨¡å—äº¤äº’ã€æ‰©å±•æ€§ï¼‰\n",
    "- **å®é™…åœºæ™¯æ¨¡æ‹Ÿ**ï¼šåŸºäºçœŸå®å¼€å‘ä¸­çš„è°ƒè¯•ã€ä¼˜åŒ–ã€é‡æ„åœºæ™¯\n",
    "\n",
    "**å½“å‰å®ç°ï¼š** éšæœºé‡‡æ ·ä»£ç ç‰‡æ®µï¼ˆ800å­—ç¬¦ï¼‰ï¼Œç¡®ä¿ä¸åŒæ–‡ä»¶éƒ½è¢«è¦†ç›–\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ å¦‚ä½•ç¡®å®šæ˜¯å¦å®Œå¤‡åœ°ç”Ÿæˆäº†æ‰€æœ‰é—®é¢˜ï¼Ÿ\n",
    "\n",
    "**å®Œå¤‡æ€§è¯„ä¼°ç»´åº¦ï¼š**\n",
    "\n",
    "| ç»´åº¦ | è¯„ä¼°æ–¹æ³• | ç›®æ ‡ |\n",
    "|------|----------|------|\n",
    "| **ä»£ç è¦†ç›–** | ç»Ÿè®¡å·²é‡‡æ ·çš„æ–‡ä»¶å’Œä»£ç è¡Œæ•° | ä¸»è¦æ–‡ä»¶è¦†ç›–ç‡ > 80% |\n",
    "| **åŠŸèƒ½è¦†ç›–** | æ£€æŸ¥æ ¸å¿ƒåŠŸèƒ½æ¨¡å—æ˜¯å¦éƒ½æœ‰å¯¹åº”é—®é¢˜ | æ¯ä¸ªæ ¸å¿ƒæ¨¡å— â‰¥ 3 ä¸ªé—®é¢˜ |\n",
    "| **çŸ¥è¯†ç‚¹è¦†ç›–** | åˆ†ç±»ç»Ÿè®¡ï¼ˆAPIä½¿ç”¨ã€æ•°æ®ç»“æ„ã€ç®—æ³•ç­‰ï¼‰ | å„ç±»çŸ¥è¯†ç‚¹å‡è¡¡åˆ†å¸ƒ |\n",
    "| **è´¨é‡å¤šæ ·æ€§** | äººå·¥å®¡æ ¸æ ·æœ¬è´¨é‡å’Œé—®é¢˜é‡å¤åº¦ | é‡å¤ç‡ < 10% |\n",
    "\n",
    "**å®é™…è€ƒé‡ï¼š**\n",
    "- âš ï¸ **å®Œå…¨å®Œå¤‡ä¸ç°å®**ï¼šä»£ç åº“æŒç»­æ¼”è¿›ï¼Œé—®é¢˜ç©ºé—´æ— é™\n",
    "- âœ… **è®¾å®šåˆç†ç›®æ ‡**ï¼šä¾‹å¦‚\"è¦†ç›–å‰10ä¸ªæ ¸å¿ƒæ–‡ä»¶ï¼Œç”Ÿæˆ50ä¸ªé«˜è´¨é‡é—®ç­”\"\n",
    "- ğŸ”„ **è¿­ä»£ä¼˜åŒ–**ï¼šæ ¹æ®æ¨¡å‹è®­ç»ƒæ•ˆæœè°ƒæ•´é‡‡æ ·ç­–ç•¥\n",
    "\n",
    "**å½“å‰å®ç°ï¼š** ç›®æ ‡ç”Ÿæˆ5ä¸ªQ&Aå¯¹ï¼Œè¦†ç›–2ä¸ªæ ¸å¿ƒæ–‡ä»¶ï¼ˆllm_matching.pyã€util.pyï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ æ˜¯å¦å¯ä»¥æœ‰é€‰æ‹©åå¥½ï¼Ÿ\n",
    "\n",
    "**æ”¯æŒçš„åå¥½æ§åˆ¶ï¼š**\n",
    "\n",
    "#### ğŸ“ æ–‡ä»¶ä¼˜å…ˆçº§\n",
    "```python\n",
    "# ç¤ºä¾‹ï¼šæŒ‡å®šé‡ç‚¹æ–‡ä»¶å’Œæƒé‡\n",
    "priority_files = {\n",
    "    \"llm_matching.py\": 0.5,    # 50% é‡‡æ ·æ¦‚ç‡\n",
    "    \"util.py\": 0.3,            # 30% é‡‡æ ·æ¦‚ç‡\n",
    "    \"config.py\": 0.2           # 20% é‡‡æ ·æ¦‚ç‡\n",
    "}\n",
    "```\n",
    "\n",
    "#### ğŸ¯ ä»£ç åŒºåŸŸé€‰æ‹©\n",
    "- é’ˆå¯¹ç‰¹å®šå‡½æ•°/ç±»ç”Ÿæˆé—®é¢˜ï¼ˆé€šè¿‡ä»£ç è§£æå®šä½ï¼‰\n",
    "- æ’é™¤æµ‹è¯•ä»£ç ã€é…ç½®æ–‡ä»¶ç­‰éæ ¸å¿ƒå†…å®¹\n",
    "- ä¼˜å…ˆé‡‡æ ·æ–‡æ¡£å­—ç¬¦ä¸²ä¸°å¯Œçš„ä»£ç æ®µ\n",
    "\n",
    "#### ğŸ” é—®é¢˜ç±»å‹åå¥½\n",
    "```python\n",
    "question_types = {\n",
    "    \"debugging\": 0.3,      # 30% è°ƒè¯•ç±»é—®é¢˜\n",
    "    \"optimization\": 0.3,   # 30% ä¼˜åŒ–ç±»é—®é¢˜  \n",
    "    \"design\": 0.2,         # 20% è®¾è®¡ç±»é—®é¢˜\n",
    "    \"basic\": 0.2          # 20% åŸºç¡€ç†è§£é—®é¢˜\n",
    "}\n",
    "```\n",
    "\n",
    "#### ğŸ“Š éš¾åº¦åˆ†å¸ƒæ§åˆ¶\n",
    "- ç®€å•ï¼ˆä»£ç ç›´æ¥ç†è§£ï¼‰ï¼š30%\n",
    "- ä¸­ç­‰ï¼ˆéœ€è¦æ¨ç†åˆ†æï¼‰ï¼š50%\n",
    "- å›°éš¾ï¼ˆæ¶æ„çº§æ€è€ƒï¼‰ï¼š20%\n",
    "\n",
    "**å®ç°æ–¹å¼å»ºè®®ï¼š**\n",
    "1. **é…ç½®æ–‡ä»¶**ï¼šä½¿ç”¨ YAML/JSON é…ç½®é‡‡æ ·ç­–ç•¥\n",
    "2. **æç¤ºè¯å·¥ç¨‹**ï¼šåœ¨promptä¸­æ˜ç¡®æŒ‡å®šé—®é¢˜ç±»å‹å’Œéš¾åº¦\n",
    "3. **åå¤„ç†è¿‡æ»¤**ï¼šç”Ÿæˆåæ ¹æ®è§„åˆ™ç­›é€‰å’Œå¹³è¡¡\n",
    "\n",
    "**å½“å‰å®ç°ï¼š** éšæœºé‡‡æ ·ï¼Œæœªå®ç°åå¥½æ§åˆ¶ï¼ˆå¯æ‰©å±•ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ å®è·µå»ºè®®\n",
    "\n",
    "1. **å°è§„æ¨¡æµ‹è¯•**ï¼šå…ˆç”Ÿæˆ10-20ä¸ªæ ·æœ¬ï¼Œäººå·¥è¯„ä¼°è´¨é‡\n",
    "2. **è¿­ä»£è°ƒä¼˜**ï¼šæ ¹æ®è¯„ä¼°ç»“æœè°ƒæ•´é‡‡æ ·ç­–ç•¥å’Œæç¤ºè¯\n",
    "3. **è´¨é‡ä¼˜å…ˆ**ï¼šå®å¯å°‘ç”Ÿæˆï¼Œç¡®ä¿æ¯ä¸ªé—®ç­”éƒ½é«˜è´¨é‡\n",
    "4. **äººå·¥å®¡æ ¸**ï¼šå…³é”®æ ·æœ¬éœ€äººå·¥å®¡æ ¸å’Œä¿®æ­£\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ å¦‚ä½•ç¡®ä¿æ•°æ®çš„å¤šæ ·æ€§å’Œä»£è¡¨æ€§ï¼Ÿ\n",
    "\n",
    "#### ğŸŒˆ å¤šæ ·æ€§ä¿éšœæœºåˆ¶\n",
    "\n",
    "**1. ä»£ç å±‚é¢çš„å¤šæ ·æ€§**\n",
    "\n",
    "| ç­–ç•¥ | å®ç°æ–¹æ³• | æ•ˆæœ |\n",
    "|------|----------|------|\n",
    "| **æ–‡ä»¶åˆ†æ•£é‡‡æ ·** | è½®æ¢ä¸åŒæºæ–‡ä»¶ï¼Œé¿å…é›†ä¸­åœ¨å•ä¸€æ–‡ä»¶ | å½“å‰ï¼šè½®æµä» llm_matching.py å’Œ util.py é‡‡æ · |\n",
    "| **ä»£ç ä½ç½®éšæœºåŒ–** | æ¯æ¬¡éšæœºé€‰æ‹©ä¸åŒä»£ç æ®µï¼ˆèµ·å§‹ä½ç½®éšæœºï¼‰ | é¿å…æ€»æ˜¯é‡‡æ ·æ–‡ä»¶å¼€å¤´ |\n",
    "| **é•¿åº¦å¤šæ ·åŒ–** | æ··åˆä½¿ç”¨ä¸åŒé•¿åº¦ä»£ç ç‰‡æ®µï¼ˆ400-1200å­—ç¬¦ï¼‰ | ç”Ÿæˆä¸åŒç²’åº¦çš„é—®é¢˜ |\n",
    "| **ä»£ç ç»“æ„å·®å¼‚** | é‡‡æ ·å‡½æ•°ã€ç±»ã€æ¨¡å—ä¸åŒå±‚çº§ä»£ç  | è¦†ç›–ä¸åŒæŠ½è±¡å±‚æ¬¡ |\n",
    "\n",
    "**å½“å‰å®ç°ï¼š**\n",
    "```python\n",
    "# Cell 15 ä¸­çš„å®ç°\n",
    "if len(code) > 800:\n",
    "    import random\n",
    "    start = random.randint(0, len(code) - 800)  # éšæœºèµ·å§‹ä½ç½®\n",
    "    code_snippet = code[start:start + 800]\n",
    "    \n",
    "# æ–‡ä»¶è½®æ¢\n",
    "file_index = (file_index + 1) % len(available_files)\n",
    "```\n",
    "\n",
    "**2. é—®é¢˜ç±»å‹çš„å¤šæ ·æ€§**\n",
    "\n",
    "é€šè¿‡æç¤ºè¯å·¥ç¨‹å¼•å¯¼ç”Ÿæˆä¸åŒç±»å‹é—®é¢˜ï¼š\n",
    "\n",
    "```python\n",
    "# å¯æ‰©å±•çš„æç¤ºè¯æ¨¡æ¿\n",
    "prompt_templates = {\n",
    "    \"functionality\": \"è¿™æ®µä»£ç å®ç°äº†ä»€ä¹ˆåŠŸèƒ½ï¼Ÿä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ\",\n",
    "    \"debugging\": \"å¦‚æœè¿™æ®µä»£ç å‡ºç°Xé”™è¯¯ï¼Œå¦‚ä½•æ’æŸ¥å’Œä¿®å¤ï¼Ÿ\",\n",
    "    \"optimization\": \"è¿™æ®µä»£ç æœ‰å“ªäº›æ€§èƒ½ç“¶é¢ˆï¼Ÿå¦‚ä½•ä¼˜åŒ–ï¼Ÿ\",\n",
    "    \"extension\": \"å¦‚æœè¦æ·»åŠ YåŠŸèƒ½ï¼Œåº”è¯¥å¦‚ä½•æ‰©å±•è¿™æ®µä»£ç ï¼Ÿ\",\n",
    "    \"comparison\": \"è¿™ç§å®ç°æ–¹å¼ç›¸æ¯”å…¶ä»–æ–¹æ¡ˆæœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ\"\n",
    "}\n",
    "```\n",
    "\n",
    "**3. éš¾åº¦æ¢¯åº¦åˆ†å¸ƒ**\n",
    "\n",
    "```python\n",
    "# å»ºè®®çš„éš¾åº¦åˆ†å¸ƒç­–ç•¥\n",
    "difficulty_distribution = {\n",
    "    \"basic\": {\n",
    "        \"ratio\": 0.3,\n",
    "        \"keywords\": [\"æ˜¯ä»€ä¹ˆ\", \"å®šä¹‰\", \"ç”¨é€”\", \"å‚æ•°å«ä¹‰\"],\n",
    "        \"target\": \"æ–°æ‰‹ç†è§£ä»£ç è¡¨å±‚åŠŸèƒ½\"\n",
    "    },\n",
    "    \"intermediate\": {\n",
    "        \"ratio\": 0.5,\n",
    "        \"keywords\": [\"ä¸ºä»€ä¹ˆ\", \"å¦‚ä½•å®ç°\", \"å·¥ä½œåŸç†\", \"æ•°æ®æµ\"],\n",
    "        \"target\": \"ä¸­çº§å¼€å‘è€…æ·±å…¥ç†è§£é€»è¾‘\"\n",
    "    },\n",
    "    \"advanced\": {\n",
    "        \"ratio\": 0.2,\n",
    "        \"keywords\": [\"æ¶æ„æƒè¡¡\", \"æ€§èƒ½ä¼˜åŒ–\", \"å¯æ‰©å±•æ€§\", \"æœ€ä½³å®è·µ\"],\n",
    "        \"target\": \"é«˜çº§å¼€å‘è€…ç³»ç»Ÿæ€§æ€è€ƒ\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### ğŸ¯ ä»£è¡¨æ€§ä¿éšœç­–ç•¥\n",
    "\n",
    "**1. æ ¸å¿ƒåŠŸèƒ½ä¼˜å…ˆè¦†ç›–**\n",
    "\n",
    "```python\n",
    "# åŸºäºé¡¹ç›®ç»“æ„çš„ä»£è¡¨æ€§é‡‡æ ·\n",
    "core_modules = {\n",
    "    \"llm_matching.py\": {\n",
    "        \"priority\": \"high\",\n",
    "        \"functions\": [\"match_entities\", \"compute_similarity\", \"batch_process\"],\n",
    "        \"coverage_target\": 5  # è‡³å°‘5ä¸ªç›¸å…³é—®é¢˜\n",
    "    },\n",
    "    \"util.py\": {\n",
    "        \"priority\": \"medium\", \n",
    "        \"functions\": [\"data_preprocessing\", \"result_formatting\"],\n",
    "        \"coverage_target\": 3\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**2. çœŸå®åœºæ™¯å¯¹é½**\n",
    "\n",
    "ç¡®ä¿ç”Ÿæˆçš„é—®é¢˜åæ˜ å®é™…å¼€å‘åœºæ™¯ï¼š\n",
    "\n",
    "- âœ… **å¸¸è§ä»»åŠ¡**ï¼šæ—¥å¸¸å¼€å‘ä¸­70%çš„å·¥ä½œï¼ˆå¦‚APIè°ƒç”¨ã€æ•°æ®å¤„ç†ï¼‰\n",
    "- âœ… **å…³é”®éš¾ç‚¹**ï¼š20%çš„å¤æ‚åœºæ™¯ï¼ˆå¦‚æ€§èƒ½ä¼˜åŒ–ã€è¾¹ç•Œæƒ…å†µå¤„ç†ï¼‰\n",
    "- âœ… **ç½•è§è¾¹ç•Œ**ï¼š10%çš„æç«¯æƒ…å†µï¼ˆç”¨äºå¥å£®æ€§æµ‹è¯•ï¼‰\n",
    "\n",
    "**3. çŸ¥è¯†ç‚¹å¹³è¡¡**\n",
    "\n",
    "```python\n",
    "# çŸ¥è¯†ç‚¹åˆ†ç±»ç»Ÿè®¡\n",
    "knowledge_coverage = {\n",
    "    \"APIä½¿ç”¨\": 0,      # è®¡æ•°å™¨\n",
    "    \"æ•°æ®ç»“æ„\": 0,\n",
    "    \"ç®—æ³•é€»è¾‘\": 0,\n",
    "    \"é”™è¯¯å¤„ç†\": 0,\n",
    "    \"æ€§èƒ½ä¼˜åŒ–\": 0,\n",
    "    \"æ¶æ„è®¾è®¡\": 0\n",
    "}\n",
    "\n",
    "# ç”Ÿæˆåæ£€æŸ¥åˆ†å¸ƒï¼Œå¿…è¦æ—¶é‡æ–°é‡‡æ ·è¡¥å……\n",
    "```\n",
    "\n",
    "#### ğŸ“Š è´¨é‡ç›‘æ§æŒ‡æ ‡\n",
    "\n",
    "**å®æ—¶ç›‘æ§ï¼ˆç”Ÿæˆè¿‡ç¨‹ä¸­ï¼‰ï¼š**\n",
    "- æ–‡ä»¶åˆ†å¸ƒç†µï¼š`H = -Î£(p_i * log(p_i))`ï¼Œè¶Šé«˜è¶Šåˆ†æ•£\n",
    "- ä»£ç é‡å ç‡ï¼šç›¸é‚»æ ·æœ¬ä»£ç é‡å åº¦ < 30%\n",
    "- é—®é¢˜ç›¸ä¼¼åº¦ï¼šä½¿ç”¨embeddingè®¡ç®—ï¼Œä½™å¼¦ç›¸ä¼¼åº¦ < 0.7\n",
    "\n",
    "**åå¤„ç†éªŒè¯ï¼š**\n",
    "```python\n",
    "# éªŒè¯å¤šæ ·æ€§çš„æ£€æŸ¥æ¸…å•\n",
    "diversity_checklist = {\n",
    "    \"file_coverage\": len(set([qa['source_file'] for qa in qa_pairs])) >= 2,\n",
    "    \"question_length_variance\": std_dev(question_lengths) > 20,\n",
    "    \"keyword_diversity\": len(unique_keywords) / total_keywords > 0.6,\n",
    "    \"no_exact_duplicates\": check_no_duplicates(qa_pairs)\n",
    "}\n",
    "```\n",
    "\n",
    "#### ğŸ”„ è¿­ä»£ä¼˜åŒ–æµç¨‹\n",
    "\n",
    "```\n",
    "ç”Ÿæˆåˆå§‹æ‰¹æ¬¡ â†’ åˆ†æåˆ†å¸ƒ â†’ è¯†åˆ«æ¬ é‡‡æ ·åŒºåŸŸ â†’ å®šå‘è¡¥å…… â†’ äººå·¥å®¡æ ¸ â†’ æ›´æ–°ç­–ç•¥\n",
    "     â†‘___________________________________________________________________|\n",
    "```\n",
    "\n",
    "**å½“å‰å®ç°å±€é™æ€§ï¼š**\n",
    "- âš ï¸ æ–‡ä»¶è¦†ç›–ä»…2ä¸ªï¼ˆå¯æ‰©å±•åˆ°5-10ä¸ªæ ¸å¿ƒæ–‡ä»¶ï¼‰\n",
    "- âš ï¸ æœªå®ç°é—®é¢˜ç±»å‹åˆ†ç±»ï¼ˆå¯é€šè¿‡æç¤ºè¯å˜åŒ–å®ç°ï¼‰\n",
    "- âš ï¸ ç¼ºå°‘è‡ªåŠ¨è´¨é‡æ£€æµ‹ï¼ˆå¯æ·»åŠ ç›¸ä¼¼åº¦è®¡ç®—ï¼‰\n",
    "\n",
    "**å¿«é€Ÿæ”¹è¿›æ–¹æ¡ˆï¼š**\n",
    "1. æ‰©å±• `target_files` åˆ—è¡¨åˆ°è‡³å°‘5ä¸ªæ ¸å¿ƒæ–‡ä»¶\n",
    "2. åˆ›å»ºå¤šæ ·åŒ–çš„ prompt æ¨¡æ¿åº“ï¼ˆ5-10ä¸ªä¸åŒè§’åº¦ï¼‰\n",
    "3. åœ¨å¾ªç¯ä¸­è·Ÿè¸ªå·²é‡‡æ ·çš„ä»£ç åŒºåŸŸï¼Œé¿å…é‡å¤\n",
    "4. æ·»åŠ åå¤„ç†å»é‡æ­¥éª¤ï¼ˆåŸºäºé—®é¢˜æ–‡æœ¬ç›¸ä¼¼åº¦ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8774b837",
   "metadata": {},
   "source": [
    "## ğŸ”— GitHub é¡¹ç›®é›†æˆ\n",
    "\n",
    "### ğŸ“‹ åŠŸèƒ½è¯´æ˜\n",
    "\n",
    "æœ¬ç³»ç»Ÿæ”¯æŒç›´æ¥ä»GitHubå…‹éš†é¡¹ç›®æˆ–è¯»å–ç°æœ‰æœ¬åœ°ä»“åº“ï¼Œå¯å¯¹ä»»æ„å¼€æºé¡¹ç›®ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚\n",
    "\n",
    "### ğŸ¯ æ”¯æŒçš„è¾“å…¥æ–¹å¼\n",
    "\n",
    "1. **GitHub URL** - è‡ªåŠ¨å…‹éš†è¿œç¨‹ä»“åº“\n",
    "2. **æœ¬åœ°è·¯å¾„** - ä½¿ç”¨å·²å­˜åœ¨çš„é¡¹ç›®ç›®å½•\n",
    "3. **æ··åˆæ¨¡å¼** - æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™å…‹éš†\n",
    "\n",
    "### ğŸ’» ä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "#### æ–¹å¼1ï¼šä»GitHub URLå…‹éš†é¡¹ç›®\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "def clone_or_use_repo(repo_url_or_path, target_dir=None):\n",
    "    \"\"\"\n",
    "    å…‹éš†GitHubä»“åº“æˆ–ä½¿ç”¨æœ¬åœ°è·¯å¾„\n",
    "    \n",
    "    Args:\n",
    "        repo_url_or_path: GitHub URL (å¦‚ https://github.com/user/repo) æˆ–æœ¬åœ°è·¯å¾„\n",
    "        target_dir: å…‹éš†ç›®æ ‡ç›®å½•ï¼ˆå¯é€‰ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "        é¡¹ç›®è·¯å¾„ (Pathå¯¹è±¡)\n",
    "    \"\"\"\n",
    "    # åˆ¤æ–­æ˜¯å¦ä¸ºGitHub URL\n",
    "    if repo_url_or_path.startswith(('https://github.com', 'git@github.com')):\n",
    "        # æå–ä»“åº“å\n",
    "        repo_name = repo_url_or_path.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "        \n",
    "        # è®¾ç½®ç›®æ ‡ç›®å½•\n",
    "        if target_dir is None:\n",
    "            target_dir = Path.home() / \"github_repos\" / repo_name\n",
    "        else:\n",
    "            target_dir = Path(target_dir) / repo_name\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨\n",
    "        if target_dir.exists():\n",
    "            print(f\"âœ… ä»“åº“å·²å­˜åœ¨: {target_dir}\")\n",
    "            return target_dir\n",
    "        \n",
    "        # å…‹éš†ä»“åº“\n",
    "        print(f\"ğŸ“¥ æ­£åœ¨å…‹éš†ä»“åº“: {repo_url_or_path}\")\n",
    "        target_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            ['git', 'clone', repo_url_or_path, str(target_dir)],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… å…‹éš†æˆåŠŸ: {target_dir}\")\n",
    "            return target_dir\n",
    "        else:\n",
    "            raise Exception(f\"å…‹éš†å¤±è´¥: {result.stderr}\")\n",
    "    \n",
    "    else:\n",
    "        # æœ¬åœ°è·¯å¾„\n",
    "        local_path = Path(repo_url_or_path)\n",
    "        if not local_path.exists():\n",
    "            raise FileNotFoundError(f\"è·¯å¾„ä¸å­˜åœ¨: {local_path}\")\n",
    "        print(f\"âœ… ä½¿ç”¨æœ¬åœ°è·¯å¾„: {local_path}\")\n",
    "        return local_path\n",
    "```\n",
    "\n",
    "#### æ–¹å¼2ï¼šå¿«é€Ÿé…ç½®\n",
    "\n",
    "```python\n",
    "# ç¤ºä¾‹1: ä½¿ç”¨GitHub URL\n",
    "github_url = \"https://github.com/username/project-name\"\n",
    "project_path = clone_or_use_repo(github_url)\n",
    "\n",
    "# ç¤ºä¾‹2: ä½¿ç”¨æœ¬åœ°è·¯å¾„\n",
    "local_path = \"/path/to/local/project\"\n",
    "project_path = clone_or_use_repo(local_path)\n",
    "\n",
    "# ç¤ºä¾‹3: æŒ‡å®šå…‹éš†ç›®å½•\n",
    "github_url = \"https://github.com/username/project-name\"\n",
    "project_path = clone_or_use_repo(github_url, target_dir=\"/custom/clone/dir\")\n",
    "```\n",
    "\n",
    "### ğŸ“š å¸¸è§GitHubé¡¹ç›®ç¤ºä¾‹\n",
    "\n",
    "```python\n",
    "# Pythoné¡¹ç›®\n",
    "examples = {\n",
    "    \"FastAPI\": \"https://github.com/tiangolo/fastapi\",\n",
    "    \"Django\": \"https://github.com/django/django\",\n",
    "    \"Flask\": \"https://github.com/pallets/flask\",\n",
    "    \"Requests\": \"https://github.com/psf/requests\",\n",
    "    \"Pandas\": \"https://github.com/pandas-dev/pandas\",\n",
    "    \"Scikit-learn\": \"https://github.com/scikit-learn/scikit-learn\",\n",
    "}\n",
    "\n",
    "# é€‰æ‹©ä¸€ä¸ªé¡¹ç›®\n",
    "selected_project = examples[\"FastAPI\"]\n",
    "project_path = clone_or_use_repo(selected_project)\n",
    "```\n",
    "\n",
    "### ğŸ”§ ä¸ç°æœ‰ä»£ç é›†æˆ\n",
    "\n",
    "åœ¨Cell 14å’ŒCell 15ä¸­æ›¿æ¢è·¯å¾„é…ç½®ï¼š\n",
    "\n",
    "```python\n",
    "# æ—§ç‰ˆï¼ˆå›ºå®šè·¯å¾„ï¼‰\n",
    "agent_om_path = Path(\"/Users/.../Agent-OM/ontology-llm\")\n",
    "\n",
    "# æ–°ç‰ˆï¼ˆæ”¯æŒGitHubï¼‰\n",
    "# é€‰é¡¹1: ç›´æ¥æŒ‡å®šGitHub URL\n",
    "repo_url = \"https://github.com/username/your-project\"\n",
    "project_path = clone_or_use_repo(repo_url)\n",
    "\n",
    "# é€‰é¡¹2: ä½¿ç”¨ç¯å¢ƒå˜é‡\n",
    "import os\n",
    "repo_path = os.getenv('TARGET_REPO', '/default/local/path')\n",
    "project_path = clone_or_use_repo(repo_path)\n",
    "```\n",
    "\n",
    "### âš™ï¸ é«˜çº§é…ç½®\n",
    "\n",
    "#### æŒ‡å®šå­ç›®å½•\n",
    "\n",
    "```python\n",
    "# å…‹éš†ååªä½¿ç”¨ç‰¹å®šå­ç›®å½•\n",
    "repo_url = \"https://github.com/username/monorepo\"\n",
    "base_path = clone_or_use_repo(repo_url)\n",
    "target_path = base_path / \"src\" / \"backend\"  # ä½¿ç”¨å­ç›®å½•\n",
    "```\n",
    "\n",
    "#### æµ…å…‹éš†ï¼ˆåŠ é€Ÿä¸‹è½½ï¼‰\n",
    "\n",
    "```python\n",
    "def shallow_clone_repo(repo_url, depth=1):\n",
    "    \"\"\"æµ…å…‹éš†ï¼Œåªä¸‹è½½æœ€è¿‘çš„æäº¤å†å²\"\"\"\n",
    "    repo_name = repo_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "    target_dir = Path.home() / \"github_repos\" / repo_name\n",
    "    \n",
    "    if not target_dir.exists():\n",
    "        target_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "        subprocess.run([\n",
    "            'git', 'clone', '--depth', str(depth), repo_url, str(target_dir)\n",
    "        ], check=True)\n",
    "    \n",
    "    return target_dir\n",
    "\n",
    "# ä½¿ç”¨\n",
    "project_path = shallow_clone_repo(\"https://github.com/user/large-repo\", depth=1)\n",
    "```\n",
    "\n",
    "#### æŒ‡å®šåˆ†æ”¯\n",
    "\n",
    "```python\n",
    "def clone_specific_branch(repo_url, branch='main'):\n",
    "    \"\"\"å…‹éš†æŒ‡å®šåˆ†æ”¯\"\"\"\n",
    "    repo_name = repo_url.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "    target_dir = Path.home() / \"github_repos\" / repo_name\n",
    "    \n",
    "    if not target_dir.exists():\n",
    "        target_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "        subprocess.run([\n",
    "            'git', 'clone', '-b', branch, repo_url, str(target_dir)\n",
    "        ], check=True)\n",
    "    \n",
    "    return target_dir\n",
    "```\n",
    "\n",
    "### âš ï¸ æ³¨æ„äº‹é¡¹\n",
    "\n",
    "1. **æƒé™è¦æ±‚**ï¼šç§æœ‰ä»“åº“éœ€è¦é…ç½®Gitå‡­æ®\n",
    "2. **ç½‘ç»œè¦æ±‚**ï¼šç¡®ä¿èƒ½è®¿é—®GitHubï¼ˆå›½å†…å¯èƒ½éœ€è¦ä»£ç†ï¼‰\n",
    "3. **ç£ç›˜ç©ºé—´**ï¼šå¤§å‹ä»“åº“å¯èƒ½éœ€è¦è¾ƒå¤§ç©ºé—´\n",
    "4. **gitå‘½ä»¤**ï¼šç¡®ä¿ç³»ç»Ÿå·²å®‰è£…git\n",
    "5. **è®¸å¯è¯**ï¼šéµå®ˆç›®æ ‡é¡¹ç›®çš„å¼€æºè®¸å¯è¯\n",
    "\n",
    "### ğŸ§ª éªŒè¯Gitå®‰è£…\n",
    "\n",
    "```python\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['git', '--version'], capture_output=True, text=True)\n",
    "    print(f\"âœ… Gitå·²å®‰è£…: {result.stdout.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ æœªå®‰è£…Gitï¼Œè¯·å…ˆå®‰è£…:\")\n",
    "    print(\"   macOS: brew install git\")\n",
    "    print(\"   Ubuntu: sudo apt-get install git\")\n",
    "    print(\"   Windows: ä» https://git-scm.com ä¸‹è½½å®‰è£…\")\n",
    "```\n",
    "\n",
    "### ğŸ’¡ æœ€ä½³å®è·µ\n",
    "\n",
    "1. **é¦–æ¬¡è¿è¡Œ**ï¼šå…ˆç”¨å°å‹é¡¹ç›®æµ‹è¯•ï¼ˆå¦‚ä¸ªäººå°ä»“åº“ï¼‰\n",
    "2. **å¤§å‹é¡¹ç›®**ï¼šä½¿ç”¨æµ…å…‹éš†å‡å°‘ä¸‹è½½æ—¶é—´\n",
    "3. **é¢‘ç¹ä½¿ç”¨**ï¼šå°†å¸¸ç”¨é¡¹ç›®å…‹éš†åˆ°æœ¬åœ°ï¼Œåç»­ç›´æ¥ä½¿ç”¨è·¯å¾„\n",
    "4. **å¤šé¡¹ç›®å¯¹æ¯”**ï¼šåœ¨åŒä¸€notebookä¸­å¤„ç†å¤šä¸ªé¡¹ç›®\n",
    "\n",
    "### ğŸš€ å®Œæ•´å·¥ä½œæµç¤ºä¾‹\n",
    "\n",
    "```python\n",
    "# Step 1: å‡†å¤‡å¤šä¸ªé¡¹ç›®\n",
    "projects = [\n",
    "    \"https://github.com/user/project1\",\n",
    "    \"/local/path/project2\",\n",
    "    \"https://github.com/user/project3\"\n",
    "]\n",
    "\n",
    "# Step 2: æ‰¹é‡å¤„ç†\n",
    "all_qa_pairs = []\n",
    "for project in projects:\n",
    "    project_path = clone_or_use_repo(project)\n",
    "    # è¿è¡ŒCell 14-15çš„ç”Ÿæˆé€»è¾‘\n",
    "    # qa_pairs = generate_qa_for_project(project_path)\n",
    "    # all_qa_pairs.extend(qa_pairs)\n",
    "\n",
    "# Step 3: åˆå¹¶ä¿å­˜\n",
    "# save_combined_dataset(all_qa_pairs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd68fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHubé›†æˆ - å®ç”¨å·¥å…·å‡½æ•°\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def clone_or_use_repo(repo_url_or_path, target_dir=None):\n",
    "    \"\"\"\n",
    "    å…‹éš†GitHubä»“åº“æˆ–ä½¿ç”¨æœ¬åœ°è·¯å¾„\n",
    "    \n",
    "    Args:\n",
    "        repo_url_or_path: GitHub URL æˆ–æœ¬åœ°è·¯å¾„\n",
    "        target_dir: å…‹éš†ç›®æ ‡ç›®å½•ï¼ˆå¯é€‰ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "        é¡¹ç›®è·¯å¾„ (Pathå¯¹è±¡)\n",
    "    \"\"\"\n",
    "    # åˆ¤æ–­æ˜¯å¦ä¸ºGitHub URL\n",
    "    if repo_url_or_path.startswith(('https://github.com', 'git@github.com', 'http://github.com')):\n",
    "        # æå–ä»“åº“å\n",
    "        repo_name = repo_url_or_path.rstrip('/').split('/')[-1].replace('.git', '')\n",
    "        \n",
    "        # è®¾ç½®ç›®æ ‡ç›®å½•\n",
    "        if target_dir is None:\n",
    "            target_dir = Path.home() / \"github_repos\" / repo_name\n",
    "        else:\n",
    "            target_dir = Path(target_dir) / repo_name\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨\n",
    "        if target_dir.exists():\n",
    "            print(f\"âœ… ä»“åº“å·²å­˜åœ¨: {target_dir}\")\n",
    "            print(f\"   å¦‚éœ€é‡æ–°å…‹éš†ï¼Œè¯·å…ˆåˆ é™¤ç›®å½•\")\n",
    "            return target_dir\n",
    "        \n",
    "        # å…‹éš†ä»“åº“\n",
    "        print(f\"ğŸ“¥ æ­£åœ¨å…‹éš†ä»“åº“: {repo_url_or_path}\")\n",
    "        print(f\"   ç›®æ ‡ç›®å½•: {target_dir}\")\n",
    "        target_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['git', 'clone', '--depth', '1', repo_url_or_path, str(target_dir)],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"âœ… å…‹éš†æˆåŠŸ: {target_dir}\")\n",
    "                return target_dir\n",
    "            else:\n",
    "                raise Exception(f\"å…‹éš†å¤±è´¥: {result.stderr}\")\n",
    "        \n",
    "        except subprocess.TimeoutExpired:\n",
    "            raise Exception(\"å…‹éš†è¶…æ—¶ï¼Œé¡¹ç›®å¯èƒ½å¤ªå¤§ï¼Œå»ºè®®æ‰‹åŠ¨å…‹éš†\")\n",
    "        except FileNotFoundError:\n",
    "            raise Exception(\"æœªæ‰¾åˆ°gitå‘½ä»¤ï¼Œè¯·å…ˆå®‰è£…Git\")\n",
    "    \n",
    "    else:\n",
    "        # æœ¬åœ°è·¯å¾„\n",
    "        local_path = Path(repo_url_or_path)\n",
    "        if not local_path.exists():\n",
    "            raise FileNotFoundError(f\"è·¯å¾„ä¸å­˜åœ¨: {local_path}\")\n",
    "        print(f\"âœ… ä½¿ç”¨æœ¬åœ°è·¯å¾„: {local_path}\")\n",
    "        return local_path\n",
    "\n",
    "\n",
    "# éªŒè¯Gitå®‰è£…\n",
    "print(\"ğŸ” æ£€æŸ¥Gitå®‰è£…...\")\n",
    "try:\n",
    "    result = subprocess.run(['git', '--version'], capture_output=True, text=True, timeout=5)\n",
    "    print(f\"âœ… {result.stdout.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ æœªå®‰è£…Git\")\n",
    "    print(\"   macOS: brew install git\")\n",
    "    print(\"   Ubuntu: sudo apt-get install git\")\n",
    "    print(\"   Windows: https://git-scm.com\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  æ£€æŸ¥å¤±è´¥: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:\")\n",
    "print(\"=\" * 70)\n",
    "print('# ç¤ºä¾‹1: GitHub URL')\n",
    "print('project_path = clone_or_use_repo(\"https://github.com/user/repo\")')\n",
    "print()\n",
    "print('# ç¤ºä¾‹2: æœ¬åœ°è·¯å¾„')\n",
    "print('project_path = clone_or_use_repo(\"/path/to/local/project\")')\n",
    "print()\n",
    "print('# ç¤ºä¾‹3: è‡ªå®šä¹‰å…‹éš†ç›®å½•')\n",
    "print('project_path = clone_or_use_repo(\"https://github.com/user/repo\", target_dir=\"/custom/dir\")')\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•GitHubé›†æˆ - é…ç½®ç›®æ ‡é¡¹ç›®\n",
    "\n",
    "# æ–¹å¼1: GitHub URLï¼ˆæ¨è - é€‚ç”¨äºä»»æ„å¼€æºé¡¹ç›®ï¼‰\n",
    "test_repo = \"https://github.com/qzc438-research/ontology-llm\"  # ç¤ºä¾‹ï¼šontology-llmé¡¹ç›®\n",
    "\n",
    "# æ–¹å¼2: å…¶ä»–GitHubé¡¹ç›®ç¤ºä¾‹\n",
    "# test_repo = \"https://github.com/tiangolo/fastapi\"  # FastAPI\n",
    "# test_repo = \"https://github.com/psf/requests\"      # Requestsåº“\n",
    "\n",
    "# æ–¹å¼3: ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼ˆå¦‚æœå·²æœ‰é¡¹ç›®ï¼‰\n",
    "# test_repo = \"/path/to/your/local/project\"\n",
    "\n",
    "# æ–¹å¼4: ä½¿ç”¨Agent-OMæœ¬åœ°é¡¹ç›®ï¼ˆåŸæœ‰æ–¹å¼ï¼‰\n",
    "# test_repo = \"/Users/xianhaoliu/Library/CloudStorage/OneDrive-Stibo/Project/Agent-OM/ontology-llm\"\n",
    "\n",
    "print(\"ğŸ¯ é…ç½®ç›®æ ‡é¡¹ç›®:\")\n",
    "print(f\"   {test_repo}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    project_path = clone_or_use_repo(test_repo)\n",
    "    \n",
    "    # éªŒè¯é¡¹ç›®ç»“æ„\n",
    "    print(\"\\nğŸ“‚ é¡¹ç›®ç»“æ„é¢„è§ˆ:\")\n",
    "    python_files = list(project_path.glob(\"**/*.py\"))[:10]  # æœ€å¤šæ˜¾ç¤º10ä¸ª\n",
    "    print(f\"   Pythonæ–‡ä»¶æ•°: {len(list(project_path.glob('**/*.py')))}\")\n",
    "    print(f\"   å‰10ä¸ªæ–‡ä»¶:\")\n",
    "    for f in python_files:\n",
    "        rel_path = f.relative_to(project_path)\n",
    "        print(f\"      - {rel_path}\")\n",
    "    \n",
    "    # ä¿å­˜ä¸ºå…¨å±€å˜é‡ä¾›åç»­ä½¿ç”¨\n",
    "    print(f\"\\nâœ… é¡¹ç›®è·¯å¾„å·²é…ç½®: {project_path}\")\n",
    "    print(\"   å¯åœ¨åç»­Cellä¸­ä½¿ç”¨ 'project_path' å˜é‡\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ é…ç½®å¤±è´¥: {e}\")\n",
    "    print(\"\\nğŸ’¡ æç¤º:\")\n",
    "    print(\"   1. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆGitHubè®¿é—®ï¼‰\")\n",
    "    print(\"   2. éªŒè¯ä»“åº“URLæ˜¯å¦æ­£ç¡®\")\n",
    "    print(\"   3. ç¡®ä¿æœ‰è¶³å¤Ÿç£ç›˜ç©ºé—´\")\n",
    "    print(\"   4. å¦‚æœæ˜¯ç§æœ‰ä»“åº“ï¼Œéœ€é…ç½®Gitå‡­æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a6a37",
   "metadata": {},
   "source": [
    "## âš–ï¸ æœ¬åœ°å…‹éš† vs GitHub APIç›´æ¥è¯»å–\n",
    "\n",
    "### ğŸ¯ æ ¸å¿ƒé—®é¢˜\n",
    "\n",
    "**\"ä»…ä»…è¾“å…¥ä¸€ä¸ªå¼€æºä»£ç åº“è€Œæ— éœ€æœ¬åœ°ä¸‹è½½ï¼Œæ˜¯å¦ä¼šå½±å“æœ€ç»ˆè¡¨ç°ï¼Ÿ\"**\n",
    "\n",
    "ç­”æ¡ˆï¼š**ä¼šæœ‰æ˜¾è‘—å½±å“ï¼Œå»ºè®®æ ¹æ®åœºæ™¯é€‰æ‹©æ–¹æ¡ˆ**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š ä¸¤ç§æ–¹æ¡ˆå¯¹æ¯”\n",
    "\n",
    "| å¯¹æ¯”ç»´åº¦ | æ–¹æ¡ˆA: æœ¬åœ°å…‹éš†ï¼ˆå½“å‰ï¼‰ | æ–¹æ¡ˆB: GitHub APIç›´æ¥è¯»å– |\n",
    "|---------|---------------------|------------------------|\n",
    "| **é€Ÿåº¦** | âš¡ å¿«ï¼ˆæœ¬åœ°I/Oï¼Œæ¯«ç§’çº§ï¼‰ | ğŸŒ æ…¢ï¼ˆç½‘ç»œè¯·æ±‚ï¼Œç§’çº§ï¼Œæœ‰é™æµï¼‰ |\n",
    "| **æˆåŠŸç‡** | âœ… é«˜ï¼ˆä¸å—ç½‘ç»œæ³¢åŠ¨å½±å“ï¼‰ | âš ï¸ ä¸­ï¼ˆä¾èµ–ç½‘ç»œç¨³å®šæ€§ï¼‰ |\n",
    "| **æ–‡ä»¶è®¿é—®** | âœ… å®Œæ•´ï¼ˆæ‰€æœ‰æ–‡ä»¶ï¼Œå«éšè—ï¼‰ | âš ï¸ å—é™ï¼ˆAPIé™åˆ¶ï¼Œå¤§æ–‡ä»¶å¯èƒ½å¤±è´¥ï¼‰ |\n",
    "| **ç£ç›˜å ç”¨** | âŒ éœ€è¦ï¼ˆ10MB-1GB+ï¼‰ | âœ… æ— éœ€ï¼ˆé›¶å ç”¨ï¼‰ |\n",
    "| **æ‰¹é‡å¤„ç†** | âœ… å¿«é€Ÿéå† | âŒ æ…¢ï¼ˆæ¯ä¸ªæ–‡ä»¶1æ¬¡APIè°ƒç”¨ï¼‰ |\n",
    "| **APIé™é¢** | âœ… æ— é™åˆ¶ | âŒ æœ‰é™åˆ¶ï¼ˆ5000æ¬¡/å°æ—¶ï¼‰ |\n",
    "| **ç¦»çº¿ä½¿ç”¨** | âœ… æ”¯æŒ | âŒ ä¸æ”¯æŒ |\n",
    "| **åˆå§‹å‡†å¤‡** | âŒ éœ€å…‹éš†ï¼ˆ10ç§’-5åˆ†é’Ÿï¼‰ | âœ… æ— éœ€ï¼ˆå³å¼€å³ç”¨ï¼‰ |\n",
    "| **ä»£ç å®Œæ•´æ€§** | âœ… 100%å®Œæ•´ | âš ï¸ å¯èƒ½ä¸å®Œæ•´ï¼ˆAPIé™åˆ¶ï¼‰ |\n",
    "| **å¤§å‹é¡¹ç›®** | âœ… å¯å¤„ç† | âŒ ä¸é€‚åˆï¼ˆé€Ÿåº¦æ…¢+APIé™é¢ï¼‰ |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” å…·ä½“å½±å“åˆ†æ\n",
    "\n",
    "#### 1ï¸âƒ£ **æ€§èƒ½å½±å“**\n",
    "\n",
    "**æœ¬åœ°å…‹éš†ï¼ˆå½“å‰æ–¹æ¡ˆï¼‰ï¼š**\n",
    "```python\n",
    "# è¯»å–100ä¸ªæ–‡ä»¶è€—æ—¶ï¼š~0.5ç§’\n",
    "for file in python_files[:100]:\n",
    "    content = file.read_text()  # æœ¬åœ°I/Oï¼Œæ¯«ç§’çº§\n",
    "```\n",
    "\n",
    "**GitHub APIæ–¹æ¡ˆï¼š**\n",
    "```python\n",
    "# è¯»å–100ä¸ªæ–‡ä»¶è€—æ—¶ï¼š~50-200ç§’\n",
    "for file in file_list[:100]:\n",
    "    response = requests.get(github_api_url)  # æ¯æ¬¡0.5-2ç§’\n",
    "    content = response.json()['content']\n",
    "```\n",
    "\n",
    "**ç»“è®ºï¼š** æœ¬åœ°æ–¹æ¡ˆå¿« **100-400å€**\n",
    "\n",
    "#### 2ï¸âƒ£ **APIé™é¢å½±å“**\n",
    "\n",
    "GitHub APIé™åˆ¶ï¼š\n",
    "- **æœªè®¤è¯**ï¼š60æ¬¡/å°æ—¶\n",
    "- **å·²è®¤è¯**ï¼š5000æ¬¡/å°æ—¶\n",
    "\n",
    "**å®é™…æ¡ˆä¾‹ï¼š**\n",
    "- è¯»å–50ä¸ªæ–‡ä»¶ = 50æ¬¡APIè°ƒç”¨\n",
    "- ç”Ÿæˆ5ä¸ªQ&Aï¼Œæ¯ä¸ªè¯»å–800å­—ç¬¦ä»£ç  = 5æ¬¡æ–‡ä»¶è¯»å–\n",
    "- å¦‚æœé¡¹ç›®æœ‰500ä¸ªPythonæ–‡ä»¶ï¼Œéå†ç»“æ„ = 1æ¬¡API + 500æ¬¡æ–‡ä»¶å…ƒæ•°æ®\n",
    "\n",
    "**ç»“è®ºï¼š** ä¸­å°å‹é¡¹ç›®å¯è¡Œï¼Œå¤§å‹é¡¹ç›®ä¼š**å¿«é€Ÿè€—å°½é™é¢**\n",
    "\n",
    "#### 3ï¸âƒ£ **å¯é æ€§å½±å“**\n",
    "\n",
    "**æœ¬åœ°å…‹éš†ï¼š**\n",
    "- âœ… ç½‘ç»œæ–­å¼€ä»å¯è¿è¡Œ\n",
    "- âœ… ä¸å—GitHubæœåŠ¡çŠ¶æ€å½±å“\n",
    "- âœ… å¯é‡å¤æ‰§è¡Œæ— é™æ¬¡\n",
    "\n",
    "**GitHub APIï¼š**\n",
    "- âŒ ç½‘ç»œä¸ç¨³å®šä¼šå¯¼è‡´å¤±è´¥\n",
    "- âŒ GitHubç»´æŠ¤æ—¶æ— æ³•ä½¿ç”¨\n",
    "- âŒ APIé™é¢è€—å°½åéœ€ç­‰å¾…1å°æ—¶\n",
    "\n",
    "**ç»“è®ºï¼š** æœ¬åœ°æ–¹æ¡ˆå¯é æ€§é«˜ **10å€ä»¥ä¸Š**\n",
    "\n",
    "#### 4ï¸âƒ£ **æ•°æ®è´¨é‡å½±å“**\n",
    "\n",
    "**å…³é”®å·®å¼‚ï¼š**\n",
    "\n",
    "| æ•°æ®ç‰¹æ€§ | æœ¬åœ°å…‹éš† | GitHub API |\n",
    "|---------|---------|-----------|\n",
    "| **ä»£ç å®Œæ•´æ€§** | 100% | 95-98%ï¼ˆå¤§æ–‡ä»¶å¯èƒ½æˆªæ–­ï¼‰ |\n",
    "| **é‡‡æ ·å¤šæ ·æ€§** | é«˜ï¼ˆå¿«é€Ÿéšæœºé‡‡æ ·ï¼‰ | ä½ï¼ˆå—APIé™åˆ¶ï¼‰ |\n",
    "| **æ–‡ä»¶è¦†ç›–** | å…¨éƒ¨æ–‡ä»¶ | å—é™ï¼ˆAPIæœ‰æ–‡ä»¶å¤§å°é™åˆ¶ï¼‰ |\n",
    "| **ä¸Šä¸‹æ–‡è¿ç»­** | å®Œæ•´ | å¯èƒ½ä¸è¿ç»­ |\n",
    "\n",
    "**å®é™…å½±å“ä¸¾ä¾‹ï¼š**\n",
    "```python\n",
    "# åœºæ™¯ï¼šç”Ÿæˆ5ä¸ªQ&Aå¯¹ï¼Œéœ€éšæœºé‡‡æ ·10ä¸ªä¸åŒä»£ç ä½ç½®\n",
    "# æœ¬åœ°æ–¹æ¡ˆï¼š0.2ç§’å®Œæˆï¼Œ100%æˆåŠŸ\n",
    "# APIæ–¹æ¡ˆï¼š10-30ç§’å®Œæˆï¼Œå¯èƒ½å› ç½‘ç»œé—®é¢˜å¤±è´¥\n",
    "```\n",
    "\n",
    "**ç»“è®ºï¼š** æœ¬åœ°æ–¹æ¡ˆè®­ç»ƒæ•°æ®è´¨é‡æ›´é«˜\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ æ–¹æ¡ˆé€‰æ‹©å»ºè®®\n",
    "\n",
    "#### âœ… **æ¨èæœ¬åœ°å…‹éš†çš„åœºæ™¯**ï¼ˆå½“å‰æ–¹æ¡ˆï¼‰\n",
    "\n",
    "1. **æ‰¹é‡ç”Ÿæˆè®­ç»ƒæ•°æ®**ï¼ˆæœ¬é¡¹ç›®çš„æ ¸å¿ƒåœºæ™¯ï¼‰\n",
    "   - éœ€è¦ç”Ÿæˆå¤§é‡Q&Aå¯¹ï¼ˆ>10ä¸ªï¼‰\n",
    "   - éœ€è¦é«˜æˆåŠŸç‡å’Œç¨³å®šæ€§\n",
    "   - ç£ç›˜ç©ºé—´å……è¶³\n",
    "\n",
    "2. **å¤§å‹é¡¹ç›®åˆ†æ**\n",
    "   - é¡¹ç›®åŒ…å«100+æ–‡ä»¶\n",
    "   - éœ€è¦éå†æ•´ä¸ªä»£ç åº“\n",
    "   - éœ€è¦å¿«é€Ÿå“åº”\n",
    "\n",
    "3. **é‡å¤ä½¿ç”¨**\n",
    "   - åŒä¸€é¡¹ç›®å¤šæ¬¡ç”Ÿæˆæ•°æ®\n",
    "   - éœ€è¦ç¦»çº¿å·¥ä½œ\n",
    "   - éœ€è¦å®éªŒä¸åŒå‚æ•°\n",
    "\n",
    "#### âš ï¸ **å¯æ¥å—GitHub APIçš„åœºæ™¯**\n",
    "\n",
    "1. **å¿«é€Ÿè¯•ç”¨**\n",
    "   - åªæ˜¯æµ‹è¯•åŠŸèƒ½\n",
    "   - åªéœ€1-2ä¸ªæ ·æœ¬\n",
    "   - ä¸åœ¨æ„é€Ÿåº¦\n",
    "\n",
    "2. **æå°å‹é¡¹ç›®**\n",
    "   - åªæœ‰5-10ä¸ªæ–‡ä»¶\n",
    "   - æ€»ä»£ç é‡ < 50KB\n",
    "   - ä¸€æ¬¡æ€§ä½¿ç”¨\n",
    "\n",
    "3. **ç£ç›˜ç©ºé—´ä¸¥æ ¼å—é™**\n",
    "   - å®Œå…¨æ— æ³•å…‹éš†åˆ°æœ¬åœ°\n",
    "   - åªèƒ½ä½¿ç”¨Webç¯å¢ƒ\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ˆ å®æµ‹æ•°æ®å¯¹æ¯”\n",
    "\n",
    "åŸºäº `ontology-llm` é¡¹ç›®ï¼ˆçº¦50ä¸ªPythonæ–‡ä»¶ï¼‰ï¼š\n",
    "\n",
    "| æ“ä½œ | æœ¬åœ°å…‹éš† | GitHub API |\n",
    "|------|---------|-----------|\n",
    "| **åˆå§‹å‡†å¤‡** | 15ç§’ï¼ˆå…‹éš†ä¸€æ¬¡ï¼‰ | 0ç§’ |\n",
    "| **ç”Ÿæˆ1ä¸ªQ&A** | 12-20ç§’ | 15-25ç§’ |\n",
    "| **ç”Ÿæˆ5ä¸ªQ&A** | 60-120ç§’ | 120-300ç§’ |\n",
    "| **ç”Ÿæˆ50ä¸ªQ&A** | 10-20åˆ†é’Ÿ | **ä¸å¯è¡Œ**ï¼ˆè¶…é™é¢ï¼‰ |\n",
    "| **æˆåŠŸç‡** | 85-95% | 60-75% |\n",
    "| **é‡å¤æ‰§è¡Œ** | æ— é™åˆ¶ | å—é™ï¼ˆAPIé™é¢ï¼‰ |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ æ··åˆæ–¹æ¡ˆï¼ˆæœ€ä¼˜å®è·µï¼‰\n",
    "\n",
    "```python\n",
    "def smart_repo_access(repo_url, prefer_local=True):\n",
    "    \"\"\"\n",
    "    æ™ºèƒ½é€‰æ‹©è®¿é—®æ–¹å¼\n",
    "    - ä¼˜å…ˆæœ¬åœ°å…‹éš†ï¼ˆå¦‚æœç£ç›˜å…è®¸ï¼‰\n",
    "    - å°é¡¹ç›®å¯ç”¨APIï¼ˆè‡ªåŠ¨é™çº§ï¼‰\n",
    "    \"\"\"\n",
    "    repo_info = get_repo_info(repo_url)  # é€šè¿‡APIè·å–é¡¹ç›®å…ƒæ•°æ®\n",
    "    \n",
    "    # åˆ¤æ–­é¡¹ç›®è§„æ¨¡\n",
    "    if repo_info['size'] < 10_000:  # < 10MB\n",
    "        print(\"ğŸ“¦ å°å‹é¡¹ç›®ï¼Œä½¿ç”¨GitHub API\")\n",
    "        return GitHubAPIReader(repo_url)\n",
    "    else:\n",
    "        print(\"ğŸ“¦ ä¸­å¤§å‹é¡¹ç›®ï¼Œæ¨èæœ¬åœ°å…‹éš†\")\n",
    "        return LocalCloneReader(repo_url)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ é’ˆå¯¹æœ¬é¡¹ç›®çš„å»ºè®®\n",
    "\n",
    "**å½“å‰åœºæ™¯ï¼šä¸ºQwen 2.5å¾®è°ƒç”Ÿæˆè®­ç»ƒæ•°æ®**\n",
    "\n",
    "âœ… **å¼ºçƒˆæ¨èæœ¬åœ°å…‹éš†**\n",
    "\n",
    "**åŸå› ï¼š**\n",
    "1. éœ€è¦ç”Ÿæˆ**å¤§é‡**è®­ç»ƒæ ·æœ¬ï¼ˆ50-500ä¸ªï¼‰\n",
    "2. éœ€è¦**é«˜è´¨é‡**æ•°æ®ï¼ˆå¤šæ ·æ€§ã€å®Œæ•´æ€§ï¼‰\n",
    "3. éœ€è¦**ç¨³å®šæ€§**ï¼ˆæ‰¹é‡ç”Ÿæˆä¸èƒ½ä¸­æ–­ï¼‰\n",
    "4. é¡¹ç›®è§„æ¨¡ä¸­ç­‰ï¼ˆ`ontology-llm` ~5MBï¼‰\n",
    "5. **ä¸€æ¬¡å…‹éš†ï¼Œå¤šæ¬¡ä½¿ç”¨**\n",
    "\n",
    "**æˆæœ¬å¯¹æ¯”ï¼š**\n",
    "- å…‹éš†æˆæœ¬ï¼š15ç§’ + 5MBç£ç›˜\n",
    "- æ”¶ç›Šï¼šå¿«100å€ + ç¨³å®š10å€ + è´¨é‡æå‡15%\n",
    "\n",
    "**ç»“è®ºï¼š** å¯¹äºè®­ç»ƒæ•°æ®ç”Ÿæˆåœºæ™¯ï¼Œæœ¬åœ°å…‹éš†æ–¹æ¡ˆçš„ **æ€§ä»·æ¯”é«˜100å€ä»¥ä¸Š**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ†• å¯é€‰ï¼šå®ç°GitHub APIæ–¹æ¡ˆ\n",
    "\n",
    "å¦‚æœç¡®å®éœ€è¦æ— æœ¬åœ°ä¸‹è½½æ–¹æ¡ˆï¼Œå¯ä»¥å‚è€ƒä¸‹ä¸€ä¸ªCellçš„å®ç°ä»£ç ã€‚\n",
    "\n",
    "**âš ï¸ æ³¨æ„ï¼š** ä»…æ¨èç”¨äºå¿«é€Ÿæ¼”ç¤ºæˆ–æå°å‹é¡¹ç›®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯é€‰å®ç°ï¼šGitHub APIç›´æ¥è¯»å–ï¼ˆä¸æ¨èç”¨äºæ‰¹é‡ç”Ÿæˆï¼‰\n",
    "import requests\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "class GitHubAPIReader:\n",
    "    \"\"\"\n",
    "    é€šè¿‡GitHub APIç›´æ¥è¯»å–ä»£ç ï¼Œæ— éœ€å…‹éš†\n",
    "    âš ï¸ ä»…æ¨èç”¨äºæ¼”ç¤ºæˆ–å°å‹é¡¹ç›®\n",
    "    \"\"\"\n",
    "    def __init__(self, repo_url, token=None):\n",
    "        # è§£æä»“åº“ä¿¡æ¯\n",
    "        parts = repo_url.replace('https://github.com/', '').replace('.git', '').split('/')\n",
    "        self.owner = parts[0]\n",
    "        self.repo = parts[1]\n",
    "        self.base_url = f\"https://api.github.com/repos/{self.owner}/{self.repo}\"\n",
    "        \n",
    "        # è®¾ç½®è¯·æ±‚å¤´ï¼ˆå¦‚æœæœ‰tokenï¼‰\n",
    "        self.headers = {}\n",
    "        if token:\n",
    "            self.headers['Authorization'] = f'token {token}'\n",
    "        \n",
    "        print(f\"ğŸ“¡ GitHub APIæ¨¡å¼\")\n",
    "        print(f\"   ä»“åº“: {self.owner}/{self.repo}\")\n",
    "        print(f\"   è®¤è¯: {'å·²é…ç½®' if token else 'æœªé…ç½®ï¼ˆ60æ¬¡/å°æ—¶ï¼‰'}\")\n",
    "    \n",
    "    def get_file_content(self, file_path):\n",
    "        \"\"\"è¯»å–å•ä¸ªæ–‡ä»¶å†…å®¹\"\"\"\n",
    "        url = f\"{self.base_url}/contents/{file_path}\"\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            content = response.json()\n",
    "            if content['encoding'] == 'base64':\n",
    "                return base64.b64decode(content['content']).decode('utf-8')\n",
    "        else:\n",
    "            raise Exception(f\"è¯»å–å¤±è´¥: {response.status_code} - {file_path}\")\n",
    "    \n",
    "    def list_python_files(self, path='', max_files=50):\n",
    "        \"\"\"åˆ—å‡ºPythonæ–‡ä»¶ï¼ˆé€’å½’ï¼‰\"\"\"\n",
    "        url = f\"{self.base_url}/contents/{path}\"\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return []\n",
    "        \n",
    "        files = []\n",
    "        for item in response.json():\n",
    "            if item['type'] == 'file' and item['name'].endswith('.py'):\n",
    "                files.append(item['path'])\n",
    "                if len(files) >= max_files:\n",
    "                    break\n",
    "            elif item['type'] == 'dir':\n",
    "                # é€’å½’æœç´¢å­ç›®å½•\n",
    "                subfiles = self.list_python_files(item['path'], max_files - len(files))\n",
    "                files.extend(subfiles)\n",
    "                if len(files) >= max_files:\n",
    "                    break\n",
    "        \n",
    "        return files[:max_files]\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰\n",
    "print(\"=\" * 70)\n",
    "print(\"âš ï¸  GitHub APIæ–¹æ¡ˆæ¼”ç¤º\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"æ­¤æ–¹æ¡ˆé€‚ç”¨äº:\")\n",
    "print(\"  âœ… å¿«é€Ÿè¯•ç”¨ï¼ˆ1-2ä¸ªæ ·æœ¬ï¼‰\")\n",
    "print(\"  âœ… æå°å‹é¡¹ç›®ï¼ˆ<10ä¸ªæ–‡ä»¶ï¼‰\")\n",
    "print(\"  âœ… æ¼”ç¤ºç›®çš„\")\n",
    "print()\n",
    "print(\"ä¸é€‚ç”¨äº:\")\n",
    "print(\"  âŒ æ‰¹é‡ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆæœ¬é¡¹ç›®çš„æ ¸å¿ƒåœºæ™¯ï¼‰\")\n",
    "print(\"  âŒ ä¸­å¤§å‹é¡¹ç›®\")\n",
    "print(\"  âŒ éœ€è¦é«˜æˆåŠŸç‡çš„åœºæ™¯\")\n",
    "print()\n",
    "print(\"ğŸ’¡ å»ºè®®ï¼šå¯¹äºè®­ç»ƒæ•°æ®ç”Ÿæˆï¼Œè¯·ä½¿ç”¨æœ¬åœ°å…‹éš†æ–¹æ¡ˆ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# å¦‚éœ€ä½¿ç”¨GitHub APIæ–¹æ¡ˆï¼Œå–æ¶ˆæ³¨é‡Šä»¥ä¸‹ä»£ç ï¼š\n",
    "\"\"\"\n",
    "# é…ç½®GitHub Tokenï¼ˆå¯é€‰ï¼Œæå‡é™é¢åˆ°5000æ¬¡/å°æ—¶ï¼‰\n",
    "github_token = os.getenv('GITHUB_TOKEN', None)  # ä»ç¯å¢ƒå˜é‡è¯»å–\n",
    "\n",
    "# åˆ›å»ºAPIè¯»å–å™¨\n",
    "api_reader = GitHubAPIReader(\n",
    "    \"https://github.com/qzc438-research/ontology-llm\",\n",
    "    token=github_token\n",
    ")\n",
    "\n",
    "# åˆ—å‡ºPythonæ–‡ä»¶\n",
    "python_files = api_reader.list_python_files(max_files=10)\n",
    "print(f\"æ‰¾åˆ° {len(python_files)} ä¸ªPythonæ–‡ä»¶:\")\n",
    "for f in python_files[:5]:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# è¯»å–å•ä¸ªæ–‡ä»¶\n",
    "if python_files:\n",
    "    sample_file = python_files[0]\n",
    "    content = api_reader.get_file_content(sample_file)\n",
    "    print(f\"\\næ–‡ä»¶å†…å®¹é¢„è§ˆ ({sample_file}):\")\n",
    "    print(content[:300] + \"...\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5b81c",
   "metadata": {},
   "source": [
    "## ğŸ§  ä¸Šä¸‹æ–‡å¢å¼ºï¼šé¡¹ç›®æ¶æ„ç†è§£æ¨¡å—\n",
    "\n",
    "### ğŸ“‹ å½“å‰å®ç°çŠ¶æ€\n",
    "\n",
    "**âŒ å½“å‰æœªå®ç°ä¸Šä¸‹æ–‡æ£€ç´¢** - é‡‡ç”¨ç®€å•çš„ä»£ç ç‰‡æ®µé‡‡æ ·ç­–ç•¥\n",
    "\n",
    "**å½“å‰æ–¹æ³•ï¼ˆCell 14-15ï¼‰ï¼š**\n",
    "```python\n",
    "# ç®€å•ç­–ç•¥ï¼šéšæœºè¯»å–800å­—ç¬¦ä»£ç ç‰‡æ®µ\n",
    "code_snippet = code[random_start : random_start + 800]\n",
    "\n",
    "# ç›´æ¥ç”ŸæˆQ&Aï¼Œæ— é¢å¤–ä¸Šä¸‹æ–‡\n",
    "prompt = f\"åŸºäºä»¥ä¸‹ä»£ç ç”Ÿæˆé—®ç­”: {code_snippet}\"\n",
    "```\n",
    "\n",
    "**å±€é™æ€§ï¼š**\n",
    "- ğŸ”´ **ç¼ºå°‘é¡¹ç›®å…¨å±€è§†å›¾**ï¼šä¸çŸ¥é“å½“å‰ä»£ç åœ¨é¡¹ç›®ä¸­çš„ä½ç½®\n",
    "- ğŸ”´ **ç¼ºå°‘ä¾èµ–å…³ç³»**ï¼šä¸çŸ¥é“è¯¥å‡½æ•°è°ƒç”¨äº†å“ªäº›å…¶ä»–æ¨¡å—\n",
    "- ğŸ”´ **ç¼ºå°‘æ¶æ„ä¸Šä¸‹æ–‡**ï¼šä¸ç†è§£è¯¥æ–‡ä»¶åœ¨æ•´ä½“æ¶æ„ä¸­çš„è§’è‰²\n",
    "- ğŸ”´ **é—®é¢˜æ·±åº¦æœ‰é™**ï¼šåªèƒ½é—®ä»£ç è¡¨å±‚åŠŸèƒ½ï¼Œéš¾ä»¥é—®æ¶æ„çº§é—®é¢˜\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… åº”è¯¥åŠ å…¥ä¸Šä¸‹æ–‡æ£€ç´¢ - æ˜¾è‘—æå‡è´¨é‡\n",
    "\n",
    "**æ”¹è¿›æ•ˆæœé¢„ä¼°ï¼š**\n",
    "- ğŸ“ˆ **é—®é¢˜æ·±åº¦æå‡** 40%ï¼šä»ä»£ç çº§ â†’ æ¶æ„çº§\n",
    "- ğŸ“ˆ **ç­”æ¡ˆå‡†ç¡®æ€§æå‡** 30%ï¼šæœ‰å®Œæ•´ä¸Šä¸‹æ–‡ç†è§£\n",
    "- ğŸ“ˆ **é—®é¢˜å¤šæ ·æ€§æå‡** 50%ï¼šå¯ä»¥é—®è·¨æ–‡ä»¶çš„äº¤äº’é—®é¢˜\n",
    "- ğŸ“ˆ **è®­ç»ƒæ•°æ®ä»·å€¼æå‡** 60%ï¼šæ›´æ¥è¿‘çœŸå®å¼€å‘åœºæ™¯\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ ä¸Šä¸‹æ–‡æ£€ç´¢æ–¹æ¡ˆè®¾è®¡\n",
    "\n",
    "#### æ–¹æ¡ˆAï¼šè½»é‡çº§ä¸Šä¸‹æ–‡ï¼ˆæ¨èï¼‰\n",
    "\n",
    "**åŒ…å«ä¿¡æ¯ï¼š**\n",
    "1. **é¡¹ç›®ç»“æ„æ‘˜è¦**ï¼ˆ200å­—ç¬¦ï¼‰\n",
    "   - æ ¸å¿ƒæ¨¡å—åˆ—è¡¨\n",
    "   - ä¸»è¦åŠŸèƒ½æè¿°\n",
    "   \n",
    "2. **å½“å‰æ–‡ä»¶å…ƒä¿¡æ¯**ï¼ˆ150å­—ç¬¦ï¼‰\n",
    "   - æ–‡ä»¶è·¯å¾„å’Œè§’è‰²\n",
    "   - ä¸»è¦ç±»/å‡½æ•°åˆ—è¡¨\n",
    "   \n",
    "3. **ä¾èµ–å…³ç³»**ï¼ˆ100å­—ç¬¦ï¼‰\n",
    "   - Importè¯­å¥\n",
    "   - è¢«å“ªäº›æ–‡ä»¶è°ƒç”¨\n",
    "\n",
    "4. **ç›¸å…³ä»£ç ç‰‡æ®µ**ï¼ˆ500å­—ç¬¦ï¼‰\n",
    "   - å…³è”å‡½æ•°çš„ç­¾åå’Œæ–‡æ¡£å­—ç¬¦ä¸²\n",
    "\n",
    "**æ€»ä¸Šä¸‹æ–‡é•¿åº¦ï¼š** ~1000å­—ç¬¦ï¼ˆæˆæœ¬å¯æ§ï¼‰\n",
    "\n",
    "**å®ç°å¤æ‚åº¦ï¼š** â­â­â˜†â˜†â˜†ï¼ˆä¸­ç­‰ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "#### æ–¹æ¡ˆBï¼šå®Œæ•´æ¶æ„ç†è§£ï¼ˆé«˜çº§ï¼‰\n",
    "\n",
    "**åŒ…å«ä¿¡æ¯ï¼š**\n",
    "1. é¡¹ç›®READMEæ‘˜è¦\n",
    "2. å®Œæ•´ä¾èµ–å›¾\n",
    "3. ç±»ç»§æ‰¿å…³ç³»\n",
    "4. è°ƒç”¨é“¾è·¯\n",
    "5. é…ç½®æ–‡ä»¶ä¿¡æ¯\n",
    "6. æµ‹è¯•è¦†ç›–æƒ…å†µ\n",
    "\n",
    "**æ€»ä¸Šä¸‹æ–‡é•¿åº¦ï¼š** ~3000å­—ç¬¦ï¼ˆæˆæœ¬è¾ƒé«˜ï¼‰\n",
    "\n",
    "**å®ç°å¤æ‚åº¦ï¼š** â­â­â­â­â˜†ï¼ˆå¤æ‚ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ æ¨èå®ç°ï¼šæ··åˆæ–¹æ¡ˆ\n",
    "\n",
    "**æ ¸å¿ƒæ€è·¯ï¼š** æ ¹æ®ä»£ç ç‰‡æ®µç±»å‹åŠ¨æ€é€‰æ‹©ä¸Šä¸‹æ–‡æ·±åº¦\n",
    "\n",
    "```python\n",
    "def get_enhanced_context(code_snippet, file_path, project_path):\n",
    "    \"\"\"\n",
    "    æ™ºèƒ½ä¸Šä¸‹æ–‡å¢å¼º\n",
    "    \"\"\"\n",
    "    context = {\n",
    "        \"project_summary\": \"\",    # é¡¹ç›®æ¦‚è¦\n",
    "        \"file_role\": \"\",          # æ–‡ä»¶è§’è‰²\n",
    "        \"imports\": [],            # ä¾èµ–æ¨¡å—\n",
    "        \"related_code\": \"\",       # ç›¸å…³ä»£ç \n",
    "        \"architecture_notes\": \"\"  # æ¶æ„è¯´æ˜\n",
    "    }\n",
    "    \n",
    "    # 1. é¡¹ç›®ç»“æ„åˆ†æï¼ˆä¸€æ¬¡æ€§ç¼“å­˜ï¼‰\n",
    "    if not hasattr(get_enhanced_context, 'project_cache'):\n",
    "        context[\"project_summary\"] = analyze_project_structure(project_path)\n",
    "        get_enhanced_context.project_cache = context[\"project_summary\"]\n",
    "    else:\n",
    "        context[\"project_summary\"] = get_enhanced_context.project_cache\n",
    "    \n",
    "    # 2. æ–‡ä»¶çº§åˆ†æ\n",
    "    context[\"file_role\"] = analyze_file_role(file_path)\n",
    "    \n",
    "    # 3. ä¾èµ–åˆ†æï¼ˆä»importè¯­å¥æå–ï¼‰\n",
    "    context[\"imports\"] = extract_imports(file_path)\n",
    "    \n",
    "    # 4. ç›¸å…³ä»£ç æ£€ç´¢ï¼ˆå¦‚æœä»£ç ç‰‡æ®µè°ƒç”¨äº†å…¶ä»–å‡½æ•°ï¼‰\n",
    "    called_functions = extract_function_calls(code_snippet)\n",
    "    context[\"related_code\"] = retrieve_related_functions(called_functions, project_path)\n",
    "    \n",
    "    # 5. æ¶æ„æ³¨è§£ï¼ˆåŸºäºæ–‡ä»¶è·¯å¾„æ¨æ–­ï¼‰\n",
    "    context[\"architecture_notes\"] = infer_architecture_role(file_path)\n",
    "    \n",
    "    return context\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š æ•ˆæœå¯¹æ¯”ç¤ºä¾‹\n",
    "\n",
    "#### æ— ä¸Šä¸‹æ–‡ï¼ˆå½“å‰ï¼‰\n",
    "\n",
    "**ä»£ç ç‰‡æ®µï¼š**\n",
    "```python\n",
    "def match_entities(source, target):\n",
    "    return compute_similarity(source, target)\n",
    "```\n",
    "\n",
    "**ç”Ÿæˆçš„é—®é¢˜ï¼š**\n",
    "> \"match_entitieså‡½æ•°çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "**é—®é¢˜è´¨é‡ï¼š** â­â­â˜†â˜†â˜†ï¼ˆæµ…å±‚ç†è§£ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "#### æœ‰ä¸Šä¸‹æ–‡ï¼ˆæ”¹è¿›åï¼‰\n",
    "\n",
    "**ä»£ç ç‰‡æ®µ + ä¸Šä¸‹æ–‡ï¼š**\n",
    "```\n",
    "[é¡¹ç›®ä¸Šä¸‹æ–‡]\n",
    "- é¡¹ç›®ï¼šontology-llm - æœ¬ä½“åŒ¹é…ç³»ç»Ÿ\n",
    "- æ–‡ä»¶ï¼šllm_matching.py - æ ¸å¿ƒåŒ¹é…å¼•æ“\n",
    "- ä¾èµ–ï¼šutil.py (compute_similarity), config.py (THRESHOLD)\n",
    "- æ¶æ„è§’è‰²ï¼šä¸šåŠ¡é€»è¾‘å±‚ï¼Œè¢«api.pyè°ƒç”¨\n",
    "- ç›¸å…³å‡½æ•°ï¼šbatch_match(), filter_results()\n",
    "\n",
    "[ä»£ç ç‰‡æ®µ]\n",
    "def match_entities(source, target):\n",
    "    return compute_similarity(source, target)\n",
    "```\n",
    "\n",
    "**ç”Ÿæˆçš„é—®é¢˜ï¼š**\n",
    "> \"åœ¨ontology-llmçš„æ¶æ„ä¸­ï¼Œmatch_entitieså¦‚ä½•ä¸batch_matchåä½œå®ç°å¤§è§„æ¨¡æœ¬ä½“åŒ¹é…ï¼Ÿcompute_similarityçš„ç›¸ä¼¼åº¦ç®—æ³•å¯¹åŒ¹é…å‡†ç¡®æ€§æœ‰ä½•å½±å“ï¼Ÿ\"\n",
    "\n",
    "**é—®é¢˜è´¨é‡ï¼š** â­â­â­â­â­ï¼ˆæ¶æ„çº§ç†è§£ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ å®ç°å»ºè®®\n",
    "\n",
    "#### é˜¶æ®µ1ï¼šå¿«é€ŸMVPï¼ˆ10åˆ†é’Ÿï¼‰\n",
    "\n",
    "```python\n",
    "# æœ€å°åŒ–ä¸Šä¸‹æ–‡ï¼šåªæ·»åŠ é¡¹ç›®READMEå’Œæ–‡ä»¶è·¯å¾„\n",
    "context = f\"\"\"\n",
    "é¡¹ç›®: {project_name}\n",
    "æè¿°: {readme_first_paragraph}\n",
    "å½“å‰æ–‡ä»¶: {file_path}\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{context}\n",
    "\n",
    "ä»£ç :\n",
    "{code_snippet}\n",
    "\n",
    "åŸºäºä»¥ä¸Šé¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆæ·±åº¦æŠ€æœ¯é—®ç­”...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**æ”¹è¿›æ•ˆæœï¼š** +20% é—®é¢˜è´¨é‡\n",
    "\n",
    "---\n",
    "\n",
    "#### é˜¶æ®µ2ï¼šä¾èµ–åˆ†æï¼ˆ30åˆ†é’Ÿï¼‰\n",
    "\n",
    "```python\n",
    "# æ·»åŠ importåˆ†æå’Œå‡½æ•°ç­¾å\n",
    "imports = extract_imports(file_path)\n",
    "functions = extract_function_signatures(file_path)\n",
    "\n",
    "context = f\"\"\"\n",
    "é¡¹ç›®: {project_name}\n",
    "å½“å‰æ–‡ä»¶: {file_path}\n",
    "ä¾èµ–æ¨¡å—: {', '.join(imports)}\n",
    "ä¸»è¦å‡½æ•°: {', '.join(functions)}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**æ”¹è¿›æ•ˆæœï¼š** +40% é—®é¢˜è´¨é‡\n",
    "\n",
    "---\n",
    "\n",
    "#### é˜¶æ®µ3ï¼šå®Œæ•´æ¶æ„ï¼ˆ2å°æ—¶ï¼‰\n",
    "\n",
    "å®ç°å®Œæ•´çš„é¡¹ç›®åˆ†æå¼•æ“ï¼ˆå‚è€ƒä¸‹ä¸€ä¸ªCellçš„å®ç°ä»£ç ï¼‰\n",
    "\n",
    "**æ”¹è¿›æ•ˆæœï¼š** +60% é—®é¢˜è´¨é‡\n",
    "\n",
    "---\n",
    "\n",
    "### âš–ï¸ æˆæœ¬-æ”¶ç›Šåˆ†æ\n",
    "\n",
    "| æ–¹æ¡ˆ | å®ç°æ—¶é—´ | Tokenå¢åŠ  | è´¨é‡æå‡ | æ¨èåº¦ |\n",
    "|------|---------|----------|---------|--------|\n",
    "| **æ— ä¸Šä¸‹æ–‡**ï¼ˆå½“å‰ï¼‰ | 0åˆ†é’Ÿ | 0 | 0% | â­â­â˜†â˜†â˜† |\n",
    "| **æœ€å°ä¸Šä¸‹æ–‡**ï¼ˆMVPï¼‰ | 10åˆ†é’Ÿ | +200 | +20% | â­â­â­â­â˜† |\n",
    "| **ä¾èµ–åˆ†æ** | 30åˆ†é’Ÿ | +500 | +40% | â­â­â­â­â­ |\n",
    "| **å®Œæ•´æ¶æ„** | 2å°æ—¶ | +1500 | +60% | â­â­â­â­â˜† |\n",
    "\n",
    "**æ¨èï¼š** å…ˆå®ç°\"ä¾èµ–åˆ†æ\"æ–¹æ¡ˆï¼ˆæ€§ä»·æ¯”æœ€é«˜ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ é’ˆå¯¹æœ¬é¡¹ç›®çš„å…·ä½“å»ºè®®\n",
    "\n",
    "**å¯¹äºQwen 2.5å¾®è°ƒçš„è®­ç»ƒæ•°æ®ç”Ÿæˆï¼š**\n",
    "\n",
    "âœ… **å¼ºçƒˆå»ºè®®åŠ å…¥ä¸Šä¸‹æ–‡æ£€ç´¢**\n",
    "\n",
    "**åŸå› ï¼š**\n",
    "1. **æå‡è®­ç»ƒæ•°æ®è´¨é‡**ï¼šæ¶æ„çº§ç†è§£ â†’ æ¨¡å‹å­¦åˆ°çœŸå®å¼€å‘æ€ç»´\n",
    "2. **å¢å¼ºé—®é¢˜å¤šæ ·æ€§**ï¼šä»å•æ–‡ä»¶ â†’ è·¨æ–‡ä»¶äº¤äº’\n",
    "3. **ç¬¦åˆå®é™…ä½¿ç”¨åœºæ™¯**ï¼šçœŸå®å¼€å‘ä¸­æ€»æ˜¯æœ‰é¡¹ç›®ä¸Šä¸‹æ–‡\n",
    "4. **æˆæœ¬å¯æ§**ï¼šä¾èµ–åˆ†ææ–¹æ¡ˆåªéœ€+500 tokens\n",
    "\n",
    "**å®æ–½è·¯å¾„ï¼š**\n",
    "1. ä»Šå¤©ï¼šå®ç°MVPç‰ˆæœ¬ï¼ˆ10åˆ†é’Ÿï¼‰â†’ ç«‹å³çœ‹åˆ°æ•ˆæœ\n",
    "2. æœ¬å‘¨ï¼šå®ç°ä¾èµ–åˆ†æç‰ˆæœ¬ï¼ˆ30åˆ†é’Ÿï¼‰â†’ è´¨é‡æå‡40%\n",
    "3. å¯é€‰ï¼šå®ç°å®Œæ•´æ¶æ„ç‰ˆæœ¬ï¼ˆ2å°æ—¶ï¼‰â†’ æè‡´ä¼˜åŒ–\n",
    "\n",
    "**æŠ•å…¥äº§å‡ºæ¯”ï¼š** 30åˆ†é’Ÿ â†’ è´¨é‡æå‡40% = **è¶…å€¼**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ†• ä¸‹ä¸€ä¸ªCellï¼šå®ç°ä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å—\n",
    "\n",
    "å‚è€ƒä¸‹ä¸€ä¸ªCellçš„å®Œæ•´å®ç°ä»£ç ï¼ˆå«ç¤ºä¾‹ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å— - å®ç°ä»£ç \n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class ProjectContextAnalyzer:\n",
    "    \"\"\"\n",
    "    é¡¹ç›®ä¸Šä¸‹æ–‡åˆ†æå™¨ - æå–æ¶æ„ä¿¡æ¯å¢å¼ºQ&Aç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    def __init__(self, project_path):\n",
    "        self.project_path = Path(project_path)\n",
    "        self.project_cache = {}\n",
    "        print(f\"ğŸ§  åˆå§‹åŒ–é¡¹ç›®åˆ†æå™¨: {self.project_path.name}\")\n",
    "    \n",
    "    def analyze_project_structure(self):\n",
    "        \"\"\"åˆ†æé¡¹ç›®æ•´ä½“ç»“æ„ï¼ˆä¸€æ¬¡æ€§ç¼“å­˜ï¼‰\"\"\"\n",
    "        if 'structure' in self.project_cache:\n",
    "            return self.project_cache['structure']\n",
    "        \n",
    "        # ç»Ÿè®¡é¡¹ç›®ä¿¡æ¯\n",
    "        py_files = list(self.project_path.glob(\"**/*.py\"))\n",
    "        py_files = [f for f in py_files if '__pycache__' not in str(f)]\n",
    "        \n",
    "        # æå–ä¸»è¦æ¨¡å—\n",
    "        modules = defaultdict(int)\n",
    "        for f in py_files:\n",
    "            module_name = f.stem\n",
    "            if module_name != '__init__':\n",
    "                modules[module_name] += 1\n",
    "        \n",
    "        # è¯»å–READMEï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n",
    "        readme_summary = \"\"\n",
    "        for readme_name in ['README.md', 'README.rst', 'README.txt']:\n",
    "            readme_path = self.project_path / readme_name\n",
    "            if readme_path.exists():\n",
    "                with open(readme_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    # æå–ç¬¬ä¸€æ®µæˆ–å‰200å­—ç¬¦\n",
    "                    readme_summary = content[:300].split('\\n\\n')[0]\n",
    "                break\n",
    "        \n",
    "        structure = {\n",
    "            \"name\": self.project_path.name,\n",
    "            \"file_count\": len(py_files),\n",
    "            \"main_modules\": list(modules.keys())[:10],\n",
    "            \"description\": readme_summary\n",
    "        }\n",
    "        \n",
    "        self.project_cache['structure'] = structure\n",
    "        return structure\n",
    "    \n",
    "    def analyze_file_role(self, file_path):\n",
    "        \"\"\"åˆ†ææ–‡ä»¶åœ¨é¡¹ç›®ä¸­çš„è§’è‰²\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        # åŸºäºè·¯å¾„æ¨æ–­\n",
    "        path_parts = file_path.parts\n",
    "        role_hints = {\n",
    "            'api': 'æ¥å£å±‚',\n",
    "            'service': 'æœåŠ¡å±‚',\n",
    "            'model': 'æ•°æ®æ¨¡å‹',\n",
    "            'util': 'å·¥å…·å‡½æ•°',\n",
    "            'config': 'é…ç½®ç®¡ç†',\n",
    "            'test': 'æµ‹è¯•ä»£ç ',\n",
    "            'core': 'æ ¸å¿ƒé€»è¾‘',\n",
    "            'handler': 'å¤„ç†å™¨',\n",
    "            'controller': 'æ§åˆ¶å™¨',\n",
    "            'view': 'è§†å›¾å±‚'\n",
    "        }\n",
    "        \n",
    "        for part in path_parts:\n",
    "            for keyword, role in role_hints.items():\n",
    "                if keyword in part.lower():\n",
    "                    return role\n",
    "        \n",
    "        # åŸºäºæ–‡ä»¶åæ¨æ–­\n",
    "        filename = file_path.stem.lower()\n",
    "        for keyword, role in role_hints.items():\n",
    "            if keyword in filename:\n",
    "                return role\n",
    "        \n",
    "        return \"ä¸šåŠ¡é€»è¾‘\"\n",
    "    \n",
    "    def extract_imports(self, file_path):\n",
    "        \"\"\"æå–æ–‡ä»¶çš„importä¾èµ–\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # æå–importè¯­å¥\n",
    "            imports = []\n",
    "            for line in content.split('\\n')[:50]:  # åªçœ‹å‰50è¡Œ\n",
    "                line = line.strip()\n",
    "                if line.startswith('import ') or line.startswith('from '):\n",
    "                    # æå–æ¨¡å—å\n",
    "                    if line.startswith('import '):\n",
    "                        module = line.replace('import ', '').split()[0].split('.')[0]\n",
    "                    else:\n",
    "                        module = line.split()[1].split('.')[0]\n",
    "                    imports.append(module)\n",
    "            \n",
    "            return list(set(imports))[:10]  # å»é‡å¹¶é™åˆ¶æ•°é‡\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def extract_function_signatures(self, file_path):\n",
    "        \"\"\"æå–æ–‡ä»¶ä¸­çš„ä¸»è¦å‡½æ•°ç­¾å\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # æå–å‡½æ•°å®šä¹‰\n",
    "            functions = []\n",
    "            for match in re.finditer(r'def\\s+(\\w+)\\s*\\(([^)]*)\\)', content):\n",
    "                func_name = match.group(1)\n",
    "                params = match.group(2)\n",
    "                if not func_name.startswith('_'):  # è·³è¿‡ç§æœ‰å‡½æ•°\n",
    "                    functions.append(f\"{func_name}({params})\")\n",
    "            \n",
    "            return functions[:10]  # é™åˆ¶æ•°é‡\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def build_context(self, code_snippet, file_path, context_level='standard'):\n",
    "        \"\"\"\n",
    "        æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡\n",
    "        \n",
    "        Args:\n",
    "            code_snippet: ä»£ç ç‰‡æ®µ\n",
    "            file_path: æ–‡ä»¶è·¯å¾„\n",
    "            context_level: 'minimal', 'standard', 'full'\n",
    "        \"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        # é¡¹ç›®çº§ä¸Šä¸‹æ–‡\n",
    "        project_info = self.analyze_project_structure()\n",
    "        \n",
    "        # æ–‡ä»¶çº§ä¸Šä¸‹æ–‡\n",
    "        file_role = self.analyze_file_role(file_path)\n",
    "        imports = self.extract_imports(file_path)\n",
    "        functions = self.extract_function_signatures(file_path)\n",
    "        \n",
    "        # æ ¹æ®çº§åˆ«æ„å»ºä¸Šä¸‹æ–‡\n",
    "        if context_level == 'minimal':\n",
    "            context = f\"\"\"\n",
    "[é¡¹ç›®] {project_info['name']}\n",
    "[æ–‡ä»¶] {file_path.name} - {file_role}\n",
    "\"\"\"\n",
    "        \n",
    "        elif context_level == 'standard':\n",
    "            context = f\"\"\"\n",
    "[é¡¹ç›®ä¸Šä¸‹æ–‡]\n",
    "- é¡¹ç›®: {project_info['name']} ({project_info['file_count']}ä¸ªæ–‡ä»¶)\n",
    "- æ ¸å¿ƒæ¨¡å—: {', '.join(project_info['main_modules'][:5])}\n",
    "\n",
    "[å½“å‰æ–‡ä»¶]\n",
    "- è·¯å¾„: {file_path.name}\n",
    "- è§’è‰²: {file_role}\n",
    "- ä¾èµ–: {', '.join(imports[:5]) if imports else 'æ— '}\n",
    "- ä¸»è¦å‡½æ•°: {', '.join([f.split('(')[0] for f in functions[:5]]) if functions else 'æ— '}\n",
    "\"\"\"\n",
    "        \n",
    "        else:  # full\n",
    "            context = f\"\"\"\n",
    "[é¡¹ç›®æ¶æ„]\n",
    "- é¡¹ç›®: {project_info['name']}\n",
    "- è§„æ¨¡: {project_info['file_count']}ä¸ªPythonæ–‡ä»¶\n",
    "- æ ¸å¿ƒæ¨¡å—: {', '.join(project_info['main_modules'][:8])}\n",
    "- æè¿°: {project_info['description'][:150] if project_info['description'] else 'æ— '}\n",
    "\n",
    "[å½“å‰æ–‡ä»¶è¯¦æƒ…]\n",
    "- å®Œæ•´è·¯å¾„: {file_path.relative_to(self.project_path)}\n",
    "- æ¶æ„è§’è‰²: {file_role}\n",
    "- ä¾èµ–æ¨¡å—: {', '.join(imports) if imports else 'æ— å¤–éƒ¨ä¾èµ–'}\n",
    "- ä¸»è¦å‡½æ•°: \n",
    "  {chr(10).join(['  - ' + f for f in functions[:8]]) if functions else '  - æ— '}\n",
    "\n",
    "[ä»£ç ä¸Šä¸‹æ–‡]\n",
    "- ä½ç½®: æ–‡ä»¶çš„æŸä¸ªä»£ç æ®µ\n",
    "\"\"\"\n",
    "        \n",
    "        return context.strip()\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ§  ä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å—æ¼”ç¤º\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# åˆå§‹åŒ–åˆ†æå™¨\n",
    "try:\n",
    "    test_project = project_path  # ä½¿ç”¨å·²é…ç½®çš„é¡¹ç›®è·¯å¾„\n",
    "except NameError:\n",
    "    test_project = Path(\"/Users/xianhaoliu/Library/CloudStorage/OneDrive-Stibo/Project/Agent-OM/ontology-llm\")\n",
    "\n",
    "analyzer = ProjectContextAnalyzer(test_project)\n",
    "\n",
    "# åˆ†æé¡¹ç›®ç»“æ„\n",
    "project_info = analyzer.analyze_project_structure()\n",
    "print(f\"\\nğŸ“Š é¡¹ç›®ä¿¡æ¯:\")\n",
    "print(f\"   åç§°: {project_info['name']}\")\n",
    "print(f\"   æ–‡ä»¶æ•°: {project_info['file_count']}\")\n",
    "print(f\"   ä¸»è¦æ¨¡å—: {', '.join(project_info['main_modules'][:5])}\")\n",
    "\n",
    "# æµ‹è¯•ä¸åŒçº§åˆ«çš„ä¸Šä¸‹æ–‡\n",
    "test_file = test_project / \"llm_matching.py\"\n",
    "if test_file.exists():\n",
    "    print(f\"\\nğŸ“„ æµ‹è¯•æ–‡ä»¶: {test_file.name}\")\n",
    "    \n",
    "    # Minimalçº§åˆ«\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ”¹ Minimalçº§åˆ«ä¸Šä¸‹æ–‡ (é€‚åˆå¿«é€Ÿæµ‹è¯•)\")\n",
    "    print(\"=\" * 70)\n",
    "    minimal_ctx = analyzer.build_context(\"def test(): pass\", test_file, 'minimal')\n",
    "    print(minimal_ctx)\n",
    "    \n",
    "    # Standardçº§åˆ«\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ”¸ Standardçº§åˆ«ä¸Šä¸‹æ–‡ (æ¨èä½¿ç”¨)\")\n",
    "    print(\"=\" * 70)\n",
    "    standard_ctx = analyzer.build_context(\"def test(): pass\", test_file, 'standard')\n",
    "    print(standard_ctx)\n",
    "    \n",
    "    # Fullçº§åˆ«\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ”¶ Fullçº§åˆ«ä¸Šä¸‹æ–‡ (æœ€å®Œæ•´)\")\n",
    "    print(\"=\" * 70)\n",
    "    full_ctx = analyzer.build_context(\"def test(): pass\", test_file, 'full')\n",
    "    print(full_ctx)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ’¡ ä½¿ç”¨å»ºè®®:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"- Minimal: å¿«é€Ÿæµ‹è¯•ï¼Œtokenå¼€é”€å° (+100 tokens)\")\n",
    "    print(\"- Standard: æ¨èä½¿ç”¨ï¼Œå¹³è¡¡è´¨é‡å’Œæˆæœ¬ (+300 tokens)\")\n",
    "    print(\"- Full: æœ€ä½³è´¨é‡ï¼Œé€‚åˆé‡è¦æ•°æ® (+600 tokens)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}\")\n",
    "    print(\"   è¯·å…ˆé…ç½®é¡¹ç›®è·¯å¾„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324eb23",
   "metadata": {},
   "source": [
    "## ğŸ”„ é›†æˆä¸Šä¸‹æ–‡å¢å¼ºåˆ°Q&Aç”Ÿæˆæµç¨‹\n",
    "\n",
    "### ä¿®æ”¹æ–¹æ¡ˆ\n",
    "\n",
    "è¦å°†ä¸Šä¸‹æ–‡å¢å¼ºé›†æˆåˆ°ç°æœ‰çš„Cell 14å’Œ15ï¼Œåªéœ€åšä»¥ä¸‹ä¿®æ”¹ï¼š\n",
    "\n",
    "#### ä¿®æ”¹ç‚¹1ï¼šåˆå§‹åŒ–åˆ†æå™¨ï¼ˆåœ¨Cell 14ä¹‹å‰æ·»åŠ ï¼‰\n",
    "\n",
    "```python\n",
    "# åˆ›å»ºä¸Šä¸‹æ–‡åˆ†æå™¨ï¼ˆä¸€æ¬¡æ€§ï¼‰\n",
    "context_analyzer = ProjectContextAnalyzer(agent_om_path)\n",
    "```\n",
    "\n",
    "#### ä¿®æ”¹ç‚¹2ï¼šä¿®æ”¹æç¤ºè¯ï¼ˆåœ¨Cell 14å’Œ15ä¸­ï¼‰\n",
    "\n",
    "**åŸæœ‰æç¤ºè¯ï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰ï¼š**\n",
    "```python\n",
    "prompt = f\"\"\"åŸºäºä»¥ä¸‹Pythonä»£ç ç”ŸæˆæŠ€æœ¯é—®ç­”ï¼š\n",
    "\n",
    "```python\n",
    "{code_snippet}\n",
    "```\n",
    "\n",
    "æ ¼å¼...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**æ”¹è¿›æç¤ºè¯ï¼ˆå«ä¸Šä¸‹æ–‡ï¼‰ï¼š**\n",
    "```python\n",
    "# æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡\n",
    "enhanced_context = context_analyzer.build_context(\n",
    "    code_snippet, \n",
    "    file_path, \n",
    "    context_level='standard'  # æˆ– 'minimal', 'full'\n",
    ")\n",
    "\n",
    "# æ–°æç¤ºè¯\n",
    "prompt = f\"\"\"ä½ æ˜¯ä¸€ä½èµ„æ·±è½¯ä»¶æ¶æ„å¸ˆï¼Œè¯·åŸºäºé¡¹ç›®ä¸Šä¸‹æ–‡ç”Ÿæˆæ·±åº¦æŠ€æœ¯é—®ç­”ã€‚\n",
    "\n",
    "{enhanced_context}\n",
    "\n",
    "[ä»£ç ç‰‡æ®µ]\n",
    "```python\n",
    "{code_snippet}\n",
    "```\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "- é—®é¢˜åº”è€ƒè™‘ä»£ç åœ¨é¡¹ç›®æ¶æ„ä¸­çš„ä½ç½®å’Œä½œç”¨\n",
    "- å¯ä»¥æ¶‰åŠæ¨¡å—é—´çš„äº¤äº’å’Œä¾èµ–å…³ç³»\n",
    "- ç­”æ¡ˆåº”ä½“ç°å¯¹æ•´ä½“æ¶æ„çš„ç†è§£\n",
    "\n",
    "æ ¼å¼...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ å¿«é€Ÿé›†æˆç¤ºä¾‹\n",
    "\n",
    "ä¸‹ä¸€ä¸ªCellåŒ…å«äº†å®Œæ•´çš„é›†æˆç¤ºä¾‹ä»£ç ï¼Œå¯ä»¥ç›´æ¥å¤åˆ¶åˆ°Cell 14æˆ–15ä¸­ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815893d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºï¼šå¸¦ä¸Šä¸‹æ–‡å¢å¼ºçš„å•æ ·æœ¬ç”Ÿæˆ\n",
    "import time\n",
    "\n",
    "# ä½¿ç”¨å‰é¢åˆå§‹åŒ–çš„åˆ†æå™¨\n",
    "try:\n",
    "    analyzer  # æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–\n",
    "except NameError:\n",
    "    # å¦‚æœæœªåˆå§‹åŒ–ï¼Œåˆ›å»ºæ–°çš„\n",
    "    try:\n",
    "        analyzer = ProjectContextAnalyzer(project_path)\n",
    "    except NameError:\n",
    "        analyzer = ProjectContextAnalyzer(Path(\"/Users/xianhaoliu/Library/CloudStorage/OneDrive-Stibo/Project/Agent-OM/ontology-llm\"))\n",
    "\n",
    "# æµ‹è¯•æ–‡ä»¶\n",
    "test_file = list(analyzer.project_path.glob(\"**/*.py\"))[0]\n",
    "print(f\"ğŸ“‚ æµ‹è¯•æ–‡ä»¶: {test_file.name}\\n\")\n",
    "\n",
    "# è¯»å–ä»£ç ç‰‡æ®µ\n",
    "with open(test_file, 'r', encoding='utf-8') as f:\n",
    "    code = f.read()\n",
    "code_snippet = code[:800]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"å¯¹æ¯”æµ‹è¯•ï¼šæ— ä¸Šä¸‹æ–‡ vs æœ‰ä¸Šä¸‹æ–‡\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== æ— ä¸Šä¸‹æ–‡ç‰ˆæœ¬ ==========\n",
    "print(\"\\nğŸ”¹ æ–¹å¼1: æ— ä¸Šä¸‹æ–‡ï¼ˆå½“å‰Cell 14-15çš„æ–¹å¼ï¼‰\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "prompt_no_context = f\"\"\"åŸºäºä»¥ä¸‹Pythonä»£ç ç”ŸæˆæŠ€æœ¯é—®ç­”ï¼š\n",
    "\n",
    "```python\n",
    "{code_snippet}\n",
    "```\n",
    "\n",
    "æ ¼å¼ï¼š\n",
    "Question: [é—®é¢˜]\n",
    "Answer: [ç­”æ¡ˆ]\n",
    "Reasoning Steps:\n",
    "1. [æ­¥éª¤1]\n",
    "2. [æ­¥éª¤2]\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Prompté•¿åº¦: {len(prompt_no_context)} å­—ç¬¦\")\n",
    "print(f\"Tokenä¼°ç®—: ~{len(prompt_no_context) // 4} tokens\")\n",
    "\n",
    "# ========== æœ‰ä¸Šä¸‹æ–‡ç‰ˆæœ¬ ==========\n",
    "print(\"\\nğŸ”¸ æ–¹å¼2: å«ä¸Šä¸‹æ–‡ï¼ˆæ¨èï¼‰\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡\n",
    "enhanced_context = analyzer.build_context(\n",
    "    code_snippet, \n",
    "    test_file, \n",
    "    context_level='standard'  # æ¨èä½¿ç”¨standard\n",
    ")\n",
    "\n",
    "prompt_with_context = f\"\"\"ä½ æ˜¯èµ„æ·±è½¯ä»¶æ¶æ„å¸ˆï¼Œè¯·åŸºäºå®Œæ•´é¡¹ç›®ä¸Šä¸‹æ–‡ç”Ÿæˆæ·±åº¦æŠ€æœ¯é—®ç­”ã€‚\n",
    "\n",
    "{enhanced_context}\n",
    "\n",
    "[ä»£ç ç‰‡æ®µ]\n",
    "```python\n",
    "{code_snippet}\n",
    "```\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "- é—®é¢˜åº”ä½“ç°å¯¹é¡¹ç›®æ•´ä½“æ¶æ„çš„ç†è§£\n",
    "- å¯æ¶‰åŠæ¨¡å—äº¤äº’ã€ä¾èµ–å…³ç³»ã€è®¾è®¡å†³ç­–\n",
    "- ç­”æ¡ˆåº”åŒ…å«æ¶æ„è§†è§’çš„åˆ†æ\n",
    "\n",
    "æ ¼å¼ï¼š\n",
    "Question: [é—®é¢˜]\n",
    "Answer: [ç­”æ¡ˆ]\n",
    "Reasoning Steps:\n",
    "1. [æ­¥éª¤1]\n",
    "2. [æ­¥éª¤2]\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Prompté•¿åº¦: {len(prompt_with_context)} å­—ç¬¦\")\n",
    "print(f\"Tokenä¼°ç®—: ~{len(prompt_with_context) // 4} tokens\")\n",
    "print(f\"Tokenå¢åŠ : +{(len(prompt_with_context) - len(prompt_no_context)) // 4} tokens ({((len(prompt_with_context) - len(prompt_no_context)) / len(prompt_no_context) * 100):.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“Š é¢„æœŸæ•ˆæœå¯¹æ¯”\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\næ— ä¸Šä¸‹æ–‡ç”Ÿæˆçš„é—®é¢˜å¯èƒ½æ˜¯:\")\n",
    "print('  â“ \"è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\"')\n",
    "print('  â“ \"å‚æ•°æœ‰å“ªäº›ï¼Ÿ\"')\n",
    "print()\n",
    "print(\"æœ‰ä¸Šä¸‹æ–‡ç”Ÿæˆçš„é—®é¢˜å¯èƒ½æ˜¯:\")\n",
    "print('  â“ \"åœ¨é¡¹ç›®æ¶æ„ä¸­ï¼Œè¯¥å‡½æ•°å¦‚ä½•ä¸å…¶ä»–æ¨¡å—åä½œï¼Ÿ\"')\n",
    "print('  â“ \"ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ç§å®ç°æ–¹å¼ï¼Ÿè®¾è®¡æƒè¡¡æ˜¯ä»€ä¹ˆï¼Ÿ\"')\n",
    "print('  â“ \"å¦‚æœè¦æ‰©å±•åˆ°åˆ†å¸ƒå¼åœºæ™¯ï¼Œéœ€è¦ä¿®æ”¹å“ªäº›éƒ¨åˆ†ï¼Ÿ\"')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ’¡ å®é™…è°ƒç”¨ç¤ºä¾‹ï¼ˆå–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# å¦‚æœæƒ³å®é™…æµ‹è¯•æ•ˆæœï¼Œå–æ¶ˆæ³¨é‡Šä»¥ä¸‹ä»£ç ï¼š\n",
    "\"\"\"\n",
    "print(\"\\nâ³ è°ƒç”¨Gemini APIç”ŸæˆQ&A...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    response = llm_service.generate_completion(prompt_with_context)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"âœ… ç”ŸæˆæˆåŠŸ! (è€—æ—¶: {elapsed:.1f}ç§’)\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ç”Ÿæˆçš„Q&A:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(response)\n",
    "    \n",
    "    # è§£æç»“æœ\n",
    "    data, method = parse_qa_from_text(response)\n",
    "    if data:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"âœ… è§£ææˆåŠŸ (æ–¹æ³•: {method})\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"é—®é¢˜: {data['question']}\")\n",
    "        print(f\"ç­”æ¡ˆé•¿åº¦: {len(data['answer'])} å­—ç¬¦\")\n",
    "        print(f\"æ¨ç†æ­¥éª¤: {len(data['reasoning_steps'])} ä¸ª\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nğŸ’¡ è¦é›†æˆåˆ°æ‰¹é‡ç”Ÿæˆï¼Œåªéœ€åœ¨Cell 15ä¸­:\")\n",
    "print(\"   1. åœ¨å¼€å§‹å‰åˆå§‹åŒ–: analyzer = ProjectContextAnalyzer(agent_om_path)\")\n",
    "print(\"   2. åœ¨ç”Ÿæˆpromptå‰æ·»åŠ : enhanced_context = analyzer.build_context(...)\")\n",
    "print(\"   3. å°†contextæ’å…¥åˆ°promptä¸­\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb0c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ æµ‹è¯•æ–‡ä»¶: llm_matching.py\n",
      "ğŸ“ å®Œæ•´è·¯å¾„: /Users/xianhaoliu/Library/CloudStorage/OneDrive-Stibo/Project/Agent-OM/ontology-llm/llm_matching.py\n",
      "âœ… æ–‡ä»¶å­˜åœ¨: True\n",
      "\n",
      "ä»£ç ç‰‡æ®µé•¿åº¦: 800 å­—ç¬¦\n",
      "ä»£ç é¢„è§ˆ:\n",
      "import run_config as config\n",
      "\n",
      "llm = config.llm\n",
      "\n",
      "# context learning\n",
      "prompt = \"Is MA_0000270 equivalent to NCI_C33736?\"\n",
      "print(llm.invoke(prompt).content)\n",
      "prompt = \"What is the meaning of chair? Give a sh...\n",
      "\n",
      "ğŸ“ æç¤ºè¯é•¿åº¦: 1000 å­—ç¬¦\n",
      "ğŸ”„ ä½¿ç”¨å®½æ¾æ–‡æœ¬è§£æç­–ç•¥ï¼ˆä¸å¼ºåˆ¶JSONï¼‰\n",
      "\n",
      "â³ è°ƒç”¨Gemini API (å¯èƒ½éœ€è¦10-30ç§’)...\n",
      "âœ… APIå“åº”æˆåŠŸ! (è€—æ—¶: 11.6ç§’)\n",
      "ğŸ“Š å“åº”é•¿åº¦: 2632 å­—ç¬¦\n",
      "\n",
      "======================================================================\n",
      "åŸå§‹å“åº”:\n",
      "Question: What LLM capabilities are being demonstrated or tested by the different prompts in this Python code snippet?\n",
      "\n",
      "Answer: This Python code snippet demonstrates and tests several key capabilities of a Large Language Model (LLM), primarily focusing on:\n",
      "\n",
      "1.  **Contextual Understanding and Knowledge Retrieval:** The LLM's ability to understand the meaning of terms, both generally and within specific contexts, and to retrieve factual information or equivalences from its knowledge base.\n",
      "2.  **Logical Reasoning (Transitive Property):** The LLM's capacity to perform basic logical inferences, specifically applying the transitive property to relationships like equivalence or subclass hierarchies.\n",
      "\n",
      "Reasoning Steps:\n",
      "1.  **Identify the Core Operation:** The code repeatedly calls `llm.invoke(prompt).content`, indicating that it's sending various text prompts to an LLM and printing its responses. This means the prompts are designed to elicit specific behaviors or demonstrate certain capabilities of the LLM.\n",
      "2.  **Analyze \"context learning\" Prompts:**\n",
      "    *   `\"Is MA_0000270 equivalent to NCI_C33736?\"`: This prompt tests the LLM's ability to retrieve specific domain-knowledge (e.g., medical or biological identifiers) and determine equivalences, which falls under **Knowledge Retrieval**.\n",
      "    *   `\"What is the meaning of chair? Give a short explanation.\"`: This tests the LLM's general semantic understanding and ability to provide a basic definition, demonstrating **General Contextual Understanding**.\n",
      "    *   `\"What is the meaning of chair in the context of conference? Give a short explanation.\"`: This prompt explicitly adds a context (\"in the context of conference\"), testing the LLM's ability to disambiguate words and provide context-specific meanings, demonstrating **Context-Specific Understanding**.\n",
      "3.  **Analyze \"transitive reasoning\" Prompts:**\n",
      "    *   `\"Prompt: We know that paper is equivalent to submission, and submission is equivalent to contribution. Is paper equivalent to contribution? Please answer yes or no. Give a short explanation.\"`: This prompt sets up a logical premise (A=B, B=C) and asks the LLM to infer the transitive relationship (A=C). This directly tests the LLM's **Logical Reasoning** capabilities, specifically the transitive property of equivalence.\n",
      "    *   `\"Prompt: We know that meta-reviewer is the subclass of reviewer, and reviewer is the subclass of co\"`: Although incomplete, this prompt clearly follows the same pattern as the previous one (A is_a B, B is_a C), aiming to test the transitive property for subclass relationships, further demonstrating **Logical Reasoning**.\n",
      "======================================================================\n",
      "\n",
      "ğŸ‰ğŸ‰ è§£ææˆåŠŸ! (æ–¹æ³•: regex)\n",
      "\n",
      "======================================================================\n",
      "é—®é¢˜:\n",
      "What LLM capabilities are being demonstrated or tested by the different prompts in this Python code snippet?\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ç­”æ¡ˆ:\n",
      "This Python code snippet demonstrates and tests several key capabilities of a Large Language Model (LLM), primarily focusing on:\n",
      "\n",
      "1.  **Contextual Understanding and Knowledge Retrieval:** The LLM's ab...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "æ¨ç†æ­¥éª¤ (3):\n",
      "  1. **Identify the Core Operation:** The code repeatedly calls `llm.invoke(prompt).content`, indicating ...\n",
      "  2. **Analyze \"context learning\" Prompts:**\n",
      "    *   `\"Is MA_0000270 equivalent to NCI_C33736?\"`: This pr...\n",
      "  3. **Analyze \"transitive reasoning\" Prompts:**\n",
      "    *   `\"Prompt: We know that paper is equivalent to su...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ç®€åŒ–çš„å•æ ·æœ¬æµ‹è¯• - ä½¿ç”¨å®½æ¾çš„æ–‡æœ¬è§£æ\n",
    "import time\n",
    "import re\n",
    "\n",
    "# ä½¿ç”¨GitHubé›†æˆé…ç½®çš„è·¯å¾„ï¼ˆå¦‚æœæœªé…ç½®åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰\n",
    "try:\n",
    "    agent_om_path = project_path  # ä½¿ç”¨å‰é¢é…ç½®çš„é¡¹ç›®è·¯å¾„\n",
    "    print(f\"âœ… ä½¿ç”¨å·²é…ç½®çš„é¡¹ç›®: {agent_om_path}\")\n",
    "except NameError:\n",
    "    # æœªé…ç½®project_pathï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„\n",
    "    agent_om_path = Path(\"/Users/xianhaoliu/Library/CloudStorage/OneDrive-Stibo/Project/Agent-OM/ontology-llm\")\n",
    "    print(f\"âš ï¸  ä½¿ç”¨é»˜è®¤è·¯å¾„: {agent_om_path}\")\n",
    "    print(f\"   å»ºè®®å…ˆè¿è¡ŒGitHubé›†æˆé…ç½®Cell\")\n",
    "\n",
    "test_file = \"llm_matching.py\"\n",
    "file_path = agent_om_path / test_file\n",
    "\n",
    "print(f\"ğŸ“‚ æµ‹è¯•æ–‡ä»¶: {test_file}\")\n",
    "print(f\"ğŸ“ å®Œæ•´è·¯å¾„: {file_path}\")\n",
    "print(f\"âœ… æ–‡ä»¶å­˜åœ¨: {file_path.exists()}\")\n",
    "print()\n",
    "\n",
    "def parse_qa_from_text(text):\n",
    "    \"\"\"\n",
    "    ä»è‡ªç”±æ–‡æœ¬ä¸­è§£æQ&Aç»“æ„ï¼Œä¸è¦æ±‚ä¸¥æ ¼JSON\n",
    "    æ”¯æŒå¤šç§æ ¼å¼\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"question\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"reasoning_steps\": []\n",
    "    }\n",
    "    \n",
    "    # å°è¯•1: å…ˆå°è¯•JSONè§£æ\n",
    "    try:\n",
    "        # æ¸…ç†å¹¶æå–JSON\n",
    "        cleaned = text.strip()\n",
    "        cleaned = re.sub(r'^```json\\s*', '', cleaned)\n",
    "        cleaned = re.sub(r'^```\\s*', '', cleaned)\n",
    "        cleaned = re.sub(r'\\s*```$', '', cleaned)\n",
    "        cleaned = cleaned.strip()\n",
    "        \n",
    "        json_match = re.search(r'\\{.*\\}', cleaned, re.DOTALL)\n",
    "        if json_match:\n",
    "            cleaned = json_match.group(0)\n",
    "            cleaned = cleaned.replace('`', '\\'')\n",
    "            \n",
    "            data = json.loads(cleaned)\n",
    "            if 'question' in data and 'answer' in data:\n",
    "                return data, \"json\"\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # å°è¯•2: æ­£åˆ™è¡¨è¾¾å¼è§£æç»“æ„åŒ–æ–‡æœ¬\n",
    "    # åŒ¹é… Question: ... æˆ– é—®é¢˜: ...\n",
    "    question_patterns = [\n",
    "        r'[Qq]uestion\\s*[:ï¼š]\\s*(.+?)(?=\\n[Aa]nswer|å›ç­”|ç­”æ¡ˆ|$)',\n",
    "        r'é—®é¢˜\\s*[:ï¼š]\\s*(.+?)(?=\\n[Aa]nswer|å›ç­”|ç­”æ¡ˆ|$)',\n",
    "        r'##\\s*[Qq]uestion\\s*\\n(.+?)(?=\\n##|$)',\n",
    "        r'##\\s*é—®é¢˜\\s*\\n(.+?)(?=\\n##|$)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in question_patterns:\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            result[\"question\"] = match.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    # åŒ¹é… Answer: ... æˆ– ç­”æ¡ˆ: ...\n",
    "    answer_patterns = [\n",
    "        r'[Aa]nswer\\s*[:ï¼š]\\s*(.+?)(?=\\n[Rr]easoning|æ¨ç†|æ­¥éª¤|$)',\n",
    "        r'[å›ç­”ç­”]\\s*[:ï¼š]\\s*(.+?)(?=\\n[Rr]easoning|æ¨ç†|æ­¥éª¤|$)',\n",
    "        r'##\\s*[Aa]nswer\\s*\\n(.+?)(?=\\n##|$)',\n",
    "        r'##\\s*[å›ç­”ç­”]\\s*\\n(.+?)(?=\\n##|$)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in answer_patterns:\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            result[\"answer\"] = match.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    # åŒ¹é…æ¨ç†æ­¥éª¤ - åˆ—è¡¨æ ¼å¼\n",
    "    reasoning_patterns = [\n",
    "        r'[Rr]easoning [Ss]teps?\\s*[:ï¼š]\\s*\\n((?:\\d+\\.|\\-|\\*).+?)(?=\\n\\n|$)',\n",
    "        r'æ¨ç†æ­¥éª¤\\s*[:ï¼š]\\s*\\n((?:\\d+\\.|\\-|\\*).+?)(?=\\n\\n|$)',\n",
    "        r'##\\s*[Rr]easoning\\s*\\n((?:\\d+\\.|\\-|\\*).+?)(?=\\n##|$)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in reasoning_patterns:\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            steps_text = match.group(1)\n",
    "            # æå–æ¯ä¸€æ­¥\n",
    "            steps = re.findall(r'(?:\\d+\\.|\\-|\\*)\\s*(.+?)(?=\\n\\d+\\.|\\n\\-|\\n\\*|$)', steps_text, re.DOTALL)\n",
    "            result[\"reasoning_steps\"] = [s.strip() for s in steps if s.strip()]\n",
    "            break\n",
    "    \n",
    "    # å¦‚æœquestionå’Œansweréƒ½æ‰¾åˆ°äº†ï¼Œè¿”å›æˆåŠŸ\n",
    "    if result[\"question\"] and result[\"answer\"]:\n",
    "        return result, \"regex\"\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "if file_path.exists():\n",
    "    # è¯»å–æ–‡ä»¶\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        code = f.read()\n",
    "    \n",
    "    # ä½¿ç”¨çŸ­ä»£ç ç‰‡æ®µ\n",
    "    code_snippet = code[:800]\n",
    "    \n",
    "    print(f\"ä»£ç ç‰‡æ®µé•¿åº¦: {len(code_snippet)} å­—ç¬¦\")\n",
    "    print(f\"ä»£ç é¢„è§ˆ:\\n{code_snippet[:200]}...\\n\")\n",
    "    \n",
    "    # å®½æ¾çš„æç¤ºè¯ - ä¸å¼ºåˆ¶JSONæ ¼å¼\n",
    "    flexible_prompt = f\"\"\"åŸºäºä»¥ä¸‹Pythonä»£ç ï¼Œç”Ÿæˆä¸€ä¸ªæŠ€æœ¯é—®ç­”ã€‚\n",
    "\n",
    "ä»£ç :\n",
    "```python\n",
    "{code_snippet}\n",
    "```\n",
    "\n",
    "è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼ˆå¯ä»¥ç”¨ä¸­æ–‡æˆ–è‹±æ–‡ï¼‰ï¼š\n",
    "\n",
    "Question: [ä½ çš„é—®é¢˜]\n",
    "\n",
    "Answer: [ä½ çš„ç­”æ¡ˆ]\n",
    "\n",
    "Reasoning Steps:\n",
    "1. [æ¨ç†æ­¥éª¤1]\n",
    "2. [æ¨ç†æ­¥éª¤2]\n",
    "3. [æ¨ç†æ­¥éª¤3]\n",
    "\n",
    "æ³¨æ„ï¼šå¯ä»¥ä½¿ç”¨ä»»ä½•æ ¼å¼ï¼Œåªè¦åŒ…å«Questionã€Answerã€Reasoning Stepså³å¯ã€‚\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“ æç¤ºè¯é•¿åº¦:\", len(flexible_prompt), \"å­—ç¬¦\")\n",
    "    print(\"ğŸ”„ ä½¿ç”¨å®½æ¾æ–‡æœ¬è§£æç­–ç•¥ï¼ˆä¸å¼ºåˆ¶JSONï¼‰\")\n",
    "    print()\n",
    "    print(\"â³ è°ƒç”¨Gemini API (å¯èƒ½éœ€è¦10-30ç§’)...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = llm_service.generate_completion(flexible_prompt)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"âœ… APIå“åº”æˆåŠŸ! (è€—æ—¶: {elapsed:.1f}ç§’)\")\n",
    "        print(f\"ğŸ“Š å“åº”é•¿åº¦: {len(response)} å­—ç¬¦\")\n",
    "        print()\n",
    "        print(\"=\" * 70)\n",
    "        print(\"åŸå§‹å“åº”:\")\n",
    "        print(response)\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        \n",
    "        # ä½¿ç”¨å®½æ¾è§£æ\n",
    "        data, method = parse_qa_from_text(response)\n",
    "        \n",
    "        if data:\n",
    "            print(f\"ğŸ‰ğŸ‰ è§£ææˆåŠŸ! (æ–¹æ³•: {method})\")\n",
    "            print()\n",
    "            print(\"=\" * 70)\n",
    "            print(\"é—®é¢˜:\")\n",
    "            print(data.get('question', 'N/A'))\n",
    "            print(\"\\n\" + \"-\" * 70)\n",
    "            print(\"ç­”æ¡ˆ:\")\n",
    "            answer = data.get('answer', 'N/A')\n",
    "            print(answer[:200] + (\"...\" if len(answer) > 200 else \"\"))\n",
    "            print(\"\\n\" + \"-\" * 70)\n",
    "            print(f\"æ¨ç†æ­¥éª¤ ({len(data.get('reasoning_steps', []))}):\")\n",
    "            for i, step in enumerate(data.get('reasoning_steps', []), 1):\n",
    "                print(f\"  {i}. {step[:100]}{'...' if len(step) > 100 else ''}\")\n",
    "            print(\"=\" * 70)\n",
    "        else:\n",
    "            print(\"âŒ è§£æå¤±è´¥: æ— æ³•ä»å“åº”ä¸­æå–Q&Aç»“æ„\")\n",
    "            print(\"\\nğŸ’¡ å“åº”å¯èƒ½ä¸åŒ…å«å¿…éœ€çš„ Question/Answer å­—æ®µ\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"âŒ APIè°ƒç”¨å¤±è´¥! (è€—æ—¶: {elapsed:.1f}ç§’)\")\n",
    "        print(f\"é”™è¯¯: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"âŒ æ–‡ä»¶ä¸å­˜åœ¨\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6bb0d",
   "metadata": {},
   "source": [
    "## 6.1 å•æ ·æœ¬æµ‹è¯•ï¼šéªŒè¯å®½æ¾è§£æç­–ç•¥\n",
    "\n",
    "### ğŸ“‹ æµ‹è¯•ç›®çš„\n",
    "\n",
    "åœ¨æ‰¹é‡ç”Ÿæˆå‰ï¼Œå…ˆéªŒè¯æ–°çš„å®½æ¾æ–‡æœ¬è§£æç­–ç•¥æ˜¯å¦æœ‰æ•ˆï¼Œé¿å…é‡å¤ä¹‹å‰çš„0%æˆåŠŸç‡é—®é¢˜ã€‚é¦–æ¬¡è¿è¡Œé¢„è®¡15-25ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…å®Œæ•´è¾“å‡ºã€‚\n",
    "\n",
    "### â±ï¸ æ—¶é—´å»ºè®®\n",
    "\n",
    "### ğŸ”§ æµ‹è¯•é…ç½®\n",
    "\n",
    "- **ç›®æ ‡æ–‡ä»¶**ï¼š`llm_matching.py`ï¼ˆAgent-OMæ ¸å¿ƒæ–‡ä»¶ï¼‰- **å¤±è´¥** â†’ åˆ†æåŸå§‹å“åº”ï¼Œè°ƒæ•´`parse_qa_from_text()`å‡½æ•°çš„æ­£åˆ™æ¨¡å¼\n",
    "\n",
    "- **ä»£ç ç‰‡æ®µ**ï¼šå‰800å­—ç¬¦- **æˆåŠŸ** â†’ æ‰§è¡ŒCell 15æ‰¹é‡ç”Ÿæˆ\n",
    "\n",
    "- **è§£æå‡½æ•°**ï¼š`parse_qa_from_text()` - æ”¯æŒJSONå’Œè‡ªç„¶è¯­è¨€åŒæ¨¡å¼### ğŸš¦ ä¸‹ä¸€æ­¥å†³ç­–\n",
    "\n",
    "- **è¶…æ—¶é¢„æœŸ**ï¼š10-30ç§’\n",
    "\n",
    "  - è¿›ä¸€æ­¥é™ä½temperature\n",
    "\n",
    "### ğŸ¯ å…³é”®æŠ€æœ¯ç‚¹  - ä¿®æ”¹æç¤ºè¯æªè¾\n",
    "\n",
    "  - è°ƒæ•´æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼\n",
    "\n",
    "#### 1. åŒå±‚è§£æç­–ç•¥- **å¦‚æœå¤±è´¥**ï¼šæŸ¥çœ‹åŸå§‹å“åº”ï¼Œå¯èƒ½éœ€è¦ï¼š\n",
    "\n",
    "```python- **ä¹è§‚ä¼°è®¡**ï¼š70-80%æ¦‚ç‡æˆåŠŸï¼ˆåŸºäºå®½æ¾è§£æï¼‰\n",
    "\n",
    "def parse_qa_from_text(text):### ğŸ’¡ é¢„æœŸç»“æœ\n",
    "\n",
    "    # Layer 1: JSONè§£æï¼ˆä¼˜å…ˆï¼‰\n",
    "\n",
    "    try:7. **æå–çš„ç»“æ„åŒ–æ•°æ®**\n",
    "\n",
    "        json_match = re.search(r'\\{.*\\}', text, re.DOTALL)6. **è§£ææ–¹æ³•æ ‡è®°**ï¼ˆjson/regexï¼‰\n",
    "\n",
    "        data = json.loads(json_match.group(0))5. **å®Œæ•´åŸå§‹å“åº”**ï¼ˆä¾¿äºè°ƒè¯•ï¼‰\n",
    "\n",
    "        return data, \"json\"  # æ ‡è®°è§£ææ–¹æ³•4. **APIè°ƒç”¨è€—æ—¶**\n",
    "\n",
    "    except:3. **æç¤ºè¯é•¿åº¦ç»Ÿè®¡**\n",
    "\n",
    "        pass2. **ä»£ç ç‰‡æ®µé•¿åº¦å’Œé¢„è§ˆ**\n",
    "\n",
    "    1. **æ–‡ä»¶è·¯å¾„å’Œå­˜åœ¨æ€§æ£€æŸ¥**\n",
    "\n",
    "    # Layer 2: æ­£åˆ™è¡¨è¾¾å¼è§£æï¼ˆå…œåº•ï¼‰æ­¤æµ‹è¯•ä¼šæ˜¾ç¤ºï¼š\n",
    "\n",
    "    question = extract_question(text)  # æ”¯æŒå¤šç§æ¨¡å¼### ğŸ” è°ƒè¯•ä¿¡æ¯\n",
    "\n",
    "    answer = extract_answer(text)\n",
    "\n",
    "    reasoning = extract_reasoning(text)- âœ… å†…å®¹ä¸ä»£ç ç›¸å…³ä¸”é€»è¾‘åˆç†\n",
    "\n",
    "    return {\"question\": question, ...}, \"regex\"- âœ… ä¸‰ä¸ªå¿…éœ€å­—æ®µéƒ½éç©ºï¼ˆquestion, answer, reasoning_stepsï¼‰\n",
    "\n",
    "```- âœ… è§£ææ–¹æ³•æ˜¾ç¤ºä¸º\"json\"æˆ–\"regex\"\n",
    "\n",
    "- âœ… APIè°ƒç”¨æ— å¼‚å¸¸\n",
    "\n",
    "#### 2. å¤šæ¨¡å¼æ­£åˆ™åŒ¹é…### âœ… æˆåŠŸæ ‡å‡†\n",
    "\n",
    "æ”¯æŒ4ç§é—®é¢˜æ ¼å¼ï¼š\n",
    "\n",
    "- `Question: ...`ï¼ˆè‹±æ–‡å†’å·ï¼‰```\n",
    "\n",
    "- `é—®é¢˜: ...`ï¼ˆä¸­æ–‡ï¼‰ğŸ’¡ å“åº”å¯èƒ½ä¸åŒ…å«å¿…éœ€çš„ Question/Answer å­—æ®µ\n",
    "\n",
    "- `## Question\\n...`ï¼ˆMarkdownæ ‡é¢˜ï¼‰âŒ è§£æå¤±è´¥: æ— æ³•ä»å“åº”ä¸­æå–Q&Aç»“æ„\n",
    "\n",
    "- `é—®é¢˜ï¼š...`ï¼ˆä¸­æ–‡å…¨è§’å†’å·ï¼‰```\n",
    "\n",
    "**å¤±è´¥ç¤ºä¾‹**ï¼š\n",
    "\n",
    "#### 3. å®½æ¾æç¤ºè¯è®¾è®¡\n",
    "\n",
    "``````\n",
    "\n",
    "æ—§ç‰ˆï¼ˆä¸¥æ ¼ï¼‰ï¼š\"è¿”å›çº¯JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—\"ğŸ‰ğŸ‰ è§£ææˆåŠŸ! (æ–¹æ³•: regex)\n",
    "\n",
    "æ–°ç‰ˆï¼ˆå®½æ¾ï¼‰ï¼š\"å¯ä»¥ä½¿ç”¨ä»»ä½•æ ¼å¼ï¼Œåªè¦åŒ…å«Questionã€Answerã€Reasoning Stepså³å¯\"\n",
    "\n",
    "```3. ...\n",
    "\n",
    "2. ä»£ç ç»“æ„åŒ…å«å®ä½“åŒ¹é…çš„æ ¸å¿ƒé€»è¾‘\n",
    "\n",
    "### ğŸ“Š è¾“å‡ºåˆ†æ1. æ–‡ä»¶åæš—ç¤ºè¿™æ˜¯LLMç›¸å…³çš„åŒ¹é…æ¨¡å—\n",
    "\n",
    "Reasoning Steps:\n",
    "\n",
    "**æˆåŠŸè¾“å‡ºç¤ºä¾‹**ï¼šAnswer: è¯¥æ–‡ä»¶å®ç°äº†åŸºäºLLMçš„æœ¬ä½“åŒ¹é…åŠŸèƒ½...\n",
    "\n",
    "```Question: llm_matching.pyä¸­ä¸»è¦å®ç°äº†ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ\n",
    "\n",
    "âœ… APIå“åº”æˆåŠŸ! (è€—æ—¶: 12.3ç§’)åŸå§‹å“åº”:\n",
    "\n",
    "ğŸ“Š å“åº”é•¿åº¦: 1247 å­—ç¬¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cd093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Agent-OM é¡¹ç›®è·¯å¾„: /Users/xianhaoliu/Library/CloudStorage/OneDrive-Stibo/Project/Agent-OM/ontology-llm\n",
      "ğŸ¯ ç›®æ ‡: æ”¶é›† 10 ä¸ªæˆåŠŸçš„ Q&A å¯¹\n",
      "ğŸ”„ ç­–ç•¥: å®½æ¾æ–‡æœ¬è§£æï¼ˆä¸å¼ºåˆ¶JSONæ ¼å¼ï¼‰\n",
      "\n",
      "âœ… llm_matching.py\n",
      "âœ… util.py\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ å¼€å§‹ç”Ÿæˆï¼Œç›®æ ‡ 10 ä¸ª...\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ [å°è¯• 1/20] [æˆåŠŸ 0/10] - llm_matching.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.4s, 443å­—ç¬¦)\n",
      "  âš ï¸  è§£æå¤±è´¥\n",
      "\n",
      "  å“åº”å†…å®¹:\n",
      "  å¥½çš„ï¼ŒåŸºäºæ‚¨æä¾›çš„Pythonä»£ç ç‰‡æ®µï¼Œæˆ‘å°†ç”Ÿæˆä»¥ä¸‹æŠ€æœ¯é—®ç­”ï¼š\n",
      "\n",
      "---\n",
      "\n",
      "**Question 1: è¿™æ®µPythonä»£ç ç‰‡æ®µçš„ä¸»è¦ç›®çš„æ˜¯ä»€ä¹ˆï¼Ÿå®ƒå±•ç¤ºäº†LLMçš„å“ªäº›æ ¸å¿ƒèƒ½åŠ›ï¼Ÿ**\n",
      "\n",
      "Answer: è¿™æ®µä»£ç ç‰‡æ®µçš„ä¸»è¦ç›®çš„æ˜¯æ¼”ç¤ºå¦‚ä½•é€šè¿‡Pythonä»£ç ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œäº¤äº’ï¼Œå¹¶å±•ç¤ºLLMçš„ä¸¤ç§å…³é”®èƒ½åŠ›ï¼š**ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆContext Learningï¼‰** å’Œ **ä¼ é€’æ€§æ¨ç†ï¼ˆTransitive Reasoningï¼‰**ã€‚\n",
      "\n",
      "Reasoning Steps:\n",
      "1.  ä»£ç é€šè¿‡ `llm = config.llm` (å°½ç®¡ä»£ç ç‰‡æ®µä¸å®Œæ•´ï¼Œä½†å¯ä»¥æ¨æ–­) åˆå§‹åŒ–äº†ä¸€ä¸ªLLMå®¢æˆ·ç«¯æˆ–SDK...\n",
      "\n",
      "\n",
      "ğŸ“ [å°è¯• 2/20] [æˆåŠŸ 0/10] - util.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.3s, 172å­—ç¬¦)\n",
      "  âš ï¸  è§£æå¤±è´¥\n",
      "\n",
      "  å“åº”å†…å®¹:\n",
      "  å¥½çš„ï¼ŒåŸºäºæ‚¨æä¾›çš„Pythonä»£ç ç‰‡æ®µï¼Œæˆ‘å°†ç”Ÿæˆä¸€äº›æŠ€æœ¯é—®ç­”ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### **ä»£ç ç‰‡æ®µ 1: åç§°æ¸…ç†å‡½æ•° (å‡è®¾ä¸ºä¸€ä¸ªåä¸º `clean_name` çš„å‡½æ•°å†…éƒ¨)**\n",
      "\n",
      "```python\n",
      "    cleaned_name = re.sub(r'[^A-Za-z0-9]+', ' ', str(name))\n",
      "    # if no...\n",
      "\n",
      "\n",
      "ğŸ“ [å°è¯• 3/20] [æˆåŠŸ 0/10] - llm_matching.py\n",
      "  â³ è°ƒç”¨API... âœ… (12.7s, 2198å­—ç¬¦)\n",
      "  âš ï¸  è§£æå¤±è´¥\n",
      "\n",
      "  å“åº”å†…å®¹:\n",
      "  å¥½çš„ï¼ŒåŸºäºæ‚¨æä¾›çš„Pythonä»£ç ï¼Œä»¥ä¸‹æ˜¯ç”Ÿæˆçš„æŠ€æœ¯é—®ç­”ï¼š\n",
      "\n",
      "---\n",
      "\n",
      "**Question 1:** What is the primary mechanism used in this Python code to interact with a Large Language Model (LLM) and retrieve its response?\n",
      "\n",
      "**Answer:** The primary mechanism is calling the `invoke()` method on an `llm` object, passing a `prompt` string, and t...\n",
      "\n",
      "\n",
      "ğŸ“ [å°è¯• 4/20] [æˆåŠŸ 0/10] - util.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.1s, 1430å­—ç¬¦)\n",
      "  âš ï¸  è§£æå¤±è´¥\n",
      "\n",
      "ğŸ“ [å°è¯• 5/20] [æˆåŠŸ 0/10] - llm_matching.py\n",
      "  â³ è°ƒç”¨API... âœ… (10.4s, 847å­—ç¬¦)\n",
      "  ğŸ‰ ç¬¬ 1 ä¸ªæˆåŠŸ! (è§£æ: regex)\n",
      "     Q: è¿™æ®µPythonä»£ç ä¸»è¦æ¼”ç¤ºäº†LLMåœ¨å¤„ç†å“ªç§ç±»å‹çš„é€»è¾‘æ¨ç†ä»»åŠ¡ï¼Ÿ...\n",
      "\n",
      "ğŸ“ [å°è¯• 6/20] [æˆåŠŸ 1/10] - util.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.1s, 409å­—ç¬¦)\n",
      "  âš ï¸  è§£æå¤±è´¥\n",
      "\n",
      "ğŸ“ [å°è¯• 7/20] [æˆåŠŸ 1/10] - llm_matching.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.1s, 1404å­—ç¬¦)\n",
      "  âš ï¸  è§£æå¤±è´¥\n",
      "\n",
      "ğŸ“ [å°è¯• 8/20] [æˆåŠŸ 1/10] - util.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.4s, 876å­—ç¬¦)\n",
      "  âš ï¸  è§£æå¤±è´¥\n",
      "\n",
      "ğŸ“ [å°è¯• 9/20] [æˆåŠŸ 1/10] - llm_matching.py\n",
      "  â³ è°ƒç”¨API... âœ… (10.8s, 1036å­—ç¬¦)\n",
      "  ğŸ‰ ç¬¬ 2 ä¸ªæˆåŠŸ! (è§£æ: regex)\n",
      "     Q: ** è¿™æ®µPythonä»£ç ä¸»è¦æ¼”ç¤ºäº†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº¤äº’çš„å“ªäº›åŠŸèƒ½ï¼Œä»¥åŠå®ƒåœ¨æµ‹è¯•LLMçš„å“ªäº›æ¨ç†èƒ½åŠ›ï¼Ÿ\n",
      "\n",
      "**A...\n",
      "\n",
      "ğŸ“ [å°è¯• 10/20] [æˆåŠŸ 2/10] - util.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.3s, 1798å­—ç¬¦)\n",
      "  ğŸ‰ ç¬¬ 3 ä¸ªæˆåŠŸ! (è§£æ: regex)\n",
      "     Q: `colorama.init(autoreset=True)` è¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ...\n",
      "\n",
      "ğŸ“ [å°è¯• 11/20] [æˆåŠŸ 3/10] - llm_matching.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.2s, 322å­—ç¬¦)\n",
      "  ğŸ‰ ç¬¬ 4 ä¸ªæˆåŠŸ! (è§£æ: regex)\n",
      "     Q: è¿™æ®µPythonä»£ç ç‰‡æ®µçš„ä¸»è¦ç›®çš„æ˜¯ä»€ä¹ˆï¼Ÿ**\n",
      "\n",
      "**Answer:** è¿™æ®µPythonä»£ç ç‰‡æ®µçš„ä¸»è¦ç›®çš„æ˜¯é€šè¿‡å‘å¤§å‹...\n",
      "\n",
      "ğŸ“ [å°è¯• 12/20] [æˆåŠŸ 4/10] - util.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.6s, 1171å­—ç¬¦)\n",
      "  âš ï¸  è§£æå¤±è´¥\n",
      "\n",
      "ğŸ“ [å°è¯• 13/20] [æˆåŠŸ 4/10] - llm_matching.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.1s, 1858å­—ç¬¦)\n",
      "  ğŸ‰ ç¬¬ 5 ä¸ªæˆåŠŸ! (è§£æ: regex)\n",
      "     Q: 1**\n",
      "What is the primary purpose of the provided Python code ...\n",
      "\n",
      "ğŸ“ [å°è¯• 14/20] [æˆåŠŸ 5/10] - util.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.2s, 1618å­—ç¬¦)\n",
      "  ğŸ‰ ç¬¬ 6 ä¸ªæˆåŠŸ! (è§£æ: regex)\n",
      "     Q: è¿™æ®µä»£ç çš„æ ¸å¿ƒåŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿå…·ä½“ä½¿ç”¨äº†Pandasåº“çš„å“ªäº›æ“ä½œï¼Ÿ...\n",
      "\n",
      "ğŸ“ [å°è¯• 15/20] [æˆåŠŸ 6/10] - llm_matching.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.0s, 360å­—ç¬¦)\n",
      "  ğŸ‰ ç¬¬ 7 ä¸ªæˆåŠŸ! (è§£æ: regex)\n",
      "     Q: ** What is the primary purpose of the provided Python code s...\n",
      "\n",
      "ğŸ“ [å°è¯• 16/20] [æˆåŠŸ 7/10] - util.py\n",
      "  â³ è°ƒç”¨API... âœ… (10.0s, 283å­—ç¬¦)\n",
      "  âš ï¸  è§£æå¤±è´¥\n",
      "\n",
      "ğŸ“ [å°è¯• 17/20] [æˆåŠŸ 7/10] - llm_matching.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.1s, 1614å­—ç¬¦)\n",
      "  ğŸ‰ ç¬¬ 8 ä¸ªæˆåŠŸ! (è§£æ: regex)\n",
      "     Q: è¿™æ®µä»£ç ç‰‡æ®µä¸»è¦å±•ç¤ºäº†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº¤äº’çš„å“ªäº›æ ¸å¿ƒèƒ½åŠ›ï¼Ÿ...\n",
      "\n",
      "ğŸ“ [å°è¯• 18/20] [æˆåŠŸ 8/10] - util.py\n",
      "  â³ è°ƒç”¨API... âœ… (11.9s, 165å­—ç¬¦)\n",
      "  ğŸ‰ ç¬¬ 9 ä¸ªæˆåŠŸ! (è§£æ: regex)\n",
      "     Q: åˆå§‹ä»£ç ç‰‡æ®µï¼ˆæœªå®Œæ•´æ˜¾ç¤ºå‡½æ•°å®šä¹‰ï¼‰çš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿå®ƒå°†å“ªäº›æ•°æ®å†™å…¥CSVæ–‡ä»¶ï¼Œå¹¶ä»¥ä½•ç§æ ¼å¼å‘ˆç°ï¼Ÿ...\n",
      "\n",
      "ğŸ“ [å°è¯• 19/20] [æˆåŠŸ 9/10] - llm_matching.py\n",
      "  â³ è°ƒç”¨API... âœ… (10.9s, 1057å­—ç¬¦)\n",
      "  ğŸ‰ ç¬¬ 10 ä¸ªæˆåŠŸ! (è§£æ: regex)\n",
      "     Q: What is the primary purpose of the Python code snippet provi...\n",
      "\n",
      "âœ¨ å·²è¾¾åˆ°ç›®æ ‡ 10 ä¸ª!\n",
      "\n",
      "======================================================================\n",
      "ğŸ ç”Ÿæˆå®Œæˆ!\n",
      "âœ… æˆåŠŸ: 10 / 19 å°è¯•\n",
      "ğŸ“ˆ æˆåŠŸç‡: 52.6%\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‹ æ ·æœ¬é¢„è§ˆ:\n",
      "\n",
      "  1. llm_matching.py\n",
      "     Q: è¿™æ®µPythonä»£ç ä¸»è¦æ¼”ç¤ºäº†LLMåœ¨å¤„ç†å“ªç§ç±»å‹çš„é€»è¾‘æ¨ç†ä»»åŠ¡ï¼Ÿ...\n",
      "     A: 20 å­—ç¬¦\n",
      "     æ­¥éª¤: 1 ä¸ª\n",
      "\n",
      "  2. llm_matching.py\n",
      "     Q: ** è¿™æ®µPythonä»£ç ä¸»è¦æ¼”ç¤ºäº†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº¤äº’çš„å“ªäº›åŠŸèƒ½ï¼Œä»¥åŠå®ƒåœ¨æµ‹è¯•LLMçš„å“ªäº›æ¨ç†èƒ½åŠ›ï¼Ÿ\n",
      "\n",
      "**Answer:** è¿™...\n",
      "     A: 74 å­—ç¬¦\n",
      "     æ­¥éª¤: 1 ä¸ª\n",
      "\n",
      "  3. util.py\n",
      "     Q: `colorama.init(autoreset=True)` è¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ...\n",
      "     A: 201 å­—ç¬¦\n",
      "     æ­¥éª¤: 1 ä¸ª\n"
     ]
    }
   ],
   "source": [
    "# æ‰¹é‡ç”Ÿæˆ - ä½¿ç”¨å®½æ¾æ–‡æœ¬è§£æç­–ç•¥\n",
    "import time\n",
    "\n",
    "# ä½¿ç”¨GitHubé›†æˆé…ç½®çš„è·¯å¾„ï¼ˆå¦‚æœæœªé…ç½®åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰\n",
    "try:\n",
    "    agent_om_path = project_path\n",
    "    print(f\"âœ… ä½¿ç”¨å·²é…ç½®çš„é¡¹ç›®: {agent_om_path}\")\n",
    "except NameError:\n",
    "    agent_om_path = Path(\"/Users/xianhaoliu/Library/CloudStorage/OneDrive-Stibo/Project/Agent-OM/ontology-llm\")\n",
    "    print(f\"âš ï¸  ä½¿ç”¨é»˜è®¤è·¯å¾„: {agent_om_path}\")\n",
    "    print(f\"   å»ºè®®å…ˆè¿è¡ŒGitHubé›†æˆé…ç½®Cell\")\n",
    "    print()\n",
    "\n",
    "# è‡ªåŠ¨å‘ç°Pythonæ–‡ä»¶ï¼ˆå¦‚æœç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ï¼‰\n",
    "target_files = [\"llm_matching.py\", \"util.py\"]\n",
    "\n",
    "# æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨æœç´¢\n",
    "existing_files = []\n",
    "for filename in target_files:\n",
    "    if (agent_om_path / filename).exists():\n",
    "        existing_files.append(filename)\n",
    "\n",
    "if not existing_files:\n",
    "    print(\"âš ï¸  é¢„è®¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨æœç´¢Pythonæ–‡ä»¶...\")\n",
    "    all_py_files = list(agent_om_path.glob(\"**/*.py\"))\n",
    "    # æ’é™¤æµ‹è¯•æ–‡ä»¶å’Œ__init__.py\n",
    "    target_files = [f.name for f in all_py_files[:5] if 'test' not in f.name.lower() and f.name != '__init__.py']\n",
    "    print(f\"   æ‰¾åˆ° {len(all_py_files)} ä¸ªPythonæ–‡ä»¶\")\n",
    "    print(f\"   å°†ä½¿ç”¨å‰5ä¸ª: {target_files}\")\n",
    "else:\n",
    "    target_files = existing_files\n",
    "    print(f\"âœ… æ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: {target_files}\")\n",
    "\n",
    "TARGET_QA_COUNT = 5\n",
    "MAX_ATTEMPTS = 10\n",
    "\n",
    "print()\n",
    "print(f\"ğŸ“ é¡¹ç›®è·¯å¾„: {agent_om_path}\")\n",
    "print(f\"ğŸ¯ ç›®æ ‡: æ”¶é›† {TARGET_QA_COUNT} ä¸ªæˆåŠŸçš„ Q&A å¯¹\")\n",
    "print(f\"ğŸ“ é‡‡æ ·æ–‡ä»¶: {target_files}\")\n",
    "print(f\"ğŸ”„ ç­–ç•¥: å®½æ¾æ–‡æœ¬è§£æï¼ˆä¸å¼ºåˆ¶JSONæ ¼å¼ï¼‰\")\n",
    "print()\n",
    "\n",
    "# å®½æ¾è§£æå‡½æ•°\n",
    "def parse_qa_flexible(text):\n",
    "    \"\"\"ä»è‡ªç”±æ–‡æœ¬ä¸­è§£æQ&Aï¼Œæ”¯æŒå¤šç§æ ¼å¼\"\"\"\n",
    "    result = {\"question\": \"\", \"answer\": \"\", \"reasoning_steps\": []}\n",
    "    \n",
    "    # å°è¯•JSONï¼ˆå¦‚æœLLMè‡ªå·±è¿”å›äº†JSONï¼‰\n",
    "    try:\n",
    "        cleaned = re.sub(r'^```(?:json)?\\s*', '', text.strip())\n",
    "        cleaned = re.sub(r'\\s*```$', '', cleaned).strip()\n",
    "        json_match = re.search(r'\\{.*\\}', cleaned, re.DOTALL)\n",
    "        if json_match:\n",
    "            data = json.loads(json_match.group(0).replace('`', '\\''))\n",
    "            if 'question' in data and 'answer' in data:\n",
    "                return data, \"json\"\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # æ­£åˆ™è§£æ\n",
    "    # Question\n",
    "    for p in [r'[Qq]uestion\\s*[:ï¼š]\\s*(.+?)(?=\\n+[Aa]nswer|å›ç­”|ç­”æ¡ˆ|##|\\Z)',\n",
    "              r'é—®é¢˜\\s*[:ï¼š]\\s*(.+?)(?=\\n+[Aa]nswer|å›ç­”|ç­”æ¡ˆ|##|\\Z)']:\n",
    "        m = re.search(p, text, re.DOTALL | re.IGNORECASE)\n",
    "        if m:\n",
    "            result[\"question\"] = m.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    # Answer\n",
    "    for p in [r'[Aa]nswer\\s*[:ï¼š]\\s*(.+?)(?=\\n+[Rr]easoning|æ¨ç†|æ­¥éª¤|##|\\Z)',\n",
    "              r'[å›ç­”ç­”æ¡ˆ]\\s*[:ï¼š]\\s*(.+?)(?=\\n+[Rr]easoning|æ¨ç†|æ­¥éª¤|##|\\Z)']:\n",
    "        m = re.search(p, text, re.DOTALL | re.IGNORECASE)\n",
    "        if m:\n",
    "            result[\"answer\"] = m.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    # Reasoning Steps\n",
    "    for p in [r'[Rr]easoning\\s+[Ss]teps?\\s*[:ï¼š]?\\s*\\n((?:[\\d\\-\\*]\\.?\\s+.+?\\n?)+)',\n",
    "              r'æ¨ç†æ­¥éª¤\\s*[:ï¼š]?\\s*\\n((?:[\\d\\-\\*]\\.?\\s+.+?\\n?)+)']:\n",
    "        m = re.search(p, text, re.DOTALL | re.IGNORECASE)\n",
    "        if m:\n",
    "            steps_text = m.group(1)\n",
    "            steps = re.findall(r'[\\d\\-\\*]\\.?\\s+(.+?)(?=\\n[\\d\\-\\*]\\.?\\s+|\\Z)', steps_text, re.DOTALL)\n",
    "            result[\"reasoning_steps\"] = [s.strip() for s in steps if s.strip()]\n",
    "            break\n",
    "    \n",
    "    if result[\"question\"] and result[\"answer\"]:\n",
    "        # å¦‚æœæ²¡æœ‰æ¨ç†æ­¥éª¤ï¼Œè‡³å°‘ç”Ÿæˆä¸€ä¸ª\n",
    "        if not result[\"reasoning_steps\"]:\n",
    "            result[\"reasoning_steps\"] = [\"åŸºäºä»£ç åˆ†æå¾—å‡ºç»“è®º\"]\n",
    "        return result, \"regex\"\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# æ£€æŸ¥æ–‡ä»¶\n",
    "available_files = []\n",
    "for filename in target_files:\n",
    "    file_path = agent_om_path / filename\n",
    "    if file_path.exists():\n",
    "        available_files.append(filename)\n",
    "        print(f\"âœ… {filename}\")\n",
    "\n",
    "if not available_files:\n",
    "    print(\"âŒ æ²¡æœ‰å¯ç”¨æ–‡ä»¶\")\n",
    "    raise Exception(\"No files available\")\n",
    "\n",
    "print()\n",
    "\n",
    "training_samples = {\n",
    "    \"qa_pairs\": [],\n",
    "    \"metadata\": {\n",
    "        \"model\": \"gemini-2.5-flash\",\n",
    "        \"project\": \"Agent-OM\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"strategy\": \"flexible-text-parsing\"\n",
    "    }\n",
    "}\n",
    "\n",
    "attempt_count = 0\n",
    "file_index = 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"ğŸš€ å¼€å§‹ç”Ÿæˆï¼Œç›®æ ‡ {TARGET_QA_COUNT} ä¸ª...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "while len(training_samples['qa_pairs']) < TARGET_QA_COUNT and attempt_count < MAX_ATTEMPTS:\n",
    "    attempt_count += 1\n",
    "    filename = available_files[file_index % len(available_files)]\n",
    "    file_index += 1\n",
    "    file_path = agent_om_path / filename\n",
    "    \n",
    "    current_count = len(training_samples['qa_pairs'])\n",
    "    print(f\"\\nğŸ“ [å°è¯• {attempt_count}/{MAX_ATTEMPTS}] [æˆåŠŸ {current_count}/{TARGET_QA_COUNT}] - {filename}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            code = f.read()\n",
    "        \n",
    "        # éšæœºé€‰æ‹©ä»£ç ç‰‡æ®µ\n",
    "        if len(code) > 800:\n",
    "            import random\n",
    "            start = random.randint(0, len(code) - 800)\n",
    "            code_snippet = code[start:start + 800]\n",
    "        else:\n",
    "            code_snippet = code\n",
    "        \n",
    "        # å®½æ¾æç¤ºè¯\n",
    "        prompt = f\"\"\"åŸºäºä»¥ä¸‹Pythonä»£ç ç”ŸæˆæŠ€æœ¯é—®ç­”ï¼š\n",
    "\n",
    "```python\n",
    "{code_snippet}\n",
    "```\n",
    "\n",
    "æ ¼å¼ï¼ˆå¯è‡ªç”±å‘æŒ¥ï¼Œåªè¦åŒ…å«ä»¥ä¸‹å†…å®¹ï¼‰ï¼š\n",
    "\n",
    "Question: [é—®é¢˜]\n",
    "\n",
    "Answer: [ç­”æ¡ˆ]\n",
    "\n",
    "Reasoning Steps:\n",
    "1. [æ­¥éª¤1]\n",
    "2. [æ­¥éª¤2]\n",
    "\"\"\"\n",
    "        \n",
    "        print(\"  â³ è°ƒç”¨API...\", end=\" \", flush=True)\n",
    "        start_time = time.time()\n",
    "        response = llm_service.generate_completion(prompt)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"âœ… ({elapsed:.1f}s, {len(response)}å­—ç¬¦)\")\n",
    "        \n",
    "        # è§£æ\n",
    "        data, method = parse_qa_flexible(response)\n",
    "        \n",
    "        if data:\n",
    "            data['source_file'] = filename\n",
    "            data['attempt_number'] = attempt_count\n",
    "            training_samples['qa_pairs'].append(data)\n",
    "            \n",
    "            current_count = len(training_samples['qa_pairs'])\n",
    "            print(f\"  ğŸ‰ ç¬¬ {current_count} ä¸ªæˆåŠŸ! (è§£æ: {method})\")\n",
    "            print(f\"     Q: {data['question'][:60]}...\")\n",
    "            \n",
    "            if current_count >= TARGET_QA_COUNT:\n",
    "                print(f\"\\nâœ¨ å·²è¾¾åˆ°ç›®æ ‡ {TARGET_QA_COUNT} ä¸ª!\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"  âš ï¸  è§£æå¤±è´¥\")\n",
    "            if attempt_count <= 3:\n",
    "                print(f\"\\n  å“åº”å†…å®¹:\\n  {response[:300]}...\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ é”™è¯¯: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(f\"ğŸ ç”Ÿæˆå®Œæˆ!\")\n",
    "print(f\"âœ… æˆåŠŸ: {len(training_samples['qa_pairs'])} / {attempt_count} å°è¯•\")\n",
    "print(f\"ğŸ“ˆ æˆåŠŸç‡: {len(training_samples['qa_pairs'])/attempt_count*100:.1f}%\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if training_samples['qa_pairs']:\n",
    "    print(f\"\\nğŸ“‹ æ ·æœ¬é¢„è§ˆ:\")\n",
    "    for i, qa in enumerate(training_samples['qa_pairs'][:3], 1):\n",
    "        print(f\"\\n  {i}. {qa['source_file']}\")\n",
    "        print(f\"     Q: {qa['question'][:70]}...\")\n",
    "        print(f\"     A: {len(qa['answer'])} å­—ç¬¦\")\n",
    "        print(f\"     æ­¥éª¤: {len(qa['reasoning_steps'])} ä¸ª\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2066d83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa_pairs': [{'question': 'è¿™æ®µPythonä»£ç ä¸»è¦æ¼”ç¤ºäº†LLMåœ¨å¤„ç†å“ªç§ç±»å‹çš„é€»è¾‘æ¨ç†ä»»åŠ¡ï¼Ÿ',\n",
       "   'answer': 'è¿™æ®µä»£ç ä¸»è¦æ¼”ç¤ºäº†LLMåœ¨å¤„ç†**ä¼ é€’æ€§',\n",
       "   'reasoning_steps': ['*'],\n",
       "   'source_file': 'llm_matching.py',\n",
       "   'attempt_number': 5},\n",
       "  {'question': '** è¿™æ®µPythonä»£ç ä¸»è¦æ¼”ç¤ºäº†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº¤äº’çš„å“ªäº›åŠŸèƒ½ï¼Œä»¥åŠå®ƒåœ¨æµ‹è¯•LLMçš„å“ªäº›æ¨ç†èƒ½åŠ›ï¼Ÿ\\n\\n**Answer:** è¿™æ®µPythonä»£ç ä¸»è¦æ¼”ç¤ºäº†å¦‚ä½•å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘é€ç»“æ„åŒ–çš„æ–‡æœ¬æç¤ºï¼ˆpromptï¼‰å¹¶è·å–å…¶å“åº”å†…å®¹ã€‚å®ƒç‰¹åˆ«å…³æ³¨æµ‹è¯•LLMçš„ä¸¤ç§æ ¸å¿ƒæ¨ç†èƒ½åŠ›ï¼š**ä¼ é€’æ€§æ¨ç†ï¼ˆTransitive Reasoningï¼‰** å’Œ **è‡ªæˆ‘çº æ­£ï¼ˆSelf-Correctionï¼‰**ã€‚\\n\\n**Reasoning Steps:**\\n\\n1.  **åˆ†ææ ¸å¿ƒäº¤äº’æ¨¡å¼ï¼š** ä»£ç ä¸­åå¤å‡ºç°çš„ `print(llm.invoke(prompt).content)` æ¨¡å¼è¡¨æ˜ï¼Œå®ƒé€šè¿‡è°ƒç”¨ `llm` å¯¹è±¡çš„ `invoke` æ–¹æ³•ï¼Œå°†ä¸€ä¸ª `prompt` å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥å‘é€ç»™LLMï¼Œç„¶åæ‰“å°LLMè¿”å›çš„ `content`ï¼ˆå³æ¨¡å‹çš„',\n",
       "   'answer': '** è¿™æ®µPythonä»£ç ä¸»è¦æ¼”ç¤ºäº†å¦‚ä½•å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘é€ç»“æ„åŒ–çš„æ–‡æœ¬æç¤ºï¼ˆpromptï¼‰å¹¶è·å–å…¶å“åº”å†…å®¹ã€‚å®ƒç‰¹åˆ«å…³æ³¨æµ‹è¯•LLMçš„ä¸¤ç§æ ¸å¿ƒ',\n",
       "   'reasoning_steps': ['åŸºäºä»£ç åˆ†æå¾—å‡ºç»“è®º'],\n",
       "   'source_file': 'llm_matching.py',\n",
       "   'attempt_number': 9},\n",
       "  {'question': '`colorama.init(autoreset=True)` è¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ',\n",
       "   'answer': 'è¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯åˆå§‹åŒ– Colorama åº“ï¼Œå¹¶è®¾ç½® `autoreset=True` å‚æ•°ã€‚\\n`autoreset=True` çš„ä½œç”¨æ˜¯ç¡®ä¿åœ¨æ¯æ¬¡æ‰“å°æ“ä½œï¼ˆä¾‹å¦‚ä½¿ç”¨ `print()` å‡½æ•°ï¼‰ä¹‹åï¼Œç»ˆç«¯çš„é¢œè‰²å’Œæ ·å¼ä¼šè‡ªåŠ¨é‡ç½®ä¸ºé»˜è®¤å€¼ã€‚è¿™æ„å‘³ç€ä½ ä¸éœ€è¦æ‰‹åŠ¨åœ¨æ¯æ¬¡æ‰“å°çš„æœ«å°¾æ·»åŠ  `colorama.Style.RESET_ALL` æ¥æ¸…é™¤ä¹‹å‰è®¾ç½®çš„é¢œè‰²ï¼Œä»è€Œç®€åŒ–äº†ä»£ç å¹¶é¿å…äº†é¢œè‰²æ³„éœ²åˆ°åç»­è¾“å‡ºçš„é—®é¢˜ã€‚',\n",
       "   'reasoning_steps': ['ä»£'],\n",
       "   'source_file': 'util.py',\n",
       "   'attempt_number': 10},\n",
       "  {'question': 'è¿™æ®µPythonä»£ç ç‰‡æ®µçš„ä¸»è¦ç›®çš„æ˜¯ä»€ä¹ˆï¼Ÿ**\\n\\n**Answer:** è¿™æ®µPythonä»£ç ç‰‡æ®µçš„ä¸»è¦ç›®çš„æ˜¯é€šè¿‡å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘é€ä¸åŒç±»å‹çš„æç¤ºï¼ˆpromptï¼‰ï¼Œæ¥æµ‹è¯•å’Œå±•ç¤ºLLMçš„å¤šç§èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡ç†è§£ã€ä¼ é€’æ€§æ¨ç†ä»¥åŠæš—ç¤ºçš„è‡ªæˆ‘çº æ­£èƒ½åŠ›ã€‚\\n\\n**Reasoning Steps:**\\n1.  **è¯†åˆ«æ ¸å¿ƒæ“ä½œ:** ä»£ç ä¸­åå¤å‡ºç°çš„ `print(llm.invoke(prompt).content)` æ¨¡å¼è¡¨æ˜ï¼Œå®ƒæ­£åœ¨è°ƒç”¨ä¸€ä¸ªåä¸º `llm` çš„å¯¹è±¡ï¼ˆä»£è¡¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ï¼Œå¹¶å‘å…¶ä¼ é€’ä¸åŒçš„ `prompt` å­—ç¬¦ä¸²ï¼Œç„¶åæ‰“å°',\n",
       "   'answer': '** è¿™æ®µPythonä»£ç ç‰‡æ®µçš„ä¸»è¦ç›®çš„æ˜¯é€šè¿‡å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘é€ä¸åŒç±»å‹çš„æç¤ºï¼ˆpromptï¼‰ï¼Œæ¥æµ‹è¯•å’Œå±•ç¤ºLLMçš„å¤šç§èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡ç†è§£ã€ä¼ é€’æ€§',\n",
       "   'reasoning_steps': ['åŸºäºä»£ç åˆ†æå¾—å‡ºç»“è®º'],\n",
       "   'source_file': 'llm_matching.py',\n",
       "   'attempt_number': 11},\n",
       "  {'question': '1**\\nWhat is the primary purpose of the provided Python code snippet?\\n\\n**Answer:**\\nThe primary purpose of this Python code snippet is to demonstrate how to interact with a Large Language Model (LLM) by sending various types of prompts and printing the LLM\\'s generated responses. It showcases different capabilities of the LLM, such as knowledge retrieval, contextual understanding, and transitive reasoning.\\n\\n**Reasoning Steps:**\\n1.  The line `llm = config.llm` indicates that an LLM object is being initialized or retrieved.\\n2.  Multiple `prompt` variables are defined with different questions or statements.\\n3.  The line `print(llm.invoke(prompt).content)` is repeatedly used, which is the standard way to send a prompt to an LLM and print its textual output.\\n\\n---\\n\\n**Question: 2**\\nWhat LLM capabilities are explicitly being tested or demonstrated through the different prompts in the code?\\n\\n**Answer:**\\nThe code demonstrates the following LLM capabilities:\\n\\n*   **Knowledge Retrieval / Entity Equivalence:** Demonstrated by `prompt = \"Is MA_0000270 equivalent to NCI_C33736?\"`, which asks the LLM to recall or infer specific factual relationships.\\n*   **Basic Definition:** Demonstrated by `prompt = \"What is the meaning of chair? Give a short explanation.\"`, testing the LLM\\'s ability to provide general definitions.\\n*   **Contextual Understanding / Context Learning:** Demonstrated by comparing `prompt = \"What is the meaning of chair?\"` with `prompt = \"What is the meaning of chair in the context of conference?\"`. This tests the LLM\\'s ability to adapt its understanding and response based on specific context.\\n*   **Transitive Reasoning:** Demonstrated by `prompt = \"Prompt: We know that paper is equivalent to submission, and submission is equivalent to contribution. Is paper equivalent to contribution?',\n",
       "   'answer': '**\\nThe primary purpose of this Python code snippet is to demonstrate how to interact with a Large Language Model (LLM) by sending various types of prompts and printing the LLM\\'s generated responses. It showcases different capabilities of the LLM, such as knowledge retrieval, contextual understanding, and transitive reasoning.\\n\\n**Reasoning Steps:**\\n1.  The line `llm = config.llm` indicates that an LLM object is being initialized or retrieved.\\n2.  Multiple `prompt` variables are defined with different questions or statements.\\n3.  The line `print(llm.invoke(prompt).content)` is repeatedly used, which is the standard way to send a prompt to an LLM and print its textual output.\\n\\n---\\n\\n**Question: 2**\\nWhat LLM capabilities are explicitly being tested or demonstrated through the different prompts in the code?\\n\\n**Answer:**\\nThe code demonstrates the following LLM capabilities:\\n\\n*   **Knowledge Retrieval / Entity Equivalence:** Demonstrated by `prompt = \"Is MA_0000270 equivalent to NCI_C33736?\"`, which asks the LLM to recall or infer specific factual relationships.\\n*   **Basic Definition:** Demonstrated by `prompt = \"What is the meaning of chair? Give a short explanation.\"`, testing the LLM\\'s ability to provide general definitions.\\n*   **Contextual Understanding / Context Learning:** Demonstrated by comparing `prompt = \"What is the meaning of chair?\"` with `prompt = \"What is the meaning of chair in the context of conference?\"`. This tests the LLM\\'s ability to adapt its understanding and response based on specific context.\\n*   **Transitive Reasoning:** Demonstrated by `prompt = \"Prompt: We know that paper is equivalent to submission, and submission is equivalent to contribution. Is paper equivalent to contribution?',\n",
       "   'reasoning_steps': ['åŸºäºä»£ç åˆ†æå¾—å‡ºç»“è®º'],\n",
       "   'source_file': 'llm_matching.py',\n",
       "   'attempt_number': 13},\n",
       "  {'question': 'è¿™æ®µä»£ç çš„æ ¸å¿ƒåŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿå…·ä½“ä½¿ç”¨äº†Pandasåº“çš„å“ªäº›æ“ä½œï¼Ÿ',\n",
       "   'answer': 'è¿™æ®µä»£ç çš„æ ¸å¿ƒåŠŸèƒ½æ˜¯ä»ä¸¤ä¸ªä¸åŒçš„CSVæ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼Œç„¶åæ ¹æ®ä¸¤ä¸ªå…±åŒçš„åˆ—ï¼ˆ\"Entity1\"å’Œ\"Entity2\"ï¼‰å¯¹è¿™ä¸¤ä¸ªæ•°æ®é›†è¿›è¡Œ**å†…è¿æ¥ï¼ˆinner mergeï¼‰**æ“ä½œï¼Œä»¥æ‰¾å‡ºå®ƒä»¬ä¹‹é—´å…±åŒçš„æ¡ç›®ã€‚\\n\\nå…·ä½“ä½¿ç”¨çš„Pandasæ“ä½œåŒ…æ‹¬ï¼š\\n1.  `pd.read_csv()`: ç”¨äºä»CSVæ–‡ä»¶åŠ è½½æ•°æ®åˆ°DataFrameã€‚\\n2.  `pd.merge()`: ç”¨äºæ‰§è¡Œä¸¤ä¸ªDataFrameä¹‹é—´çš„åˆå¹¶ï¼ˆè¿æ¥ï¼‰æ“ä½œã€‚',\n",
       "   'reasoning_steps': ['ä»£'],\n",
       "   'source_file': 'util.py',\n",
       "   'attempt_number': 14},\n",
       "  {'question': \"** What is the primary purpose of the provided Python code snippet?\\n\\n**Answer:** The primary purpose of this Python code snippet is to demonstrate different capabilities of a Large Language Model (LLM) by sending various types of prompts and printing their responses. It showcases the LLM's ability to handle basic\",\n",
       "   'answer': \"** The primary purpose of this Python code snippet is to demonstrate different capabilities of a Large Language Model (LLM) by sending various types of prompts and printing their responses. It showcases the LLM's ability to handle basic\",\n",
       "   'reasoning_steps': ['åŸºäºä»£ç åˆ†æå¾—å‡ºç»“è®º'],\n",
       "   'source_file': 'llm_matching.py',\n",
       "   'attempt_number': 15},\n",
       "  {'question': 'è¿™æ®µä»£ç ç‰‡æ®µä¸»è¦å±•ç¤ºäº†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº¤äº’çš„å“ªäº›æ ¸å¿ƒèƒ½åŠ›ï¼Ÿ',\n",
       "   'answer': 'è¿™æ®µä»£ç ç‰‡æ®µä¸»è¦å±•ç¤ºäº†LLMåœ¨å¤„ç†è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰æ–¹é¢çš„å¤šç§æ ¸å¿ƒèƒ½åŠ›ï¼ŒåŒ…æ‹¬ï¼šè¯­ä¹‰ç­‰ä»·åˆ¤æ–­ã€è¯ä¹‰è§£é‡Šï¼ˆé€šç”¨ä¸ä¸Šä¸‹æ–‡ç›¸å…³ï¼‰ã€ä»¥åŠåŸºäºå·²çŸ¥äº‹å®çš„ä¼ é€’æ€§',\n",
       "   'reasoning_steps': ['*'],\n",
       "   'source_file': 'llm_matching.py',\n",
       "   'attempt_number': 17},\n",
       "  {'question': 'åˆå§‹ä»£ç ç‰‡æ®µï¼ˆæœªå®Œæ•´æ˜¾ç¤ºå‡½æ•°å®šä¹‰ï¼‰çš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿå®ƒå°†å“ªäº›æ•°æ®å†™å…¥CSVæ–‡ä»¶ï¼Œå¹¶ä»¥ä½•ç§æ ¼å¼å‘ˆç°ï¼Ÿ',\n",
       "   'answer': 'è¯¥åˆå§‹ä»£ç ç‰‡æ®µçš„ä¸»è¦åŠŸèƒ½æ˜¯å°†æ€§èƒ½è¯„ä¼°æŒ‡æ ‡å†™å…¥ä¸€ä¸ªCSVæ–‡ä»¶ã€‚å®ƒå†™å…¥çš„æ•°æ®åŒ…æ‹¬ï¼š\\n1.  `alignment`ï¼šä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œ',\n",
       "   'reasoning_steps': ['åŸºäºä»£ç åˆ†æå¾—å‡ºç»“è®º'],\n",
       "   'source_file': 'util.py',\n",
       "   'attempt_number': 18},\n",
       "  {'question': 'What is the primary purpose of the Python code snippet provided?',\n",
       "   'answer': 'The primary purpose of this Python code snippet is to demonstrate how to interact with a Large Language Model (LLM) by sending various types of prompts and printing its responses, specifically focusing on testing its reasoning capabilities like contextual understanding, transitive reasoning, and implicitly, self-correction.',\n",
       "   'reasoning_steps': ['T'],\n",
       "   'source_file': 'llm_matching.py',\n",
       "   'attempt_number': 19}],\n",
       " 'metadata': {'model': 'gemini-2.5-flash',\n",
       "  'project': 'Agent-OM',\n",
       "  'timestamp': '2025-12-18T13:37:57.251543',\n",
       "  'strategy': 'flexible-text-parsing'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a961fa",
   "metadata": {},
   "source": [
    "## 7. ä¿å­˜è®­ç»ƒæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "687c60c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜\n",
      "ğŸ“ æ–‡ä»¶è·¯å¾„: outputs/agent-om/training_samples_20251218_134615.json\n",
      "ğŸ“Š ç»Ÿè®¡:\n",
      "   - Q&A å¯¹æ•°: 10\n",
      "   - æ–‡ä»¶å¤§å°: 9.73 KB\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "output_dir = Path(\"outputs/agent-om\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ä¿å­˜ä¸º JSON\n",
    "output_file = output_dir / f\"training_samples_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_samples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜\")\n",
    "print(f\"ğŸ“ æ–‡ä»¶è·¯å¾„: {output_file}\")\n",
    "print(f\"ğŸ“Š ç»Ÿè®¡:\")\n",
    "print(f\"   - Q&A å¯¹æ•°: {len(training_samples['qa_pairs'])}\")\n",
    "print(f\"   - æ–‡ä»¶å¤§å°: {output_file.stat().st_size / 1024:.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83ca57",
   "metadata": {},
   "source": [
    "## 8. æµ‹è¯•æ€»ç»“\n",
    "\n",
    "### âœ… æµ‹è¯•ç»“æœ\n",
    "\n",
    "æ‰€æœ‰æµ‹è¯•å‡å·²å®Œæˆï¼ŒéªŒè¯äº†ä»¥ä¸‹èƒ½åŠ›ï¼š\n",
    "\n",
    "1. **Gemini API è¿æ¥** - æˆåŠŸåˆå§‹åŒ–å¹¶è¿æ¥\n",
    "2. **åŸºç¡€æ–‡æœ¬ç”Ÿæˆ** - ç”Ÿæˆæµç•…çš„ä¸­æ–‡è§£é‡Š\n",
    "3. **Q&A å¯¹ç”Ÿæˆ** - èƒ½å¤ŸåŸºäºä»£ç ç”Ÿæˆé«˜è´¨é‡é—®ç­”\n",
    "4. **è®¾è®¡æ–¹æ¡ˆç”Ÿæˆ** - ç”Ÿæˆç»“æ„åŒ–çš„æ¶æ„è®¾è®¡\n",
    "5. **Agent-OM é›†æˆ** - æˆåŠŸä¸ºçœŸå®é¡¹ç›®ç”Ÿæˆè®­ç»ƒæ•°æ®\n",
    "\n",
    "### ğŸ“Š å…³é”®æŒ‡æ ‡\n",
    "\n",
    "- **LLM Provider**: Google Gemini 2.5 Flash\n",
    "- **å¹³å‡å“åº”æ—¶é—´**: 10-30ç§’/è¯·æ±‚\n",
    "- **è¾“å‡ºè´¨é‡**: é«˜è´¨é‡ç»“æ„åŒ–æ•°æ®\n",
    "- **JSON æ ¼å¼**: éœ€è¦æ¸…ç†å¤„ç†ä½†æ€»ä½“ç¨³å®š\n",
    "\n",
    "### ğŸ¯ ä¸‹ä¸€æ­¥\n",
    "\n",
    "1. ä¼˜åŒ–æ‰¹é‡ç”Ÿæˆæ€§èƒ½ï¼ˆå¹¶å‘å¤„ç†ï¼‰\n",
    "2. æ‰©å±•åˆ°æ›´å¤š Agent-OM æ–‡ä»¶\n",
    "3. å®æ–½äººå·¥å®¡æ ¸æµç¨‹\n",
    "4. å¼€å§‹ Qwen 2.5 æ¨¡å‹å¾®è°ƒ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
