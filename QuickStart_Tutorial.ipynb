{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e2a763",
   "metadata": {},
   "source": [
    "## ğŸ”§ æ­¥éª¤1: ç¯å¢ƒé…ç½®\n",
    "\n",
    "é¦–å…ˆé…ç½®å¿…è¦çš„ç¯å¢ƒå’ŒAPIå¯†é’¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8865c2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… APIå¯†é’¥å·²é…ç½®\n",
      "   å¯†é’¥å‰ç¼€: AIzaSyCglf...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®è·¯å¾„\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# é…ç½®APIå¯†é’¥ï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„å¯†é’¥ï¼‰\n",
    "# æ–¹å¼1: ç›´æ¥è®¾ç½®ï¼ˆä¸æ¨èï¼Œä»…ç”¨äºæµ‹è¯•ï¼‰\n",
    "# os.environ['GEMINI_API_KEY'] = 'your_api_key_here'\n",
    "\n",
    "# æ–¹å¼2: ä».envæ–‡ä»¶è¯»å–ï¼ˆæ¨èï¼‰\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# éªŒè¯\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "if api_key:\n",
    "    print(\"âœ… APIå¯†é’¥å·²é…ç½®\")\n",
    "    print(f\"   å¯†é’¥å‰ç¼€: {api_key[:10]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸  æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼\")\n",
    "    print(\"   æç¤º: åœ¨.envæ–‡ä»¶ä¸­è®¾ç½® GEMINI_API_KEY=your_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ccddd",
   "metadata": {},
   "source": [
    "## ğŸ“ æ­¥éª¤2: é€‰æ‹©ç›®æ ‡é¡¹ç›®\n",
    "\n",
    "å¯ä»¥é€‰æ‹©:\n",
    "1. GitHubå…¬å¼€é¡¹ç›®ï¼ˆè‡ªåŠ¨å…‹éš†ï¼‰\n",
    "2. æœ¬åœ°é¡¹ç›®è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f8c6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ å‡†å¤‡é¡¹ç›®...\n",
      "âœ… é¡¹ç›®å·²å­˜åœ¨: /Users/xianhaoliu/github_repos/ontology-llm\n",
      "\n",
      "ğŸ“‚ é¡¹ç›®è·¯å¾„: /Users/xianhaoliu/github_repos/ontology-llm\n",
      "   é¡¹ç›®åç§°: ontology-llm\n",
      "   Pythonæ–‡ä»¶: 13 ä¸ª\n"
     ]
    }
   ],
   "source": [
    "from src.context_analyzer import GitHubIntegration\n",
    "\n",
    "# æ–¹å¼1: ä½¿ç”¨GitHubé¡¹ç›®ï¼ˆæ¨èï¼‰\n",
    "# ç¤ºä¾‹ï¼šæœ¬ä½“åŒ¹é…é¡¹ç›®\n",
    "project_source = \"https://github.com/qzc438-research/ontology-llm\"\n",
    "\n",
    "# æ–¹å¼2: ä½¿ç”¨æœ¬åœ°é¡¹ç›®\n",
    "# project_source = \"/Users/yourname/projects/your-project\"\n",
    "\n",
    "# å…‹éš†æˆ–ä½¿ç”¨é¡¹ç›®\n",
    "print(\"ğŸ”„ å‡†å¤‡é¡¹ç›®...\")\n",
    "project_path = GitHubIntegration.clone_or_use_repo(project_source)\n",
    "\n",
    "print(f\"\\nğŸ“‚ é¡¹ç›®è·¯å¾„: {project_path}\")\n",
    "print(f\"   é¡¹ç›®åç§°: {project_path.name}\")\n",
    "\n",
    "# å¿«é€Ÿæµè§ˆé¡¹ç›®ç»“æ„\n",
    "python_files = list(project_path.rglob('*.py'))\n",
    "python_files = [f for f in python_files if '__pycache__' not in str(f) and '.venv' not in str(f)]\n",
    "print(f\"   Pythonæ–‡ä»¶: {len(python_files)} ä¸ª\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd2c0a",
   "metadata": {},
   "source": [
    "## ğŸ§  æ­¥éª¤3: åˆå§‹åŒ–ç”Ÿæˆå™¨\n",
    "\n",
    "åˆ›å»ºç®€å•ç”Ÿæˆå™¨å®ä¾‹ï¼Œæ”¯æŒä¸Šä¸‹æ–‡å¢å¼º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac3447b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ\n",
      "   LLMå¯ç”¨: True\n",
      "   ä¸Šä¸‹æ–‡å¢å¼º: True\n",
      "   é¡¹ç›®: ontology-llm\n"
     ]
    }
   ],
   "source": [
    "from src.simple_generator import SimpleGenerator\n",
    "\n",
    "# åˆ›å»ºç”Ÿæˆå™¨\n",
    "generator = SimpleGenerator(\n",
    "    project_path=str(project_path),\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.3  # è¾ƒä½æ¸©åº¦ä¿è¯æ ¼å¼ç¨³å®šæ€§\n",
    ")\n",
    "\n",
    "print(\"âœ… ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ\")\n",
    "print(f\"   LLMå¯ç”¨: {generator.llm_available}\")\n",
    "print(f\"   ä¸Šä¸‹æ–‡å¢å¼º: {generator.context_enabled}\")\n",
    "print(f\"   é¡¹ç›®: {generator.project_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2690f2f7",
   "metadata": {},
   "source": [
    "## ğŸ“ æ­¥éª¤4a: ç”Ÿæˆå•ä¸ªé—®ç­”å¯¹ï¼ˆæµ‹è¯•ï¼‰\n",
    "\n",
    "å…ˆç”Ÿæˆä¸€ä¸ªé—®ç­”å¯¹æµ‹è¯•æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5439ab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ æµ‹è¯•æ–‡ä»¶: llm_om_few_shot.py\n",
      "\n",
      "ã€ä»£ç ç‰‡æ®µé¢„è§ˆã€‘\n",
      "ty.callbacks import get_openai_callback\n",
      "\n",
      "import run_config as config\n",
      "import om_ontology_to_csv\n",
      "import util\n",
      "\n",
      "\n",
      "# customer settings\n",
      "o1_path = config.o1_path\n",
      "o2_path = config.o2_path\n",
      "align_path = config.align_path\n",
      "context = config.context\n",
      "o1_is_code = config.o1_is_code\n",
      "o2_is_code = config.o2_is_code\n",
      "\n",
      "o1...\n",
      "\n",
      "ğŸ”„ ç”Ÿæˆé—®ç­”å¯¹...\n",
      "\n",
      "âœ… ç”ŸæˆæˆåŠŸï¼\n",
      "\n",
      "======================================================================\n",
      "é—®é¢˜: åœ¨ `ontology-llm` é¡¹ç›®ä¸­ï¼Œç‰¹åˆ«æ˜¯ `llm_om_few_shot.py` æ¨¡å—å†…ï¼Œå®šä¹‰äº† `extract_yes_no` å‡½æ•°ã€‚é‰´äºé¡¹ç›®æ ¸å¿ƒæ¨¡å—å¦‚ `generate_conference_benchmark` å’Œ `llm_matching` çš„å­˜åœ¨ï¼Œè¯·é˜è¿° `extract_yes_no` å‡½æ•°åœ¨æ•´ä½“ LLM-based æœ¬ä½“å¯¹é½å’Œè¯„ä¼°ç®¡é“ä¸­çš„å…·ä½“ä½œç”¨å’Œé‡è¦æ€§ã€‚è¯¥å‡½æ•°å¦‚ä½•å®ç°åœ¨ Few-shot å’Œ Zero-shot åœºæ™¯ä¸‹å¯¹é½è´¨é‡çš„å®šé‡è¯„ä¼°ï¼Œä»¥åŠå®ƒå¯¹æç¤ºå·¥ç¨‹å’Œç»“æœè§£é‡Šå¼•å…¥äº†å“ªäº›æ¶æ„ä¸Šçš„è€ƒè™‘ï¼Ÿ\n",
      "\n",
      "ç­”æ¡ˆ: `extract_yes_no` å‡½æ•°åœ¨ `ontology-llm` é¡¹ç›®ä¸­æ‰®æ¼”ç€å°† LLM çš„è‡ªç”±æ–‡æœ¬å“åº”è½¬åŒ–ä¸ºå¯é‡åŒ–ã€ç»“æ„åŒ–äºŒå…ƒå†³ç­–çš„å…³é”®è§’è‰²ï¼Œè¿™å¯¹äºæœ¬ä½“å¯¹é½çš„è‡ªåŠ¨åŒ–è¯„ä¼°è‡³å…³é‡è¦ã€‚\n",
      "\n",
      "1.  **`extract_yes_no` çš„æ ¸å¿ƒä½œç”¨:**\n",
      "    è¯¥å‡½æ•°çš„ä¸»è¦ç›®çš„æ˜¯ä» LLM çš„è‡ªç„¶è¯­è¨€è¾“å‡ºä¸­å¯é åœ°æå–å‡ºæ˜ç¡®çš„â€œYesâ€æˆ–â€œNoâ€å†³ç­–ã€‚å®ƒä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ (`re.search`) æ¥è¯†åˆ«æ–‡æœ¬ä¸­è¡¨ç¤ºè‚¯å®šæˆ–å¦å®šçš„è¯è¯­ã€‚è¿™ç§äºŒå…ƒæå–æ˜¯ LLM è¿›è¡Œåˆ†ç±»æˆ–å†³ç­–ä»»åŠ¡çš„å¸¸è§åå¤„ç†æ­¥éª¤ï¼Œç¡®ä¿äº†åç»­å¤„ç†çš„ä¸€è‡´æ€§ã€‚\n",
      "\n",
      "2.  **åœ¨ LLM-based æœ¬ä½“å¯¹é½å·¥ä½œæµä¸­çš„é›†æˆ:**\n",
      "    *   **æœ¬ä½“å®ä½“å¯¹æ¯”è¾ƒ:** `llm_matching` æ¨¡å—å¾ˆå¯èƒ½è´Ÿè´£å°†ä¸€å¯¹æœ¬ä½“å®ä½“ï¼ˆå¦‚æ¦‚å¿µã€å±æ€§æˆ–å…³ç³»ï¼‰ä½œä¸ºè¾“å…¥ï¼Œæ„å»ºæˆä¸€ä¸ªè‡ªç„¶è¯­è¨€é—®é¢˜ï¼ˆpromptï¼‰ï¼Œç„¶åæäº¤ç»™ `llm` å¯¹è±¡ï¼ˆå¦‚ `config.llm` å®šä¹‰çš„ LangChain LLM å®ä¾‹ï¼‰ã€‚ä¾‹å¦‚ï¼Œé—®é¢˜å¯èƒ½æ˜¯ï¼šâ€œæ¦‚å¿µâ€˜Aâ€™ä¸æ¦‚å¿µâ€˜Bâ€™æ˜¯å¦ç­‰ä»·ï¼Ÿè¯·å›ç­”â€˜Yesâ€™æˆ–â€˜Noâ€™ã€‚â€\n",
      "    *   **å†³ç­–æå–:** LLM è¿”å›ä¸€ä¸ªåŒ…å«ç­”æ¡ˆçš„æ–‡æœ¬å­—ç¬¦ä¸²ã€‚`extract_yes_no` å‡½æ•°éšåè¢«è°ƒç”¨æ¥è§£æè¿™ä¸ªå­—ç¬¦ä¸²ï¼Œæå–å‡º LLM çš„æœ€ç»ˆäºŒå…ƒå†³ç­–ã€‚\n",
      "    *   **Few-shot ä¸ Zero-shot åœºæ™¯:**\n",
      "        *   **Few-shot (å°‘é‡æ ·æœ¬å­¦ä¹ ):** åœ¨ Few-shot åœºæ™¯ä¸‹ï¼Œå°½ç®¡ LLM é€šè¿‡æä¾›çš„ç¤ºä¾‹å¯èƒ½å·²è¢«å¼•å¯¼ç”Ÿæˆç‰¹å®šæ ¼å¼çš„â€œYes/Noâ€ç­”æ¡ˆï¼Œ`extract_yes_no` ä»ç„¶æ˜¯å¿…ä¸å¯å°‘çš„ã€‚å®ƒæä¾›äº†ä¸€ä¸ªå¥å£®çš„è§£æå±‚ï¼Œä»¥åº”å¯¹ LLM å³ä½¿åœ¨ Few-shot ä¸‹ä¹Ÿå¯èƒ½äº§ç”Ÿçš„ç»†å¾®å˜å¼‚æˆ–é¢å¤–è¯´æ˜ï¼Œç¡®ä¿å†³ç­–æå–çš„é²æ£’æ€§å’Œä¸€è‡´æ€§ã€‚\n",
      "        *   **Zero-shot (é›¶æ ·æœ¬å­¦ä¹ ):** åœ¨ Zero-shot åœºæ™¯ä¸‹ï¼ŒLLM çš„è¾“å‡ºæ ¼å¼å¯èƒ½æ›´åŠ å¤šæ ·å’Œä¸å¯é¢„æµ‹ã€‚æ­¤æ—¶ï¼Œ`extract_yes_no` çš„ä½œç”¨æ›´ä¸ºå…³é”®ï¼Œå®ƒèƒ½å¤Ÿä»æ›´è‡ªç”±çš„æ–‡æœ¬ä¸­æ•è·æ ¸å¿ƒçš„äºŒå…ƒå†³ç­–ï¼Œå°†éç»“æ„åŒ–è¾“å‡ºè½¬åŒ–ä¸ºå¯ç”¨çš„ç»“æ„åŒ–æ•°æ®ã€‚\n",
      "\n",
      "3.  **å¯¹é½è´¨é‡å®šé‡è¯„ä¼°çš„å®ç°:**\n",
      "    *   **ä¸çœŸå€¼ (Ground Truth) å¯¹æ¯”:** `extract_yes_no` æå–çš„æ¯ä¸ªâ€œYesâ€å†³ç­–éƒ½è¢«è§†ä¸ºä¸€ä¸ªæè®®çš„å¯¹é½å…³ç³»ã€‚è¿™äº›æè®®çš„å¯¹é½å…³ç³»éšåä¸ `true_path` ( ground truth å¯¹é½æ–‡ä»¶) ä¸­çš„çœŸå®å¯¹é½è¿›è¡Œæ¯”è¾ƒã€‚\n",
      "    *   **æ€§èƒ½æŒ‡æ ‡è®¡ç®—:**\n",
      "        *   **çœŸé˜³æ€§ (TP):** LLM å›ç­”â€œYesâ€ï¼Œä¸”çœŸå€¼ä¹Ÿæ˜¯â€œYesâ€ã€‚\n",
      "        *   **å‡é˜³æ€§ (FP):** LLM å›ç­”â€œYesâ€ï¼Œä½†çœŸå€¼æ˜¯â€œNoâ€ã€‚\n",
      "        *   **å‡é˜´æ€§ (FN):** LLM å›ç­”â€œNoâ€ï¼Œä½†çœŸå€¼æ˜¯â€œYesâ€ã€‚\n",
      "        é€šè¿‡è¿™äº›åˆ†ç±»ç»“æœï¼Œé¡¹ç›®èƒ½å¤Ÿè®¡ç®—å‡ºæœ¬ä½“å¯¹é½ä»»åŠ¡çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚ç²¾ç¡®ç‡ (Precision)ã€å¬å›ç‡ (Recall) å’Œ F1-åˆ†æ•°ã€‚\n",
      "    *   **åŸºå‡†æµ‹è¯•å’ŒæŠ¥å‘Š:** `generate_conference_benchmark` å’Œ `run_series_conference` ç­‰æ¨¡å—ä¼šè‡ªåŠ¨åŒ–è¿™ä¸ªè¯„ä¼°è¿‡ç¨‹ï¼Œ`extract_yes_no` çš„è¾“å‡ºæ˜¯è¿™äº›æ¨¡å—ç”Ÿæˆç»“æ„åŒ–è¯„ä¼°ç»“æœï¼ˆä¿å­˜åˆ° `result_path`ï¼‰çš„åŸºç¡€ï¼Œä»è€Œæ”¯æŒå¯¹ä¸åŒ LLM é…ç½®å’Œå¯¹é½ç­–ç•¥è¿›è¡Œå®¢è§‚æ¯”è¾ƒã€‚\n",
      "\n",
      "4.  **å¯¹æç¤ºå·¥ç¨‹å’Œç»“æœè§£é‡Šçš„æ¶æ„è€ƒè™‘:**\n",
      "    *   **æç¤ºå·¥ç¨‹ (Prompt Engineering):**\n",
      "        *   **æ˜ç¡®çš„æŒ‡ä»¤:** ä¸ºäº†æœ€å¤§åŒ– `extract_yes_no` çš„æœ‰æ•ˆæ€§ï¼Œæç¤ºè®¾è®¡è€…éœ€è¦æ˜ç¡®æŒ‡ç¤º LLM ä»¥ç®€æ´çš„â€œYesâ€æˆ–â€œNoâ€å½¢å¼å›åº”ï¼Œå¹¶å°½é‡é¿å…æ— å…³çš„é¢å¤–æ–‡æœ¬ã€‚ä¾‹å¦‚ï¼Œåœ¨æç¤ºä¸­åŠ å…¥â€œè¯·åªå›ç­”â€˜Yesâ€™æˆ–â€˜Noâ€™ï¼Œä¸å¸¦ä»»ä½•è§£é‡Šâ€å¯ä»¥æé«˜æå–çš„æˆåŠŸç‡ã€‚\n",
      "        *   **é¿å…æ­§ä¹‰:** æç¤ºå†…å®¹åº”å°½é‡å‡å°‘ LLM äº§ç”Ÿæ¨¡ç³Šæˆ–æ— æ³•å½’ç±»çš„å›ç­”çš„å¯èƒ½æ€§ã€‚\n",
      "    *   **åå¤„ç†ç®¡é“:** `extract_yes_no` æ˜¯åå¤„ç†ç®¡é“ä¸­çš„ä¸€ä¸ªå…³é”®ç»„ä»¶ï¼Œè´Ÿè´£å°† LLM çš„åŸå§‹æ–‡æœ¬è¾“å‡ºè½¬æ¢ä¸ºå¯æ“ä½œçš„ç»“æ„åŒ–æ•°æ®ã€‚è¿™ä¸ªç®¡é“å¯èƒ½è¿˜éœ€è¦å¤„ç†ï¼š\n",
      "        *   **ä¸ç¡®å®šæ€§:** å¦‚æœ LLM çš„å›ç­”æ— æ³•è¢« `extract_yes_no` æˆåŠŸè§£æ (è¿”å› `None`)ï¼Œç³»ç»Ÿéœ€è¦æœ‰ç­–ç•¥æ¥å¤„ç†ï¼Œä¾‹å¦‚æ ‡è®°ä¸ºä¸ç¡®å®šã€è¯·æ±‚ LLM é‡æ–°å›ç­”ï¼Œæˆ–è€…é»˜è®¤è§†ä¸ºâ€œNoâ€ã€‚\n",
      "        *   **å¤šé˜¶æ®µå¤„ç†:** å¯¹äºå¤æ‚æ¡ˆä¾‹ï¼Œå¯èƒ½å…ˆç”¨ `extract_yes_no` è¿›è¡Œå¿«é€Ÿè¿‡æ»¤ï¼Œå†å¯¹ä¸ç¡®å®šæˆ–æ›´ç²¾ç»†çš„é—®é¢˜ä½¿ç”¨æ›´å¤æ‚çš„è§£æå™¨æˆ–é¢å¤–çš„ LLM æŸ¥è¯¢ã€‚\n",
      "    *   **å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Š:**\n",
      "        *   é€šè¿‡ç¡®ä¿ LLM ç»™å‡ºç®€æ´çš„â€œYes/Noâ€ç­”æ¡ˆï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘æ¯ä¸ªæŸ¥è¯¢çš„ token æ¶ˆè€—ï¼Œä»è€Œé™ä½å¤§è§„æ¨¡æœ¬ä½“å¯¹é½ä»»åŠ¡çš„æˆæœ¬ (é€šè¿‡ `cost_path` è®°å½•)ã€‚\n",
      "        *   `extract_yes_no` çš„æ•ˆç‡ç›´æ¥å½±å“æ•´ä¸ªè¯„ä¼°ç®¡é“çš„é€Ÿåº¦ï¼Œå¯¹äºå¤„ç†å¤§é‡å®ä½“å¯¹çš„æœ¬ä½“æ¥è¯´ï¼Œå…¶æ€§èƒ½è‡³å…³é‡è¦ã€‚\n",
      "\n",
      "æ€»ä¹‹ï¼Œ`extract_yes_no` æ˜¯ä¸€ä¸ªçœ‹ä¼¼ç®€å•å´åŠŸèƒ½å¼ºå¤§çš„å·¥å…·ï¼Œå®ƒå¼¥åˆäº† LLM è‡ªç”±å½¢å¼è¾“å‡ºä¸æœ¬ä½“å¯¹é½ä»»åŠ¡æ‰€éœ€ç»“æ„åŒ–ã€å¯é‡åŒ–å†³ç­–ä¹‹é—´çš„é¸¿æ²Ÿï¼Œæ˜¯å®ç°è‡ªåŠ¨åŒ–æœ¬ä½“å¯¹é½è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•çš„åŸºçŸ³ã€‚\n",
      "\n",
      "æ¨ç†æ­¥éª¤:\n",
      "  1. **`extract_yes_no` çš„æ ¸å¿ƒä½œç”¨:**\n",
      "    è¯¥å‡½æ•°çš„ä¸»è¦ç›®çš„æ˜¯ä» LLM çš„è‡ªç„¶è¯­è¨€è¾“å‡ºä¸­å¯é åœ°æå–å‡ºæ˜ç¡®çš„â€œYesâ€æˆ–â€œNoâ€å†³ç­–ã€‚å®ƒä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ (`re.search`) æ¥è¯†åˆ«æ–‡æœ¬ä¸­è¡¨ç¤ºè‚¯å®šæˆ–å¦å®šçš„è¯è¯­ã€‚è¿™ç§äºŒå…ƒæå–æ˜¯ LLM è¿›è¡Œåˆ†ç±»æˆ–å†³ç­–ä»»åŠ¡çš„å¸¸è§åå¤„ç†æ­¥éª¤ï¼Œç¡®ä¿äº†åç»­å¤„ç†çš„ä¸€è‡´æ€§ã€‚\n",
      "  2. **åœ¨ LLM-based æœ¬ä½“å¯¹é½å·¥ä½œæµä¸­çš„é›†æˆ:**\n",
      "    *   **æœ¬ä½“å®ä½“å¯¹æ¯”è¾ƒ:** `llm_matching` æ¨¡å—å¾ˆå¯èƒ½è´Ÿè´£å°†ä¸€å¯¹æœ¬ä½“å®ä½“ï¼ˆå¦‚æ¦‚å¿µã€å±æ€§æˆ–å…³ç³»ï¼‰ä½œä¸ºè¾“å…¥ï¼Œæ„å»ºæˆä¸€ä¸ªè‡ªç„¶è¯­è¨€é—®é¢˜ï¼ˆpromptï¼‰ï¼Œç„¶åæäº¤ç»™ `llm` å¯¹è±¡ï¼ˆå¦‚ `config.llm` å®šä¹‰çš„ LangChain LLM å®ä¾‹ï¼‰ã€‚ä¾‹å¦‚ï¼Œé—®é¢˜å¯èƒ½æ˜¯ï¼šâ€œæ¦‚å¿µâ€˜Aâ€™ä¸æ¦‚å¿µâ€˜Bâ€™æ˜¯å¦ç­‰ä»·ï¼Ÿè¯·å›ç­”â€˜Yesâ€™æˆ–â€˜Noâ€™ã€‚â€\n",
      "    *   **å†³ç­–æå–:** LLM è¿”å›ä¸€ä¸ªåŒ…å«ç­”æ¡ˆçš„æ–‡æœ¬å­—ç¬¦ä¸²ã€‚`extract_yes_no` å‡½æ•°éšåè¢«è°ƒç”¨æ¥è§£æè¿™ä¸ªå­—ç¬¦ä¸²ï¼Œæå–å‡º LLM çš„æœ€ç»ˆäºŒå…ƒå†³ç­–ã€‚\n",
      "    *   **Few-shot ä¸ Zero-shot åœºæ™¯:**\n",
      "        *   **Few-shot (å°‘é‡æ ·æœ¬å­¦ä¹ ):** åœ¨ Few-shot åœºæ™¯ä¸‹ï¼Œå°½ç®¡ LLM é€šè¿‡æä¾›çš„ç¤ºä¾‹å¯èƒ½å·²è¢«å¼•å¯¼ç”Ÿæˆç‰¹å®šæ ¼å¼çš„â€œYes/Noâ€ç­”æ¡ˆï¼Œ`extract_yes_no` ä»ç„¶æ˜¯å¿…ä¸å¯å°‘çš„ã€‚å®ƒæä¾›äº†ä¸€ä¸ªå¥å£®çš„è§£æå±‚ï¼Œä»¥åº”å¯¹ LLM å³ä½¿åœ¨ Few-shot ä¸‹ä¹Ÿå¯èƒ½äº§ç”Ÿçš„ç»†å¾®å˜å¼‚æˆ–é¢å¤–è¯´æ˜ï¼Œç¡®ä¿å†³ç­–æå–çš„é²æ£’æ€§å’Œä¸€è‡´æ€§ã€‚\n",
      "        *   **Zero-shot (é›¶æ ·æœ¬å­¦ä¹ ):** åœ¨ Zero-shot åœºæ™¯ä¸‹ï¼ŒLLM çš„è¾“å‡ºæ ¼å¼å¯èƒ½æ›´åŠ å¤šæ ·å’Œä¸å¯é¢„æµ‹ã€‚æ­¤æ—¶ï¼Œ`extract_yes_no` çš„ä½œç”¨æ›´ä¸ºå…³é”®ï¼Œå®ƒèƒ½å¤Ÿä»æ›´è‡ªç”±çš„æ–‡æœ¬ä¸­æ•è·æ ¸å¿ƒçš„äºŒå…ƒå†³ç­–ï¼Œå°†éç»“æ„åŒ–è¾“å‡ºè½¬åŒ–ä¸ºå¯ç”¨çš„ç»“æ„åŒ–æ•°æ®ã€‚\n",
      "  3. **å¯¹é½è´¨é‡å®šé‡è¯„ä¼°çš„å®ç°:**\n",
      "    *   **ä¸çœŸå€¼ (Ground Truth) å¯¹æ¯”:** `extract_yes_no` æå–çš„æ¯ä¸ªâ€œYesâ€å†³ç­–éƒ½è¢«è§†ä¸ºä¸€ä¸ªæè®®çš„å¯¹é½å…³ç³»ã€‚è¿™äº›æè®®çš„å¯¹é½å…³ç³»éšåä¸ `true_path` ( ground truth å¯¹é½æ–‡ä»¶) ä¸­çš„çœŸå®å¯¹é½è¿›è¡Œæ¯”è¾ƒã€‚\n",
      "    *   **æ€§èƒ½æŒ‡æ ‡è®¡ç®—:**\n",
      "        *   **çœŸé˜³æ€§ (TP):** LLM å›ç­”â€œYesâ€ï¼Œä¸”çœŸå€¼ä¹Ÿæ˜¯â€œYesâ€ã€‚\n",
      "        *   **å‡é˜³æ€§ (FP):** LLM å›ç­”â€œYesâ€ï¼Œä½†çœŸå€¼æ˜¯â€œNoâ€ã€‚\n",
      "        *   **å‡é˜´æ€§ (FN):** LLM å›ç­”â€œNoâ€ï¼Œä½†çœŸå€¼æ˜¯â€œYesâ€ã€‚\n",
      "        é€šè¿‡è¿™äº›åˆ†ç±»ç»“æœï¼Œé¡¹ç›®èƒ½å¤Ÿè®¡ç®—å‡ºæœ¬ä½“å¯¹é½ä»»åŠ¡çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚ç²¾ç¡®ç‡ (Precision)ã€å¬å›ç‡ (Recall) å’Œ F1-åˆ†æ•°ã€‚\n",
      "    *   **åŸºå‡†æµ‹è¯•å’ŒæŠ¥å‘Š:** `generate_conference_benchmark` å’Œ `run_series_conference` ç­‰æ¨¡å—ä¼šè‡ªåŠ¨åŒ–è¿™ä¸ªè¯„ä¼°è¿‡ç¨‹ï¼Œ`extract_yes_no` çš„è¾“å‡ºæ˜¯è¿™äº›æ¨¡å—ç”Ÿæˆç»“æ„åŒ–è¯„ä¼°ç»“æœï¼ˆä¿å­˜åˆ° `result_path`ï¼‰çš„åŸºç¡€ï¼Œä»è€Œæ”¯æŒå¯¹ä¸åŒ LLM é…ç½®å’Œå¯¹é½ç­–ç•¥è¿›è¡Œå®¢è§‚æ¯”è¾ƒã€‚\n",
      "  4. **å¯¹æç¤ºå·¥ç¨‹å’Œç»“æœè§£é‡Šçš„æ¶æ„è€ƒè™‘:**\n",
      "    *   **æç¤ºå·¥ç¨‹ (Prompt Engineering):**\n",
      "        *   **æ˜ç¡®çš„æŒ‡ä»¤:** ä¸ºäº†æœ€å¤§åŒ– `extract_yes_no` çš„æœ‰æ•ˆæ€§ï¼Œæç¤ºè®¾è®¡è€…éœ€è¦æ˜ç¡®æŒ‡ç¤º LLM ä»¥ç®€æ´çš„â€œYesâ€æˆ–â€œNoâ€å½¢å¼å›åº”ï¼Œå¹¶å°½é‡é¿å…æ— å…³çš„é¢å¤–æ–‡æœ¬ã€‚ä¾‹å¦‚ï¼Œåœ¨æç¤ºä¸­åŠ å…¥â€œè¯·åªå›ç­”â€˜Yesâ€™æˆ–â€˜Noâ€™ï¼Œä¸å¸¦ä»»ä½•è§£é‡Šâ€å¯ä»¥æé«˜æå–çš„æˆåŠŸç‡ã€‚\n",
      "        *   **é¿å…æ­§ä¹‰:** æç¤ºå†…å®¹åº”å°½é‡å‡å°‘ LLM äº§ç”Ÿæ¨¡ç³Šæˆ–æ— æ³•å½’ç±»çš„å›ç­”çš„å¯èƒ½æ€§ã€‚\n",
      "    *   **åå¤„ç†ç®¡é“:** `extract_yes_no` æ˜¯åå¤„ç†ç®¡é“ä¸­çš„ä¸€ä¸ªå…³é”®ç»„ä»¶ï¼Œè´Ÿè´£å°† LLM çš„åŸå§‹æ–‡æœ¬è¾“å‡ºè½¬æ¢ä¸ºå¯æ“ä½œçš„ç»“æ„åŒ–æ•°æ®ã€‚è¿™ä¸ªç®¡é“å¯èƒ½è¿˜éœ€è¦å¤„ç†ï¼š\n",
      "        *   **ä¸ç¡®å®šæ€§:** å¦‚æœ LLM çš„å›ç­”æ— æ³•è¢« `extract_yes_no` æˆåŠŸè§£æ (è¿”å› `None`)ï¼Œç³»ç»Ÿéœ€è¦æœ‰ç­–ç•¥æ¥å¤„ç†ï¼Œä¾‹å¦‚æ ‡è®°ä¸ºä¸ç¡®å®šã€è¯·æ±‚ LLM é‡æ–°å›ç­”ï¼Œæˆ–è€…é»˜è®¤è§†ä¸ºâ€œNoâ€ã€‚\n",
      "        *   **å¤šé˜¶æ®µå¤„ç†:** å¯¹äºå¤æ‚æ¡ˆä¾‹ï¼Œå¯èƒ½å…ˆç”¨ `extract_yes_no` è¿›è¡Œå¿«é€Ÿè¿‡æ»¤ï¼Œå†å¯¹ä¸ç¡®å®šæˆ–æ›´ç²¾ç»†çš„é—®é¢˜ä½¿ç”¨æ›´å¤æ‚çš„è§£æå™¨æˆ–é¢å¤–çš„ LLM æŸ¥è¯¢ã€‚\n",
      "    *   **å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Š:**\n",
      "        *   é€šè¿‡ç¡®ä¿ LLM ç»™å‡ºç®€æ´çš„â€œYes/Noâ€ç­”æ¡ˆï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘æ¯ä¸ªæŸ¥è¯¢çš„ token æ¶ˆè€—ï¼Œä»è€Œé™ä½å¤§è§„æ¨¡æœ¬ä½“å¯¹é½ä»»åŠ¡çš„æˆæœ¬ (é€šè¿‡ `cost_path` è®°å½•)ã€‚\n",
      "        *   `extract_yes_no` çš„æ•ˆç‡ç›´æ¥å½±å“æ•´ä¸ªè¯„ä¼°ç®¡é“çš„é€Ÿåº¦ï¼Œå¯¹äºå¤„ç†å¤§é‡å®ä½“å¯¹çš„æœ¬ä½“æ¥è¯´ï¼Œå…¶æ€§èƒ½è‡³å…³é‡è¦ã€‚\n",
      "  5. **ç†è§£é¡¹ç›®ä¸æ¨¡å—æ ¸å¿ƒç›®æ ‡:** æ ¹æ®é¡¹ç›®å `ontology-llm` å’Œæ ¸å¿ƒæ¨¡å— `llm_matching`, `generate_conference_benchmark`ï¼Œæ¨æ–­é¡¹ç›®æ—¨åœ¨åˆ©ç”¨ LLM è¿›è¡Œæœ¬ä½“åŒ¹é…å’Œè¯„ä¼°ã€‚`llm_om_few_shot.py` æ–‡ä»¶ååˆ™è¡¨æ˜äº†å…·ä½“å®ç°å¯èƒ½æ¶‰åŠ Few-shot å­¦ä¹ ã€‚\n",
      "  6. **åˆ†æ `extract_yes_no` å‡½æ•°çš„ä½œç”¨:** ä»å‡½æ•°åå’Œä»£ç ç‰‡æ®µä¸­ `re.search` çš„ç”¨æ³•ï¼ˆå°½ç®¡ä¸å®Œæ•´ï¼‰åˆ¤æ–­ï¼Œè¯¥å‡½æ•°ç”¨äºä»æ–‡æœ¬ä¸­æå–äºŒå…ƒï¼ˆYes/Noï¼‰å†³ç­–ã€‚è¿™æç¤º LLM çš„è¾“å‡ºé€šå¸¸éœ€è¦è¿™ç§åå¤„ç†ã€‚\n",
      "  7. **è¿æ¥ `extract_yes_no` åˆ° LLM äº¤äº’æµç¨‹:** è€ƒè™‘åˆ°æœ¬ä½“å¯¹é½é€šå¸¸æ¶‰åŠå¯¹å¤§é‡å®ä½“å¯¹è¿›è¡Œæ¯”è¾ƒï¼ŒLLM æœ€å¯èƒ½è¢«é—®è¯¢çš„æ˜¯â€œä¸¤ä¸ªå®ä½“æ˜¯å¦ç­‰ä»·ï¼Ÿâ€æ­¤ç±»é—®é¢˜ï¼Œå…¶ç†æƒ³å›ç­”æ˜¯äºŒå…ƒå†³ç­–ã€‚`extract_yes_no` æ­£æ˜¯ä¸ºæ­¤ç±»äº¤äº’è®¾è®¡çš„è§£æå™¨ã€‚\n",
      "  8. **è€ƒå¯Ÿ Few-shot/Zero-shot åœºæ™¯ä¸­çš„é€‚ç”¨æ€§:** è®¨è®ºæ— è®ºæ˜¯åœ¨ Few-shot æä¾›çš„ç¤ºä¾‹ä¸‹è¿˜æ˜¯åœ¨ Zero-shot æ›´è‡ªç”±çš„è¾“å‡ºä¸‹ï¼Œ`extract_yes_no` éƒ½æä¾›äº†ä¸€ç§ç»Ÿä¸€ä¸”é²æ£’çš„æ–¹å¼æ¥æå–å†³ç­–ï¼Œç¡®ä¿è¯„ä¼°çš„ä¸€è‡´æ€§ã€‚\n",
      "  9. **å…³è”åˆ°å®šé‡è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•:** æå–çš„â€œYes/Noâ€å†³ç­–æ˜¯è®¡ç®—æœ¬ä½“å¯¹é½ä»»åŠ¡ä¸­ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1 åˆ†æ•°ç­‰æ€§èƒ½æŒ‡æ ‡çš„ç›´æ¥ä¾æ®ã€‚è¿™äº›æŒ‡æ ‡æ˜¯ `generate_conference_benchmark` ç­‰æ¨¡å—è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°çš„å…³é”®ã€‚`true_path` çš„å­˜åœ¨è¿›ä¸€æ­¥è¯å®äº†è¿™ä¸€ç‚¹ã€‚\n",
      "  10. **æ¨å¯¼å¯¹æ¶æ„çš„å½±å“:**\n",
      "    *   **æç¤ºå·¥ç¨‹:** ä¸ºç¡®ä¿ `extract_yes_no` èƒ½æœ‰æ•ˆå·¥ä½œï¼ŒLLM çš„æç¤ºå¿…é¡»è®¾è®¡å¾—é¼“åŠ±æˆ–å¼ºåˆ¶ LLM ä»¥â€œYes/Noâ€å½¢å¼å›ç­”ã€‚\n",
      "    *   **åå¤„ç†:** `extract_yes_no` æ˜ç¡®äº†ä¸€ä¸ªåå¤„ç†é˜¶æ®µçš„å­˜åœ¨ï¼Œç”¨äºå°†åŸå§‹ LLM è¾“å‡ºè½¬åŒ–ä¸ºç»“æ„åŒ–æ•°æ®ã€‚\n",
      "    *   **å¥å£®æ€§ä¸è¾¹ç¼˜æƒ…å†µ:** è€ƒè™‘ `extract_yes_no` çš„é²æ£’æ€§ï¼Œä»¥åŠå¦‚ä½•å¤„ç† LLM ç»™å‡ºä¸æ˜ç¡®ç­”æ¡ˆçš„æƒ…å†µã€‚\n",
      "    *   **æˆæœ¬ä¸æ•ˆç‡:** ç®€æ´çš„â€œYes/Noâ€å›ç­”å¯ä»¥é™ä½ LLM token ä½¿ç”¨é‡ï¼Œæé«˜æ•ˆç‡ï¼Œè¿™å¯¹äºå¤§è§„æ¨¡è¯„ä¼°è‡³å…³é‡è¦ã€‚\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# éšæœºé€‰æ‹©ä¸€ä¸ªæ–‡ä»¶\n",
    "files = generator.discover_python_files()\n",
    "test_file = random.choice(files)\n",
    "rel_path = test_file.relative_to(project_path)\n",
    "\n",
    "print(f\"ğŸ“„ æµ‹è¯•æ–‡ä»¶: {rel_path}\\n\")\n",
    "\n",
    "# æå–ä»£ç ç‰‡æ®µ\n",
    "code_snippet = generator.extract_code_snippet(test_file, length=600)\n",
    "print(\"ã€ä»£ç ç‰‡æ®µé¢„è§ˆã€‘\")\n",
    "print(code_snippet[:300] + \"...\\n\")\n",
    "\n",
    "# ç”Ÿæˆé—®ç­”å¯¹\n",
    "print(\"ğŸ”„ ç”Ÿæˆé—®ç­”å¯¹...\")\n",
    "qa = generator.generate_qa_pair(\n",
    "    code_snippet=code_snippet,\n",
    "    file_path=str(rel_path),\n",
    "    use_context=True  # å¯ç”¨ä¸Šä¸‹æ–‡å¢å¼º\n",
    ")\n",
    "\n",
    "if qa:\n",
    "    print(\"\\nâœ… ç”ŸæˆæˆåŠŸï¼\\n\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"é—®é¢˜: {qa['question']}\\n\")\n",
    "    print(f\"ç­”æ¡ˆ: {qa['answer']}\\n\")\n",
    "    print(\"æ¨ç†æ­¥éª¤:\")\n",
    "    for i, step in enumerate(qa['reasoning_steps'], 1):\n",
    "        print(f\"  {i}. {step}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"âŒ ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b8f85",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ æ­¥éª¤4b: ç”Ÿæˆå•ä¸ªè®¾è®¡æ–¹æ¡ˆï¼ˆæµ‹è¯•ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8572551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ æµ‹è¯•éœ€æ±‚: ä¸ºé¡¹ç›®æ·»åŠ æ‰¹é‡å¤„ç†åŠŸèƒ½ï¼Œæ”¯æŒå¹¶å‘å¤„ç†å¤§é‡æ•°æ®\n",
      "\n",
      "ğŸ”„ ç”Ÿæˆè®¾è®¡æ–¹æ¡ˆ...\n",
      "\n",
      "âœ… ç”ŸæˆæˆåŠŸï¼\n",
      "\n",
      "======================================================================\n",
      "éœ€æ±‚: ä¸ºé¡¹ç›®æ·»åŠ æ‰¹é‡å¤„ç†åŠŸèƒ½ï¼Œæ”¯æŒå¹¶å‘å¤„ç†å¤§é‡æ•°æ®\n",
      "\n",
      "è§£å†³æ–¹æ¡ˆ: ç»“åˆ `ontology-llm` é¡¹ç›®çš„æ ¸å¿ƒæ¨¡å—ï¼Œæˆ‘ä»¬å°†é€šè¿‡å¼•å…¥ Python çš„ `concurrent.futures` æ¨¡å—æ¥å®ç°æ‰¹é‡å¹¶å‘å¤„ç†ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†æŠŠå•ä¸ªæœ¬ä½“åŒ¹é…ä»»åŠ¡ï¼ˆç”± `llm_om_zero_shot` æˆ– `llm_matching` æ¨¡å—æ‰§è¡Œï¼‰å°è£…ä¸ºå¯å¹¶è¡Œæ‰§è¡Œçš„å•å…ƒã€‚`run_series_conference` æ¨¡å—å°†ä½œä¸ºæ‰¹å¤„ç†çš„è°ƒåº¦å™¨ï¼Œè´Ÿè´£åŠ è½½ä»»åŠ¡åˆ—è¡¨ã€åˆ›å»ºçº¿ç¨‹æ± ï¼ˆæˆ–è¿›ç¨‹æ± ï¼Œå–å†³äºä»»åŠ¡æ€§è´¨ï¼‰ï¼Œåˆ†å‘ä»»åŠ¡ï¼Œå¹¶èšåˆç»“æœã€‚`util` æ¨¡å—å°†æä¾›è¾…åŠ©å‡½æ•°ï¼Œå¦‚ä»»åŠ¡é…ç½®è§£æã€è¿›åº¦ç®¡ç†å’Œç»“æœä¿å­˜ã€‚\n",
      "\n",
      "å®æ–½æ­¥éª¤:\n",
      "  1. **å®šä¹‰æ‰¹å¤„ç†ä»»åŠ¡å•å…ƒç»“æ„:**\n",
      "    *   ç¡®å®šä¸€ä¸ªæ ‡å‡†çš„æ•°æ®ç»“æ„ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªå­—å…¸æˆ–ä¸€ä¸ªPydanticæ¨¡å‹ï¼‰æ¥è¡¨ç¤ºä¸€ä¸ªç‹¬ç«‹çš„æœ¬ä½“åŒ¹é…ä»»åŠ¡ã€‚è¿™ä¸ªç»“æ„åº”è¯¥åŒ…å« `llm_om_zero_shot` æˆ– `llm_matching` æ¨¡å—æ‰§è¡Œä¸€æ¬¡åŒ¹é…æ‰€éœ€çš„æ‰€æœ‰å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œæœ¬ä½“å¯¹IDã€æ¨¡å‹é…ç½®ã€è¾“å…¥æ–‡ä»¶è·¯å¾„ã€è¾“å‡ºè·¯å¾„ç­‰ï¼‰ã€‚\n",
      "    *   **ç¤ºä¾‹:** `{\"task_id\": \"conference_pair_1_0\", \"ontology_a_path\": \"data/conf/conf_1.owl\", \"ontology_b_path\": \"data/conf/conf_0.owl\", \"llm_config\": {\"model_name\": \"gpt-3.5-turbo\", \"temperature\": 0.7}}`\n",
      "  2. **å°è£…æ ¸å¿ƒå¤„ç†é€»è¾‘ä¸ºç‹¬ç«‹å‡½æ•°:**\n",
      "    *   åœ¨ `llm_om_zero_shot.py` (å’Œ/æˆ– `llm_matching.py`) ä¸­ï¼Œå°†æ‰§è¡Œå•ä¸ªæœ¬ä½“åŒ¹é…çš„é€»è¾‘å°è£…æˆä¸€ä¸ªç‹¬ç«‹çš„ã€æ— çŠ¶æ€çš„å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°å°†æ¥æ”¶æ­¥éª¤1ä¸­å®šä¹‰çš„ä»»åŠ¡å•å…ƒç»“æ„ä½œä¸ºå‚æ•°ï¼Œå¹¶è¿”å›è¯¥ä»»åŠ¡çš„å¤„ç†ç»“æœï¼ˆä¾‹å¦‚ï¼ŒåŒ¹é…ç»“æœã€è¯„ä¼°æŒ‡æ ‡ã€æ—¥å¿—ä¿¡æ¯ï¼‰ã€‚\n",
      "    *   **ç¤ºä¾‹:** `def process_single_matching_task(task_config: Dict) -> Dict: ...` ç¡®ä¿è¿™ä¸ªå‡½æ•°å†…éƒ¨èƒ½å¤„ç†æ‰€æœ‰å¿…è¦çš„åŠ è½½ã€LLMè°ƒç”¨å’Œç»“æœè®¡ç®—ï¼Œå¹¶ä¸”æ˜¯çº¿ç¨‹å®‰å…¨çš„ã€‚\n",
      "  3. **å¼€å‘æ‰¹å¤„ç†è°ƒåº¦å™¨ (`run_series_conference.py`):**\n",
      "    *   åœ¨ `run_series_conference.py` ä¸­æ·»åŠ ä¸€ä¸ªæ–°å‡½æ•°ï¼Œä¾‹å¦‚ `run_concurrent_series(task_list: List[Dict], max_workers: int = os.cpu_count() * 2)`ã€‚\n",
      "    *   è¿™ä¸ªå‡½æ•°å°†ï¼š\n",
      "        *   æ¥æ”¶ä¸€ä¸ªæ‰¹å¤„ç†ä»»åŠ¡å•å…ƒåˆ—è¡¨ (`task_list`)ã€‚\n",
      "        *   ä½¿ç”¨ `concurrent.futures.ThreadPoolExecutor` (å¯¹äºLLM APIè°ƒç”¨é€šå¸¸æ˜¯I/Oå¯†é›†å‹ä»»åŠ¡ï¼Œçº¿ç¨‹æ± æ›´åˆé€‚) æˆ– `ProcessPoolExecutor` (å¦‚æœæœ¬ä½“é¢„å¤„ç†æ˜¯CPUå¯†é›†å‹) åˆå§‹åŒ–ä¸€ä¸ªæ‰§è¡Œå™¨ã€‚\n",
      "        *   éå† `task_list`ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡è°ƒç”¨æ­¥éª¤2ä¸­å°è£…çš„å‡½æ•°ï¼Œå¹¶ä½¿ç”¨ `executor.submit()` æäº¤ä»»åŠ¡åˆ°æ‰§è¡Œå™¨ã€‚\n",
      "        *   ä½¿ç”¨ `concurrent.futures.as_completed()` æˆ– `executor.map()` æ¥æ”¶é›†å·²å®Œæˆä»»åŠ¡çš„ç»“æœï¼ŒåŒæ—¶æä¾›è¿›åº¦æ¡ï¼ˆä¾‹å¦‚ä½¿ç”¨ `tqdm` åº“ï¼‰è¿›è¡Œå¯è§†åŒ–ã€‚\n",
      "        *   æ•è·å¹¶å¤„ç†å•ä¸ªä»»åŠ¡æ‰§è¡Œä¸­çš„å¼‚å¸¸ï¼Œç¡®ä¿ä¸€ä¸ªä»»åŠ¡çš„å¤±è´¥ä¸ä¼šä¸­æ–­æ•´ä¸ªæ‰¹å¤„ç†ã€‚\n",
      "        *   èšåˆæ‰€æœ‰ä»»åŠ¡çš„ç»“æœï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç»“æœå’Œæ½œåœ¨é”™è¯¯ä¿¡æ¯çš„åˆ—è¡¨ã€‚\n",
      "  4. **å¢å¼ºä»»åŠ¡åŠ è½½ä¸ç»“æœå­˜å‚¨ (`util.py`):**\n",
      "    *   åœ¨ `util.py` ä¸­æ·»åŠ å‡½æ•°æ¥åŠ è½½æ‰¹å¤„ç†ä»»åŠ¡åˆ—è¡¨ã€‚è¿™å¯èƒ½åŒ…æ‹¬ä»CSVã€JSONæ–‡ä»¶æˆ–æ ¹æ®æŸç§æ¨¡å¼æ‰«æç›®å½•æ¥ç”Ÿæˆä»»åŠ¡é…ç½®ã€‚\n",
      "    *   æ·»åŠ å‡½æ•°æ¥å°†èšåˆåçš„ç»“æœä¿å­˜åˆ°æŒ‡å®šä½ç½®ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªç»¼åˆçš„JSONæ–‡ä»¶ã€CSVæ–‡ä»¶ï¼Œæˆ–å°†æ¯ä¸ªä»»åŠ¡çš„ç»“æœä¿å­˜ä¸ºå•ç‹¬çš„æ–‡ä»¶ï¼‰ã€‚\n",
      "    *   è€ƒè™‘æ·»åŠ å¹¶å‘å®‰å…¨çš„æ—¥å¿—è®°å½•å™¨é…ç½®ã€‚\n",
      "  5. **æ›´æ–°ä¸»å…¥å£ç‚¹å’Œå‘½ä»¤è¡Œæ¥å£:**\n",
      "    *   ä¿®æ”¹é¡¹ç›®çš„ `main` å…¥å£ï¼ˆå¦‚æœå­˜åœ¨ï¼Œæˆ– `run_series_conference.py` çš„é¡¶å±‚æ‰§è¡Œé€»è¾‘ï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿè§£ææ–°çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œä¾‹å¦‚ `--batch_config_path` (æŒ‡å®šåŒ…å«æ‰¹å¤„ç†ä»»åŠ¡å®šä¹‰çš„æ–‡ä»¶è·¯å¾„)ã€`--max_workers` (æŒ‡å®šå¹¶å‘å·¥ä½œçº¿ç¨‹/è¿›ç¨‹æ•°)ã€`--output_batch_results_path`ã€‚\n",
      "    *   æ ¹æ®è¿™äº›å‚æ•°ï¼Œå†³å®šæ˜¯æ‰§è¡Œå•ä¸ªä»»åŠ¡è¿˜æ˜¯å¯åŠ¨æ‰¹å¤„ç†è°ƒåº¦å™¨ã€‚\n",
      "\n",
      "éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶:\n",
      "  - llm` é¡¹ç›®çš„æ ¸å¿ƒæ¨¡å—ï¼Œæˆ‘ä»¬å°†é€šè¿‡å¼•å…¥ Python çš„ `concurrent.futures` æ¨¡å—æ¥å®ç°æ‰¹é‡å¹¶å‘å¤„ç†ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†æŠŠå•ä¸ªæœ¬ä½“åŒ¹é…ä»»åŠ¡ï¼ˆç”± `llm_om_zero_shot` æˆ– `llm_matching` æ¨¡å—æ‰§è¡Œï¼‰å°è£…ä¸ºå¯å¹¶è¡Œæ‰§è¡Œçš„å•å…ƒã€‚`run_series_conference` æ¨¡å—å°†ä½œä¸ºæ‰¹å¤„ç†çš„è°ƒåº¦å™¨ï¼Œè´Ÿè´£åŠ è½½ä»»åŠ¡åˆ—è¡¨ã€åˆ›å»ºçº¿ç¨‹æ± ï¼ˆæˆ–è¿›ç¨‹æ± ï¼Œå–å†³äºä»»åŠ¡æ€§è´¨ï¼‰ï¼Œåˆ†å‘ä»»åŠ¡ï¼Œå¹¶èšåˆç»“æœã€‚`util` æ¨¡å—å°†æä¾›è¾…åŠ©å‡½æ•°ï¼Œå¦‚ä»»åŠ¡é…ç½®è§£æã€è¿›åº¦ç®¡ç†å’Œç»“æœä¿å­˜ã€‚\n",
      "\n",
      "Implementation Steps: 1.  **å®šä¹‰æ‰¹å¤„ç†ä»»åŠ¡å•å…ƒç»“æ„:**\n",
      "    *   ç¡®å®šä¸€ä¸ªæ ‡å‡†çš„æ•°æ®ç»“æ„ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªå­—å…¸æˆ–ä¸€ä¸ªPydanticæ¨¡å‹ï¼‰æ¥è¡¨ç¤ºä¸€ä¸ªç‹¬ç«‹çš„æœ¬ä½“åŒ¹é…ä»»åŠ¡ã€‚è¿™ä¸ªç»“æ„åº”è¯¥åŒ…å« `llm_om_zero_shot` æˆ– `llm_matching` æ¨¡å—æ‰§è¡Œä¸€æ¬¡åŒ¹é…æ‰€éœ€çš„æ‰€æœ‰å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œæœ¬ä½“å¯¹IDã€æ¨¡å‹é…ç½®ã€è¾“å…¥æ–‡ä»¶è·¯å¾„ã€è¾“å‡ºè·¯å¾„ç­‰ï¼‰ã€‚\n",
      "    *   **ç¤ºä¾‹:** `{\"task_id\": \"conference_pair_1_0\", \"ontology_a_path\": \"data/conf/conf_1.owl\", \"ontology_b_path\": \"data/conf/conf_0.owl\", \"llm_config\": {\"model_name\": \"gpt-3.5-turbo\", \"temperature\": 0.7}}`\n",
      "  - > Dict: ...` ç¡®ä¿è¿™ä¸ªå‡½æ•°å†…éƒ¨èƒ½å¤„ç†æ‰€æœ‰å¿…è¦çš„åŠ è½½ã€LLMè°ƒç”¨å’Œç»“æœè®¡ç®—ï¼Œå¹¶ä¸”æ˜¯çº¿ç¨‹å®‰å…¨çš„ã€‚\n",
      "  - -batch_config_path` (æŒ‡å®šåŒ…å«æ‰¹å¤„ç†ä»»åŠ¡å®šä¹‰çš„æ–‡ä»¶è·¯å¾„)ã€`--max_workers` (æŒ‡å®šå¹¶å‘å·¥ä½œçº¿ç¨‹/è¿›ç¨‹æ•°)ã€`--output_batch_results_path`ã€‚\n",
      "    *   æ ¹æ®è¿™äº›å‚æ•°ï¼Œå†³å®šæ˜¯æ‰§è¡Œå•ä¸ªä»»åŠ¡è¿˜æ˜¯å¯åŠ¨æ‰¹å¤„ç†è°ƒåº¦å™¨ã€‚\n",
      "\n",
      "Files to Modify: *   **`run_series_conference.py`**:\n",
      "    *   **ä¿®æ”¹åŸå› :** è¿™æ˜¯é¡¹ç›®è¿è¡Œâ€œç³»åˆ—â€ä¼šè®®ç›¸å…³ä»»åŠ¡çš„æ ¸å¿ƒè°ƒåº¦æ¨¡å—ï¼Œéå¸¸é€‚åˆæ‰©å±•ä¸ºæ‰¹å¤„ç†çš„è°ƒåº¦ä¸­å¿ƒã€‚éœ€è¦æ·»åŠ  `run_concurrent_series` å‡½æ•°ï¼ŒåŒ…å« `ThreadPoolExecutor` çš„é€»è¾‘ã€ä»»åŠ¡æäº¤ã€ç»“æœæ”¶é›†å’Œé”™è¯¯å¤„ç†ã€‚\n",
      "*   **`llm_om_zero_shot.py`** (å’Œ/æˆ– **`llm_matching.py`**):\n",
      "    *   **ä¿®æ”¹åŸå› :** è¿™äº›æ˜¯æ‰§è¡Œå•ä¸ªæœ¬ä½“åŒ¹é…ä»»åŠ¡çš„æ¨¡å—ã€‚éœ€è¦å°†æ ¸å¿ƒåŒ¹é…é€»è¾‘å°è£…æˆä¸€ä¸ªç‹¬ç«‹çš„ã€å¯æ¥æ”¶å‚æ•°å¹¶è¿”å›ç»“æœçš„å‡½æ•°ï¼Œä»¥ä¾¿åœ¨å¹¶å‘ç¯å¢ƒä¸­è¢«è°ƒåº¦å™¨è°ƒç”¨ã€‚\n",
      "*   **`util.py`**:\n",
      "    *   **ä¿®æ”¹åŸå› :** é€šç”¨å·¥å…·æ¨¡å—ã€‚éœ€è¦æ·»åŠ å‡½æ•°æ¥ï¼š\n",
      "        *   è§£æå’ŒåŠ è½½æ‰¹å¤„ç†ä»»åŠ¡é…ç½®æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼Œ`load_batch_tasks_from_json`ï¼‰ã€‚\n",
      "        *   ä¿å­˜èšåˆåçš„æ‰¹å¤„ç†ç»“æœï¼ˆä¾‹å¦‚ï¼Œ`save_batch_results_to_json`ï¼‰ã€‚\n",
      "        *   å¯èƒ½éœ€è¦ä¸ºå¹¶å‘ç¯å¢ƒè°ƒæ•´æ—¥å¿—é…ç½®ã€‚\n",
      "*   **(å¯é€‰) æ–°å¢æ–‡ä»¶ï¼Œä¾‹å¦‚ `batch_config_template.json`**:\n",
      "    *   **ä¿®æ”¹åŸå› :** æä¾›ä¸€ä¸ªç¤ºä¾‹é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å¯¼ç”¨æˆ·å¦‚ä½•å®šä¹‰æ‰¹å¤„ç†ä»»åŠ¡åˆ—è¡¨ã€‚\n",
      "*   **(å¦‚æœå­˜åœ¨) `main.py` æˆ–é¡¹ç›®ä¸»æ‰§è¡Œè„šæœ¬**:\n",
      "    *   **ä¿®æ”¹åŸå› :** æ·»åŠ å‘½ä»¤è¡Œå‚æ•°è§£æï¼Œä»¥æ”¯æŒæ‰¹å¤„ç†æ¨¡å¼å’Œç›¸å…³é…ç½®ï¼ˆå¦‚æœ€å¤§å¹¶å‘æ•°ã€æ‰¹å¤„ç†è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼‰ã€‚\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰éœ€æ±‚\n",
    "test_requirement = \"ä¸ºé¡¹ç›®æ·»åŠ æ‰¹é‡å¤„ç†åŠŸèƒ½ï¼Œæ”¯æŒå¹¶å‘å¤„ç†å¤§é‡æ•°æ®\"\n",
    "\n",
    "print(f\"ğŸ“‹ æµ‹è¯•éœ€æ±‚: {test_requirement}\\n\")\n",
    "\n",
    "# ç”Ÿæˆè®¾è®¡æ–¹æ¡ˆ\n",
    "print(\"ğŸ”„ ç”Ÿæˆè®¾è®¡æ–¹æ¡ˆ...\")\n",
    "design = generator.generate_design_solution(\n",
    "    requirement=test_requirement,\n",
    "    use_context=True\n",
    ")\n",
    "\n",
    "if design:\n",
    "    print(\"\\nâœ… ç”ŸæˆæˆåŠŸï¼\\n\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"éœ€æ±‚: {design['requirement']}\\n\")\n",
    "    print(f\"è§£å†³æ–¹æ¡ˆ: {design['solution']}\\n\")\n",
    "    print(\"å®æ–½æ­¥éª¤:\")\n",
    "    for i, step in enumerate(design['steps'], 1):\n",
    "        print(f\"  {i}. {step}\")\n",
    "    print(\"\\néœ€è¦ä¿®æ”¹çš„æ–‡ä»¶:\")\n",
    "    for item in design['files_to_modify']:\n",
    "        print(f\"  - {item['file']}: {item['reason']}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"âŒ ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75667fa9",
   "metadata": {},
   "source": [
    "## ğŸš€ æ­¥éª¤5: æ‰¹é‡ç”Ÿæˆè®­ç»ƒæ•°æ®\n",
    "\n",
    "ä¸€é”®ç”Ÿæˆå®Œæ•´è®­ç»ƒæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba640d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ¯ æ‰¹é‡ç”Ÿæˆé…ç½®\n",
      "======================================================================\n",
      "   é—®ç­”å¯¹æ•°é‡: 10\n",
      "   è®¾è®¡æ–¹æ¡ˆæ•°é‡: 5\n",
      "   ä¸Šä¸‹æ–‡å¢å¼º: True\n",
      "   é¢„è®¡è€—æ—¶: 3.8 åˆ†é’Ÿ\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ å¼€å§‹ç”Ÿæˆè®­ç»ƒæ•°æ®\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ å‘ç°Pythonæ–‡ä»¶...\n",
      "   æ‰¾åˆ° 13 ä¸ªæ–‡ä»¶\n",
      "\n",
      "ğŸ“ ç”Ÿæˆ 10 ä¸ªé—®ç­”å¯¹...\n",
      "   [1/10] å¤„ç†æ–‡ä»¶: run_series_conference.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [2/10] å¤„ç†æ–‡ä»¶: llm_om_few_shot.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [3/10] å¤„ç†æ–‡ä»¶: llm_matching.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [4/10] å¤„ç†æ–‡ä»¶: generate_anatomy_mse_benchmark.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [5/10] å¤„ç†æ–‡ä»¶: llm_matching.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [6/10] å¤„ç†æ–‡ä»¶: fix_multifarm_reference.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [7/10] å¤„ç†æ–‡ä»¶: generate_anatomy_mse_benchmark.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [8/10] å¤„ç†æ–‡ä»¶: fix_multifarm_reference.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [9/10] å¤„ç†æ–‡ä»¶: generate_anatomy_mse_benchmark.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [10/10] å¤„ç†æ–‡ä»¶: generate_anatomy_mse_benchmark.py\n",
      "       âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ—ï¸  ç”Ÿæˆ 5 ä¸ªè®¾è®¡æ–¹æ¡ˆ...\n",
      "   [1/5] éœ€æ±‚: æ‰©å±•generate_conference_benchmarkä»¥æ”¯æŒå¼±ç½‘ç¯å¢ƒåœºæ™¯...\n",
      "       âœ… æˆåŠŸ\n",
      "   [2/5] éœ€æ±‚: é‡æ„om_csv_to_databaseä»¥æ”¯æŒå›½é™…åŒ–...\n",
      "       âœ… æˆåŠŸ\n",
      "   [3/5] éœ€æ±‚: ä¸ºfix_multifarm_referenceæ·»åŠ é”™è¯¯å¤„ç†åŠŸèƒ½...\n",
      "       âœ… æˆåŠŸ\n",
      "   [4/5] éœ€æ±‚: é›†æˆgRPCåˆ°generate_conference_benchmarkä¸­...\n",
      "       âœ… æˆåŠŸ\n",
      "   [5/5] éœ€æ±‚: ä¸ºom_csv_to_databaseæ·»åŠ é…ç½®ç®¡ç†åŠŸèƒ½...\n",
      "       âœ… æˆåŠŸ\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ç”Ÿæˆå®Œæˆ\n",
      "======================================================================\n",
      "   é—®ç­”å¯¹: 10/10\n",
      "   è®¾è®¡æ–¹æ¡ˆ: 5/5\n",
      "   æˆåŠŸç‡: 100.0%\n",
      "\n",
      "ğŸ‰ æ‰¹é‡ç”Ÿæˆå®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# é…ç½®ç”Ÿæˆå‚æ•°\n",
    "NUM_QA_PAIRS = 10      # é—®ç­”å¯¹æ•°é‡\n",
    "NUM_DESIGN = 5         # è®¾è®¡æ–¹æ¡ˆæ•°é‡\n",
    "USE_CONTEXT = True     # æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡å¢å¼º\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ¯ æ‰¹é‡ç”Ÿæˆé…ç½®\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   é—®ç­”å¯¹æ•°é‡: {NUM_QA_PAIRS}\")\n",
    "print(f\"   è®¾è®¡æ–¹æ¡ˆæ•°é‡: {NUM_DESIGN}\")\n",
    "print(f\"   ä¸Šä¸‹æ–‡å¢å¼º: {USE_CONTEXT}\")\n",
    "print(f\"   é¢„è®¡è€—æ—¶: {(NUM_QA_PAIRS + NUM_DESIGN) * 15 / 60:.1f} åˆ†é’Ÿ\\n\")\n",
    "\n",
    "# æ‰§è¡Œæ‰¹é‡ç”Ÿæˆ\n",
    "dataset = generator.generate_batch(\n",
    "    num_qa=NUM_QA_PAIRS,\n",
    "    num_design=NUM_DESIGN,\n",
    "    use_context=USE_CONTEXT\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ‰ æ‰¹é‡ç”Ÿæˆå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36ebda",
   "metadata": {},
   "source": [
    "## ğŸ“Š æ­¥éª¤6: æ•°æ®è´¨é‡åˆ†æ\n",
    "\n",
    "åˆ†æç”Ÿæˆæ•°æ®çš„è´¨é‡æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "570ed3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“Š é—®ç­”å¯¹è´¨é‡åˆ†æ\n",
      "======================================================================\n",
      "   æ€»æ•°é‡: 10\n",
      "   å¹³å‡é—®é¢˜é•¿åº¦: 497 å­—ç¬¦\n",
      "   å¹³å‡ç­”æ¡ˆé•¿åº¦: 4152 å­—ç¬¦\n",
      "   å¹³å‡æ¨ç†æ­¥éª¤: 12.3 æ­¥\n",
      "\n",
      "   æ–‡ä»¶è¦†ç›–æ•°: 5\n",
      "   æ–‡ä»¶åˆ†å¸ƒ: {'generate_anatomy_mse_benchmark.py': 4, 'llm_matching.py': 2, 'fix_multifarm_reference.py': 2}\n",
      "\n",
      "   ã€ç¤ºä¾‹é—®ç­”å¯¹ã€‘\n",
      "   Q: The provided code snippet lists a series of paths such as `\"conference/cmt-sigkd...\n",
      "   A: The path structure exemplified by `\"conference/cmt-sigkdd/component/\"` holds sig...\n",
      "\n",
      "======================================================================\n",
      "ğŸ—ï¸  è®¾è®¡æ–¹æ¡ˆè´¨é‡åˆ†æ\n",
      "======================================================================\n",
      "   æ€»æ•°é‡: 5\n",
      "   å¹³å‡å®æ–½æ­¥éª¤: 7.6 æ­¥\n",
      "   å¹³å‡ä¿®æ”¹æ–‡ä»¶: 11.6 ä¸ª\n",
      "\n",
      "   ã€ç¤ºä¾‹è®¾è®¡æ–¹æ¡ˆã€‘\n",
      "   éœ€æ±‚: æ‰©å±•generate_conference_benchmarkä»¥æ”¯æŒå¼±ç½‘ç¯å¢ƒåœºæ™¯...\n",
      "   æ–¹æ¡ˆ: ä¸ºäº†æ‰©å±•`generate_conference_benchmark`ä»¥æ”¯æŒå¼±ç½‘ç¯å¢ƒï¼Œæ ¸å¿ƒç­–ç•¥å°†å›´ç»•**æé«˜LLM AP...\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# ç»Ÿè®¡é—®ç­”å¯¹\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š é—®ç­”å¯¹è´¨é‡åˆ†æ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if dataset['qa_pairs']:\n",
    "    # åŸºç¡€ç»Ÿè®¡\n",
    "    total_qa = len(dataset['qa_pairs'])\n",
    "    avg_question_len = sum(len(qa['question']) for qa in dataset['qa_pairs']) / total_qa\n",
    "    avg_answer_len = sum(len(qa['answer']) for qa in dataset['qa_pairs']) / total_qa\n",
    "    avg_reasoning_steps = sum(len(qa['reasoning_steps']) for qa in dataset['qa_pairs']) / total_qa\n",
    "    \n",
    "    print(f\"   æ€»æ•°é‡: {total_qa}\")\n",
    "    print(f\"   å¹³å‡é—®é¢˜é•¿åº¦: {avg_question_len:.0f} å­—ç¬¦\")\n",
    "    print(f\"   å¹³å‡ç­”æ¡ˆé•¿åº¦: {avg_answer_len:.0f} å­—ç¬¦\")\n",
    "    print(f\"   å¹³å‡æ¨ç†æ­¥éª¤: {avg_reasoning_steps:.1f} æ­¥\")\n",
    "    \n",
    "    # æ–‡ä»¶åˆ†å¸ƒ\n",
    "    source_files = [qa.get('source_file', 'unknown') for qa in dataset['qa_pairs']]\n",
    "    file_dist = Counter(source_files)\n",
    "    print(f\"\\n   æ–‡ä»¶è¦†ç›–æ•°: {len(file_dist)}\")\n",
    "    print(f\"   æ–‡ä»¶åˆ†å¸ƒ: {dict(list(file_dist.most_common(3)))}\")\n",
    "    \n",
    "    # ç¤ºä¾‹å±•ç¤º\n",
    "    print(\"\\n   ã€ç¤ºä¾‹é—®ç­”å¯¹ã€‘\")\n",
    "    sample = dataset['qa_pairs'][0]\n",
    "    print(f\"   Q: {sample['question'][:80]}...\")\n",
    "    print(f\"   A: {sample['answer'][:80]}...\")\n",
    "else:\n",
    "    print(\"   âš ï¸  æœªç”Ÿæˆé—®ç­”å¯¹\")\n",
    "\n",
    "# ç»Ÿè®¡è®¾è®¡æ–¹æ¡ˆ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ—ï¸  è®¾è®¡æ–¹æ¡ˆè´¨é‡åˆ†æ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if dataset['design_solutions']:\n",
    "    total_design = len(dataset['design_solutions'])\n",
    "    avg_steps = sum(len(d['steps']) for d in dataset['design_solutions']) / total_design\n",
    "    avg_files = sum(len(d['files_to_modify']) for d in dataset['design_solutions']) / total_design\n",
    "    \n",
    "    print(f\"   æ€»æ•°é‡: {total_design}\")\n",
    "    print(f\"   å¹³å‡å®æ–½æ­¥éª¤: {avg_steps:.1f} æ­¥\")\n",
    "    print(f\"   å¹³å‡ä¿®æ”¹æ–‡ä»¶: {avg_files:.1f} ä¸ª\")\n",
    "    \n",
    "    # ç¤ºä¾‹å±•ç¤º\n",
    "    print(\"\\n   ã€ç¤ºä¾‹è®¾è®¡æ–¹æ¡ˆã€‘\")\n",
    "    sample = dataset['design_solutions'][0]\n",
    "    print(f\"   éœ€æ±‚: {sample['requirement'][:60]}...\")\n",
    "    print(f\"   æ–¹æ¡ˆ: {sample['solution'][:60]}...\")\n",
    "else:\n",
    "    print(\"   âš ï¸  æœªç”Ÿæˆè®¾è®¡æ–¹æ¡ˆ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d744f",
   "metadata": {},
   "source": [
    "## ğŸ’¾ æ­¥éª¤7: ä¿å­˜è®­ç»ƒæ•°æ®\n",
    "\n",
    "å°†æ•°æ®ä¿å­˜ä¸ºJSONæ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e76bc51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ æ•°æ®å·²ä¿å­˜: outputs/ontology-llm/training_data_20251219_131646.json\n",
      "   æ–‡ä»¶å¤§å°: 201.9 KB\n",
      "\n",
      "âœ… æ–‡ä»¶å·²ä¿å­˜\n",
      "   è·¯å¾„: outputs/ontology-llm/training_data_20251219_131646.json\n",
      "   å¤§å°: 201.9 KB\n",
      "   é—®ç­”å¯¹: 10\n",
      "   è®¾è®¡æ–¹æ¡ˆ: 5\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "project_name = project_path.name\n",
    "output_file = f\"outputs/{project_name}/training_data_{timestamp}.json\"\n",
    "\n",
    "# ä¿å­˜æ•°æ®\n",
    "generator.save_dataset(dataset, output_file)\n",
    "\n",
    "# éªŒè¯æ–‡ä»¶\n",
    "output_path = Path(output_file)\n",
    "if output_path.exists():\n",
    "    file_size = output_path.stat().st_size\n",
    "    print(f\"\\nâœ… æ–‡ä»¶å·²ä¿å­˜\")\n",
    "    print(f\"   è·¯å¾„: {output_path}\")\n",
    "    print(f\"   å¤§å°: {file_size / 1024:.1f} KB\")\n",
    "    \n",
    "    # è¯»å–éªŒè¯\n",
    "    with open(output_path, 'r', encoding='utf-8') as f:\n",
    "        loaded = json.load(f)\n",
    "    print(f\"   é—®ç­”å¯¹: {len(loaded['qa_pairs'])}\")\n",
    "    print(f\"   è®¾è®¡æ–¹æ¡ˆ: {len(loaded['design_solutions'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb4c32a",
   "metadata": {},
   "source": [
    "## ğŸ“ æ­¥éª¤8: æ•°æ®é¢„è§ˆ\n",
    "\n",
    "æŸ¥çœ‹å®Œæ•´çš„è®­ç»ƒæ•°æ®æ ·æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a89489e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“ å®Œæ•´é—®ç­”å¯¹ç¤ºä¾‹\n",
      "======================================================================\n",
      "{\n",
      "  \"question\": \"The provided code snippet lists a series of paths such as `\\\"conference/cmt-sigkdd/component/\\\"` and `\\\"conference/confof-edas/component/\\\"`. Given the project `ontology-llm`, its purpose in working with ontologies, and core modules like `llm_om_zero_shot` and `run_series_conference`, what is the architectural significance of this path structure for organizing and executing ontology matching benchmarks, and how does `run_series_conference.py` leverage it to facilitate the systematic evaluation process?\",\n",
      "  \"answer\": \"The path structure exemplified by `\\\"conference/cmt-sigkdd/component/\\\"` holds significant architectural importance within the `ontology-llm` project for organizing and automating ontology matching benchmarks.\\n\\n**Architectural Significance:**\\n\\n1.  **Modularization of Matching Tasks:** Each unique path (`conference/<ontology1>-<ontology2>/component/`) represents a distinct, self-contained ontology matching task. This structure effectively modularizes the overall benchmarking suite. For example, `cmt-sigkdd` clearly denotes the task of aligning the CMT ontology with the SigKDD ontology.\\n2.  **Standardized Benchmark Organization:** This approach establishes a consistent and standardized way to organize multiple ontology matching problems, particularly those derived from well-known datasets like the OAEI (Ontology Alignment Evaluation Initiative) Conference track. The `component/` subdirectory within each pair's folder likely contains all necessary resources for that specific task: the source ontology file (e.g., `cmt.owl`), the target ontology file (e.g., `sigkdd.owl`), and potentially a ground truth (reference) alignment file for evaluation.\\n3.  **Isolation and Reproducibility:** By encapsulating each matching pair within its own dedicated directory, the structure ensures isolation. This prevents interference between different experiments, makes it easier to manage task-specific data or configurations, and significantly enhances the reproducibility of experimental results.\\n4.  **Clear Identification of Evaluation Units:** Each path serves as a unique identifier for an evaluation unit. This clarity is crucial for managing large-scale benchmark series, reporting results for specific ontology pairs, and debugging individual matching problems.\\n\\n**Leveraging by `run_series_conference.py`:**\\n\\nThe `run_series_conference.py` script is designed to orchestrate the execution of these benchmarks systematically:\\n\\n1.  **Iteration and Automation:** The script would iterate through this list of paths, treating each entry as a directive to run a specific ontology matching experiment. This automates the execution of a whole \\\"series\\\" of matching tasks, eliminating manual intervention for each pair.\\n2.  **Context Switching:** For each path, `run_series_conference.py` would likely:\\n    *   Change the current working directory or construct full paths to the required files (ontologies, reference alignments) within the specified `component/` directory.\\n    *   Parse the directory name (e.g., `cmt-sigkdd`) to infer the names of the source and target ontologies.\\n3.  **Invocation of `llm_om_zero_shot`:** With the correct file paths identified, the script would then invoke the `llm_om_zero_shot` (or `llm_matching`) module. This module would be responsible for loading the two ontologies, applying the LLM-based zero-shot matching logic, and generating an alignment.\\n4.  **Result Management and Evaluation:** After `llm_om_zero_shot` completes, `run_series_conference.py` would then manage the output. It would likely use the `generate_conference_benchmark` or `util` modules to:\\n    *   Store the generated alignment file within the respective `component/` directory.\\n    *   Compare the generated alignment against the reference alignment (if available) to calculate evaluation metrics (precision, recall, F-measure).\\n    *   Aggregate these results across all benchmark tasks to provide an overall performance assessment.\\n5.  **Error Handling and Logging:** The `subprocess` and `os` dependencies would be critical here, allowing the script to safely navigate directories, execute external tools (if any), and robustly manage file I/O and potential errors during the execution of each individual benchmark task.\\n\\nIn essence, this path structure provides the blueprint for a highly organized, automated, and scalable benchmarking framework, enabling the `ontology-llm` project to rigorously evaluate its LLM-based ontology matching approaches across a diverse set of real-world problems.\",\n",
      "  \"reasoning_steps\": [\n",
      "    \"**Modularization of Matching Tasks:** Each unique path (`conference/<ontology1>-<ontology2>/component/`) represents a distinct, self-contained ontology matching task. This structure effectively modularizes the overall benchmarking suite. For example, `cmt-sigkdd` clearly denotes the task of aligning the CMT ontology with the SigKDD ontology.\",\n",
      "    \"**Standardized Benchmark Organization:** This approach establishes a consistent and standardized way to organize multiple ontology matching problems, particularly those derived from well-known datasets like the OAEI (Ontology Alignment Evaluation Initiative) Conference track. The `component/` subdirectory within each pair's folder likely contains all necessary resources for that specific task: the source ontology file (e.g., `cmt.owl`), the target ontology file (e.g., `sigkdd.owl`), and potentially a ground truth (reference) alignment file for evaluation.\",\n",
      "    \"**Isolation and Reproducibility:** By encapsulating each matching pair within its own dedicated directory, the structure ensures isolation. This prevents interference between different experiments, makes it easier to manage task-specific data or configurations, and significantly enhances the reproducibility of experimental results.\",\n",
      "    \"**Clear Identification of Evaluation Units:** Each path serves as a unique identifier for an evaluation unit. This clarity is crucial for managing large-scale benchmark series, reporting results for specific ontology pairs, and debugging individual matching problems.\",\n",
      "    \"**Iteration and Automation:** The script would iterate through this list of paths, treating each entry as a directive to run a specific ontology matching experiment. This automates the execution of a whole \\\"series\\\" of matching tasks, eliminating manual intervention for each pair.\",\n",
      "    \"**Context Switching:** For each path, `run_series_conference.py` would likely:\\n    *   Change the current working directory or construct full paths to the required files (ontologies, reference alignments) within the specified `component/` directory.\\n    *   Parse the directory name (e.g., `cmt-sigkdd`) to infer the names of the source and target ontologies.\",\n",
      "    \"**Invocation of `llm_om_zero_shot`:** With the correct file paths identified, the script would then invoke the `llm_om_zero_shot` (or `llm_matching`) module. This module would be responsible for loading the two ontologies, applying the LLM-based zero-shot matching logic, and generating an alignment.\",\n",
      "    \"**Result Management and Evaluation:** After `llm_om_zero_shot` completes, `run_series_conference.py` would then manage the output. It would likely use the `generate_conference_benchmark` or `util` modules to:\\n    *   Store the generated alignment file within the respective `component/` directory.\\n    *   Compare the generated alignment against the reference alignment (if available) to calculate evaluation metrics (precision, recall, F-measure).\\n    *   Aggregate these results across all benchmark tasks to provide an overall performance assessment.\",\n",
      "    \"**Error Handling and Logging:** The `subprocess` and `os` dependencies would be critical here, allowing the script to safely navigate directories, execute external tools (if any), and robustly manage file I/O and potential errors during the execution of each individual benchmark task.\",\n",
      "    \"**Analyze Code Snippet and Identify Pattern:** The provided code is a list of strings, all following the pattern `conference/<ontology1>-<ontology2>/component/`. The names like `cmt`, `sigkdd`, `confof`, `edas`, `ekaw`, `iasted` are recognized as specific ontologies commonly used in the OAEI Conference track for ontology matching. This immediately suggests that each string represents a specific ontology matching task.\",\n",
      "    \"**Connect to Project Context and Core Modules:**\\n    *   `ontology-llm`: Confirms the domain is ontologies, likely involving LLMs.\\n    *   `run_series_conference.py`: The filename explicitly indicates running a *series* of *conference* related tasks, strongly correlating with the list of paths.\\n    *   `llm_om_zero_shot`, `llm_matching`: These modules are the core logic for performing ontology matching using LLMs. They would need input (the two ontologies) for each task.\\n    *   `generate_conference_benchmark`, `util`: These modules suggest data preparation, result aggregation, and general utility, fitting into a benchmarking workflow.\\n    *   `subprocess`, `os`: Indicate file system operations and potential execution of external commands, necessary for managing experiment environments.\",\n",
      "    \"**Infer Architectural Significance:** Given the pattern and project context, the paths must serve as a structured way to organize the benchmark datasets. Each path is a unit of work. This leads to the concepts of modularization, standardization, isolation, and clear identification of tasks, which are key aspects of a robust benchmarking architecture.\",\n",
      "    \"**Deduce Workflow of `run_series_conference.py`:** The \\\"series\\\" aspect implies iteration. The script would loop through this list. For each path, it must extract the ontology pair, locate the actual ontology files within the `component/` directory, invoke the appropriate LLM matching module (`llm_om_zero_shot`), and then handle the output and evaluation. This demonstrates how the script \\\"leverages\\\" the structure.\",\n",
      "    \"**Detail Module Interaction:** Explain how `llm_om_zero_shot` would receive its inputs based on the path structure and how the results would be processed (e.g., by `generate_conference_benchmark` for evaluation) after the matching step. The roles of `os` and `subprocess` in managing the file system and potentially executing external processes for each task are also crucial to this workflow.\"\n",
      "  ],\n",
      "  \"code_context\": \"/\\\",\\n                  \\\"conference/cmt-sigkdd/component/\\\",\\n                  \\\"conference/conference-confof/component/\\\",\\n                  \\\"conference/conference-edas/component/\\\",\\n                  \\\"conference/conference-ekaw/component/\\\",\\n                  \\\"conference/conference-iasted/component/\\\",\\n                  \\\"conference/conference-sigkdd/component/\\\",\\n                  \\\"conference/confof-edas/component/\\\",\\n                  \\\"conference/confof-ekaw/component/\\\",\\n                  \\\"conference/confof-iasted/component/\\\",\\n                  \\\"conference/confof-sigkdd/component/\\\",\\n                  \\\"conference/edas-ekaw/component/\\\",\\n                  \\\"conference/edas-iasted/component/\\\",\\n                  \\\"conference/edas-sigkdd/component/\\\",\\n                  \\\"conference/ekaw-iasted/component/\\\",\",\n",
      "  \"source_file\": \"run_series_conference.py\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gemini-2.5-flash\",\n",
      "    \"temperature\": 0.3,\n",
      "    \"timestamp\": \"2025-12-19T13:10:30.169611\",\n",
      "    \"context_enabled\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ—ï¸  å®Œæ•´è®¾è®¡æ–¹æ¡ˆç¤ºä¾‹\n",
      "======================================================================\n",
      "{\n",
      "  \"requirement\": \"æ‰©å±•generate_conference_benchmarkä»¥æ”¯æŒå¼±ç½‘ç¯å¢ƒåœºæ™¯\",\n",
      "  \"solution\": \"ä¸ºäº†æ‰©å±•`generate_conference_benchmark`ä»¥æ”¯æŒå¼±ç½‘ç¯å¢ƒï¼Œæ ¸å¿ƒç­–ç•¥å°†å›´ç»•**æé«˜LLM APIè°ƒç”¨çš„é²æ£’æ€§**å’Œ**æ”¯æŒä»»åŠ¡ä¸­æ–­åçš„æ¢å¤**ã€‚è¿™åŒ…æ‹¬å¼•å…¥**é‡è¯•æœºåˆ¶**ï¼ˆå¸¦æŒ‡æ•°é€€é¿ï¼‰ï¼Œ**è¯·æ±‚è¶…æ—¶**ï¼Œä»¥åŠ**ä¸­é—´ç»“æœç¼“å­˜ä¸ä¿å­˜/æ¢å¤æœºåˆ¶**ã€‚è¿™äº›åŠŸèƒ½å°†ä½œä¸ºç‹¬ç«‹çš„ã€å¯é…ç½®çš„æ¨¡å—æ·»åŠ åˆ°`util`ä¸­ï¼Œç„¶åé›†æˆåˆ°`llm_om_zero_shot`å’Œ`llm_matching`ç­‰æ ¸å¿ƒLLMäº¤äº’æ¨¡å—ï¼Œæœ€ç»ˆç”±`generate_conference_benchmark`è¿›è¡Œåè°ƒå’Œç®¡ç†ã€‚\",\n",
      "  \"steps\": [\n",
      "    \"**åœ¨`util.py`ä¸­å®ç°ä¸€ä¸ªé€šç”¨çš„LLM APIé‡è¯•ä¸è¶…æ—¶è£…é¥°å™¨/å‡½æ•°:**\\n    *   **ç›®çš„**: å°è£…LLM APIè°ƒç”¨ï¼Œä½¿å…¶åœ¨é‡åˆ°ç½‘ç»œé”™è¯¯ã€APIæœåŠ¡ç¬æ—¶æ•…éšœï¼ˆå¦‚500é”™è¯¯ã€é€Ÿç‡é™åˆ¶ï¼‰æ—¶èƒ½å¤Ÿè‡ªåŠ¨é‡è¯•ï¼Œå¹¶è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´é˜²æ­¢é•¿æ—¶é—´é˜»å¡ã€‚\\n    *   **å…·ä½“å®ç°**:\\n        *   å¼•å…¥ç¬¬ä¸‰æ–¹åº“ï¼Œä¾‹å¦‚ `tenacity` ç”¨äºé‡è¯•ï¼Œç»“åˆ `requests` æˆ– LLM SDK è‡ªå¸¦çš„è¶…æ—¶å‚æ•°ã€‚\\n        *   åˆ›å»ºä¸€ä¸ªè£…é¥°å™¨ `@retry_llm_call`ï¼Œæ¥æ”¶å‚æ•°å¦‚æœ€å¤§é‡è¯•æ¬¡æ•°ã€é‡è¯•é—´éš”ï¼ˆæŒ‡æ•°é€€é¿ï¼‰ã€è¦æ•è·çš„å¼‚å¸¸ç±»å‹ï¼ˆç½‘ç»œé”™è¯¯ã€HTTPé”™è¯¯ã€APIé€Ÿç‡é™åˆ¶é”™è¯¯ç­‰ï¼‰ã€‚\\n        *   è¯¥è£…é¥°å™¨ä¹Ÿåº”åŒ…å«å¯¹APIè¯·æ±‚è¶…æ—¶æ—¶é—´çš„ç®¡ç†ã€‚\\n        *   ç¤ºä¾‹ï¼š\\n            ```python\\n            # util.py\\n            from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\\n            import requests\",\n",
      "    \"**åœ¨`util.py`ä¸­å®ç°ä¸€ä¸ªé€šç”¨çš„ç»“æœç¼“å­˜æœºåˆ¶:**\\n    *   **ç›®çš„**: é¿å…é‡å¤æ‰§è¡Œç›¸åŒçš„LLM APIè°ƒç”¨ã€‚å¯¹äºå·²æˆåŠŸç”Ÿæˆè¿‡ç»“æœçš„promptï¼Œç›´æ¥ä»ç¼“å­˜ä¸­è¯»å–ï¼Œå¤§å¤§å‡å°‘å¯¹LLMæœåŠ¡çš„ä¾èµ–å’Œå¼±ç½‘ç¯å¢ƒä¸‹çš„é‡è¯•è´Ÿæ‹…ã€‚\\n    *   **å…·ä½“å®ç°**:\\n        *   åˆ›å»ºä¸€ä¸ªå‡½æ•°æˆ–è£…é¥°å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥ï¼ˆprompt, LLMæ¨¡å‹å‚æ•°ç­‰ï¼‰ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„å“ˆå¸Œé”®ã€‚\\n        *   ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿï¼ˆä¾‹å¦‚JSONæ–‡ä»¶ï¼‰æˆ–è½»é‡çº§æ•°æ®åº“ï¼ˆå¦‚SQLiteï¼‰ä½œä¸ºç¼“å­˜åç«¯ï¼Œå°†LLMçš„è¾“å…¥å’Œè¾“å‡ºå­˜å‚¨èµ·æ¥ã€‚\\n        *   æä¾›è¯»å–å’Œå†™å…¥ç¼“å­˜çš„æ¥å£ã€‚\\n        *   å¯ä»¥è€ƒè™‘ç¼“å­˜çš„è¿‡æœŸç­–ç•¥ï¼ˆè™½ç„¶å¯¹äºåŸºå‡†æµ‹è¯•é€šå¸¸ä¸éœ€è¦ï¼Œé™¤éLLMæ¨¡å‹æœ¬èº«æ›´æ–°ï¼‰ã€‚\\n        *   ç¤ºä¾‹ï¼š\\n            ```python\\n            # util.py\\n            import json\\n            import hashlib\\n            import os\",\n",
      "    \"**åœ¨`llm_om_zero_shot.py`å’Œ`llm_matching.py`ä¸­åº”ç”¨é‡è¯•å’Œç¼“å­˜é€»è¾‘:**\\n    *   **ç›®çš„**: å°†å¼±ç½‘æ”¯æŒèƒ½åŠ›æ³¨å…¥åˆ°å®é™…æ‰§è¡ŒLLMè°ƒç”¨çš„æ¨¡å—ä¸­ã€‚\\n    *   **å…·ä½“å®ç°**:\\n        *   æ‰¾åˆ°è¿™ä¸¤ä¸ªæ¨¡å—ä¸­ä¸LLMæœåŠ¡è¿›è¡Œäº¤äº’çš„æ ¸å¿ƒå‡½æ•°ï¼ˆä¾‹å¦‚ `generate_response`, `query_llm` ç­‰ï¼‰ã€‚\\n        *   å°†æ­¥éª¤1ä¸­å®ç°çš„ `@retry_llm_call` è£…é¥°å™¨åº”ç”¨åˆ°è¿™äº›å‡½æ•°ä¸Šã€‚\\n        *   åœ¨è¿™äº›å‡½æ•°å†…éƒ¨ï¼Œæˆ–åœ¨å…¶å¤–éƒ¨åŒ…è£…ä¸€å±‚ï¼Œå…ˆå°è¯•ä»ç¼“å­˜åŠ è½½ç»“æœï¼Œå¦‚æœæœªå‘½ä¸­ï¼Œåˆ™æ‰§è¡ŒLLMè°ƒç”¨ï¼Œå¹¶å°†æˆåŠŸçš„ç»“æœä¿å­˜åˆ°ç¼“å­˜ä¸­ã€‚\\n        *   ç¤ºä¾‹ï¼ˆä»¥`llm_om_zero_shot.py`ä¸ºä¾‹ï¼‰ï¼š\\n            ```python\\n            # llm_om_zero_shot.py (å‡è®¾æœ‰å‡½æ•°åä¸º generate_llm_response)\\n            from .util import retry_llm_call, load_from_cache, save_to_cache, get_cache_key\\n            # ... å…¶ä»–å¯¼å…¥\",\n",
      "    \"7),\\n                    timeout=model_config.get('timeout', 180) # ç¡®ä¿è¶…æ—¶å‚æ•°ä¼ é€’\\n                )\\n                return response.choices[0].message.content\",\n",
      "    \"**ä¿®æ”¹`generate_conference_benchmark.py`ä»¥æ”¯æŒè¿›åº¦ä¿å­˜ä¸æ¢å¤:**\\n    *   **ç›®çš„**: å½“ç”Ÿæˆå¤§é‡åŸºå‡†æµ‹è¯•æ•°æ®æ—¶ï¼Œå¼±ç½‘å¯èƒ½å¯¼è‡´ä»»åŠ¡é•¿æ—¶é—´è¿è¡Œæˆ–ä¸­æ–­ã€‚æ­¤åŠŸèƒ½å…è®¸ç”¨æˆ·ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­ã€‚\\n    *   **å…·ä½“å®ç°**:\\n        *   åœ¨ `generate_conference_benchmark` çš„ä¸»å¾ªç¯ä¸­ï¼Œå®šæœŸä¿å­˜å·²å¤„ç†å®Œæˆçš„benchmarké¡¹çš„çŠ¶æ€æˆ–ç»“æœã€‚å¯ä»¥å°†å…¶ä¿å­˜ä¸ºä¸€ä¸ªJSONæ–‡ä»¶æˆ–SQLiteæ•°æ®åº“ã€‚\\n        *   åœ¨ç¨‹åºå¯åŠ¨æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸Šæ¬¡æœªå®Œæˆçš„è¿›åº¦æ–‡ä»¶ã€‚å¦‚æœå­˜åœ¨ï¼Œåˆ™åŠ è½½è¿™äº›è¿›åº¦ï¼Œå¹¶è·³è¿‡å·²ç»å¤„ç†è¿‡çš„éƒ¨åˆ†ï¼Œä»ä¸Šæ¬¡ä¸­æ–­çš„ä½ç½®ç»§ç»­ã€‚\\n        *   æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼ˆä¾‹å¦‚ `--resume`, `--cache-dir`ï¼‰æ¥æ§åˆ¶è¿™äº›æ–°åŠŸèƒ½ã€‚\\n        *   ç¤ºä¾‹ï¼š\\n            ```python\\n            # generate_conference_benchmark.py\\n            import argparse\\n            import os\\n            import json\\n            # ... å¯¼å…¥llm_om_zero_shot, llm_matching, utilç­‰æ¨¡å—\"\n",
      "  ],\n",
      "  \"files_to_modify\": [\n",
      "    {\n",
      "      \"file\": \"> str\",\n",
      "      \"reason\": \"# ç»“åˆpromptå’Œæ¨¡å‹é…ç½®ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„å“ˆå¸Œé”®\\n                key_data = json.dumps({\\\"prompt\\\": prompt, \\\"model_config\\\": model_config}, sort_keys=True)\\n                return hashlib.md5(key_data.encode('utf-8')).hexdigest()\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"> dict | None\",\n",
      "      \"reason\": \"os.makedirs(CACHE_DIR, exist_ok=True)\\n                cache_path = os.path.join(CACHE_DIR, f\\\"{key}.json\\\")\\n                if os.path.exists(cache_path):\\n                    with open(cache_path, 'r', encoding='utf-8') as f:\\n                        return json.load(f)\\n                return None\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"8') as f\",\n",
      "      \"reason\": \"json.dump(data, f, ensure_ascii=False, indent=2)\\n            ```\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"> str\",\n",
      "      \"reason\": \"# è¿™é‡Œçš„ä»£ç æ˜¯å®é™…è°ƒç”¨OpenAI, HuggingFaceç­‰LLM APIçš„é€»è¾‘\\n                # ...\\n                response = some_llm_client.chat.completions.create(\\n                    model=model_config['model'],\\n                    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": prompt}],\\n                    temperature=model_config.get('temperature', 0.7),\\n                    timeout=model_config.get('timeout', 180) # ç¡®ä¿è¶…æ—¶å‚æ•°ä¼ é€’\\n                )\\n                return response.choices[0].message.content\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"> str\",\n",
      "      \"reason\": \"cache_key = get_cache_key(prompt, model_config)\\n                cached_result = load_from_cache(cache_key)\\n                if cached_result:\\n                    print(f\\\"Cache hit for key: {cache_key}\\\")\\n                    return cached_result['response']\\n                \\n                print(f\\\"Cache miss, calling LLM for key: {cache_key}\\\")\\n                response_content = _call_llm_api(prompt, model_config)\\n                save_to_cache(cache_key, {\\\"prompt\\\": prompt, \\\"model_config\\\": model_config, \\\"response\\\": response_content})\\n                return response_content\\n            ```\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"-resume`, `--cache-dir`ï¼‰æ¥æ§åˆ¶è¿™äº›æ–°åŠŸèƒ½ã€‚\\n        *   ç¤ºä¾‹ï¼š\\n            ```python\\n            # generate_conference_benchmark.py\\n            import argparse\\n            import os\\n            import json\\n            # ... å¯¼å…¥llm_om_zero_shot, llm_matching, utilç­‰æ¨¡å—\\n\\n            # å®šä¹‰è¿›åº¦æ–‡ä»¶è·¯å¾„\\n            PROGRESS_FILE = \\\"benchmark_progress.json\\\"\\n\\n            def generate_conference_benchmark(config_path\",\n",
      "      \"reason\": \"str, output_dir: str, resume: bool = False, cache_dir: str = None):\\n                if cache_dir:\\n                    util.CACHE_DIR = cache_dir # æ›´æ–°utilæ¨¡å—çš„ç¼“å­˜ç›®å½•\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"8') as f\",\n",
      "      \"reason\": \"generated_items = json.load(f)\\n                    print(f\\\"Resuming from {len(generated_items)} completed items.\\\")\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"8') as f\",\n",
      "      \"reason\": \"json.dump(generated_items, f, ensure_ascii=False, indent=2)\\n                            print(f\\\"Progress saved. {len(generated_items)}/{total_items} items processed.\\\")\\n                        \\n                    except Exception as e:\\n                        print(f\\\"Error processing item {item_id}: {e}\\\")\\n                        # å¯ä»¥åœ¨è¿™é‡Œé€‰æ‹©æ˜¯å¦ç»§ç»­ï¼Œæˆ–è®°å½•é”™è¯¯å¹¶è·³è¿‡\\n                        # ä¸ºäº†å¼±ç½‘æ¢å¤ï¼Œé€šå¸¸ä¼šè®°å½•é”™è¯¯å¹¶ç»§ç»­ï¼Œç¡®ä¿ä¸å½±å“å·²å®Œæˆçš„éƒ¨åˆ†\\n                        continue\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"8') as f\",\n",
      "      \"reason\": \"json.dump(generated_items, f, ensure_ascii=False, indent=2)\\n                # æ¸…ç†è¿›åº¦æ–‡ä»¶\\n                if os.path.exists(PROGRESS_FILE):\\n                    os.remove(PROGRESS_FILE)\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"-config\\\", required=True, help=\\\"Path to configuration file\\\")\\n                parser.add_argument(\\\"--output_dir\\\", required=True, help=\\\"Directory to save benchmark results\\\")\\n                parser.add_argument(\\\"--resume\\\", action=\\\"store_true\\\", help=\\\"Resume from last saved progress\\\")\\n                parser.add_argument(\\\"--cache_dir\\\", default=\\\"llm_cache\\\", help=\\\"Directory for LLM response cache\\\")\\n                args = parser.parse_args()\\n\\n                generate_conference_benchmark(args.config, args.output_dir, args.resume, args.cache_dir)\\n            ```\\n\\nFiles to Modify\",\n",
      "      \"reason\": \"-   `util.py`:\\n    *   **ä¿®æ”¹åŸå› **: å®ç°é€šç”¨çš„LLM APIé‡è¯•ä¸è¶…æ—¶æœºåˆ¶ï¼ˆ`retry_llm_call`è£…é¥°å™¨ï¼‰å’Œç»“æœç¼“å­˜æœºåˆ¶ï¼ˆ`load_from_cache`, `save_to_cache`, `get_cache_key`ï¼‰ã€‚è¿™äº›æ˜¯ç‹¬ç«‹äºå…·ä½“ä¸šåŠ¡é€»è¾‘çš„é€šç”¨å·¥å…·å‡½æ•°ã€‚\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"`llm_om_zero_shot.py`\",\n",
      "      \"reason\": \"*   **ä¿®æ”¹åŸå› **: å°†`util.py`ä¸­å®ç°çš„é‡è¯•å’Œç¼“å­˜é€»è¾‘åº”ç”¨åˆ°è¯¥æ¨¡å—ä¸­å®é™…æ‰§è¡ŒLLM APIè°ƒç”¨çš„æ ¸å¿ƒå‡½æ•°ä¸Šï¼Œæé«˜å…¶åœ¨å¼±ç½‘ç¯å¢ƒä¸‹çš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"`llm_matching.py`\",\n",
      "      \"reason\": \"*   **ä¿®æ”¹åŸå› **: åŒ`llm_om_zero_shot.py`ï¼Œå°†é‡è¯•å’Œç¼“å­˜é€»è¾‘åº”ç”¨åˆ°å…¶LLM APIè°ƒç”¨å‡½æ•°ä¸Šã€‚\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"`generate_conference_benchmark.py`\",\n",
      "      \"reason\": \"*   **ä¿®æ”¹åŸå› **:\\n        *   é›†æˆ`util.py`ä¸­çš„ç¼“å­˜ç›®å½•é…ç½®ã€‚\\n        *   å®ç°æ•´ä¸ªåŸºå‡†æµ‹è¯•ç”Ÿæˆè¿‡ç¨‹çš„è¿›åº¦ä¿å­˜ä¸æ¢å¤é€»è¾‘ã€‚\\n        *   æ·»åŠ å‘½ä»¤è¡Œå‚æ•°æ¥æ§åˆ¶æ¢å¤å’Œç¼“å­˜ç›®å½•ï¼Œä½œä¸ºå¼±ç½‘æ”¯æŒçš„ç”¨æˆ·æ¥å£ã€‚\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"**ç¼“å­˜ä¸€è‡´æ€§/å¤±æ•ˆ**\",\n",
      "      \"reason\": \"è™½ç„¶å¯¹äºåŸºå‡†æµ‹è¯•ï¼ŒLLMè¾“å…¥é€šå¸¸æ˜¯å›ºå®šçš„ï¼Œä½†å¦‚æœLLMæ¨¡å‹æˆ–å…¶é…ç½®ï¼ˆå¦‚æ¸©åº¦ã€top_pï¼‰å‘ç”Ÿå˜åŒ–ï¼Œæ—§çš„ç¼“å­˜ç»“æœå¯èƒ½ä¸å†é€‚ç”¨ã€‚éœ€è¦ç¡®ä¿ç¼“å­˜é”®èƒ½å¤Ÿå……åˆ†åæ˜ æ‰€æœ‰å½±å“LLMè¾“å‡ºçš„å‚æ•°ã€‚\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"**é”™è¯¯å¤„ç†çš„ç²’åº¦**\",\n",
      "      \"reason\": \"`tenacity`å¯ä»¥å¤„ç†å¤šç§å¼‚å¸¸ï¼Œä½†åŒºåˆ†ç¬æ—¶é”™è¯¯ï¼ˆå¯é‡è¯•ï¼‰å’ŒæŒä¹…é”™è¯¯ï¼ˆå¦‚APIå¯†é’¥æ— æ•ˆã€promptæ ¼å¼é”™è¯¯ï¼Œä¸åº”é‡è¯•ï¼‰çš„ç²’åº¦éœ€è¦ä»”ç»†è®¾è®¡ã€‚è¿‡åº¦é‡è¯•æŒä¹…é”™è¯¯ä¼šæµªè´¹èµ„æºã€‚\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"**å†…å­˜ä¸ç£ç›˜å ç”¨**\",\n",
      "      \"reason\": \"å¤§é‡çš„åŸºå‡†æµ‹è¯•æ•°æ®æ„å‘³ç€ç¼“å­˜æ–‡ä»¶å¯èƒ½ä¼šéå¸¸å¤§ã€‚éœ€è¦è€ƒè™‘ç¼“å­˜æ¸…ç†ç­–ç•¥æˆ–é™åˆ¶ç¼“å­˜å¤§å°ã€‚è¿›åº¦æ–‡ä»¶ä¹Ÿå¯èƒ½å˜å¤§ã€‚\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"**å¹¶å‘æ€§é—®é¢˜**\",\n",
      "      \"reason\": \"å¦‚æœ`generate_conference_benchmark`æœªæ¥æ”¯æŒå¤šçº¿ç¨‹æˆ–å¤šè¿›ç¨‹å¹¶å‘ç”Ÿæˆï¼Œéœ€è¦ç¡®ä¿ç¼“å­˜å’Œè¿›åº¦æ–‡ä»¶çš„è¯»å†™æ˜¯çº¿ç¨‹å®‰å…¨çš„ã€‚ç›®å‰å•çº¿ç¨‹å¯èƒ½ä¸æ˜¯ä¸»è¦é—®é¢˜ï¼Œä½†éœ€é¢„ç•™æ‰©å±•æ€§ã€‚\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"**ç”¨æˆ·ä½“éªŒ**\",\n",
      "      \"reason\": \"æä¾›æ¸…æ™°çš„æ—¥å¿—è¾“å‡ºï¼Œå‘ŠçŸ¥ç”¨æˆ·å½“å‰æ­£åœ¨é‡è¯•ã€ä»ç¼“å­˜åŠ è½½æˆ–å·²ä¿å­˜è¿›åº¦ï¼Œä»¥å¢å¼ºç”¨æˆ·å¯¹å¼±ç½‘åœºæ™¯ä¸‹ç¨‹åºè¡Œä¸ºçš„ç†è§£ã€‚\"\n",
      "    },\n",
      "    {\n",
      "      \"file\": \"**LLM APIé€Ÿç‡é™åˆ¶**\",\n",
      "      \"reason\": \"å³ä½¿æœ‰æŒ‡æ•°é€€é¿é‡è¯•ï¼Œå¦‚æœLLMæœåŠ¡æœ‰éå¸¸ä¸¥æ ¼çš„é€Ÿç‡é™åˆ¶ï¼Œé•¿æ—¶é—´çš„è¯·æ±‚ä»ç„¶å¯èƒ½å¯¼è‡´è¢«é™æµã€‚å¯èƒ½éœ€è¦æ›´é«˜çº§çš„ä»¤ç‰Œæ¡¶ç®—æ³•æˆ–é’ˆå¯¹ç‰¹å®šLLMæœåŠ¡çš„é™æµå¤„ç†ã€‚\"\n",
      "    }\n",
      "  ],\n",
      "  \"challenges\": [\n",
      "    \"**ç¼“å­˜ä¸€è‡´æ€§/å¤±æ•ˆ**: è™½ç„¶å¯¹äºåŸºå‡†æµ‹è¯•ï¼ŒLLMè¾“å…¥é€šå¸¸æ˜¯å›ºå®šçš„ï¼Œä½†å¦‚æœLLMæ¨¡å‹æˆ–å…¶é…ç½®ï¼ˆå¦‚æ¸©åº¦ã€top_pï¼‰å‘ç”Ÿå˜åŒ–ï¼Œæ—§çš„ç¼“å­˜ç»“æœå¯èƒ½ä¸å†é€‚ç”¨ã€‚éœ€è¦ç¡®ä¿ç¼“å­˜é”®èƒ½å¤Ÿå……åˆ†åæ˜ æ‰€æœ‰å½±å“LLMè¾“å‡ºçš„å‚æ•°ã€‚\",\n",
      "    \"**é”™è¯¯å¤„ç†çš„ç²’åº¦**: `tenacity`å¯ä»¥å¤„ç†å¤šç§å¼‚å¸¸ï¼Œä½†åŒºåˆ†ç¬æ—¶é”™è¯¯ï¼ˆå¯é‡è¯•ï¼‰å’ŒæŒä¹…é”™è¯¯ï¼ˆå¦‚APIå¯†é’¥æ— æ•ˆã€promptæ ¼å¼é”™è¯¯ï¼Œä¸åº”é‡è¯•ï¼‰çš„ç²’åº¦éœ€è¦ä»”ç»†è®¾è®¡ã€‚è¿‡åº¦é‡è¯•æŒä¹…é”™è¯¯ä¼šæµªè´¹èµ„æºã€‚\",\n",
      "    \"**å†…å­˜ä¸ç£ç›˜å ç”¨**: å¤§é‡çš„åŸºå‡†æµ‹è¯•æ•°æ®æ„å‘³ç€ç¼“å­˜æ–‡ä»¶å¯èƒ½ä¼šéå¸¸å¤§ã€‚éœ€è¦è€ƒè™‘ç¼“å­˜æ¸…ç†ç­–ç•¥æˆ–é™åˆ¶ç¼“å­˜å¤§å°ã€‚è¿›åº¦æ–‡ä»¶ä¹Ÿå¯èƒ½å˜å¤§ã€‚\",\n",
      "    \"**å¹¶å‘æ€§é—®é¢˜**: å¦‚æœ`generate_conference_benchmark`æœªæ¥æ”¯æŒå¤šçº¿ç¨‹æˆ–å¤šè¿›ç¨‹å¹¶å‘ç”Ÿæˆï¼Œéœ€è¦ç¡®ä¿ç¼“å­˜å’Œè¿›åº¦æ–‡ä»¶çš„è¯»å†™æ˜¯çº¿ç¨‹å®‰å…¨çš„ã€‚ç›®å‰å•çº¿ç¨‹å¯èƒ½ä¸æ˜¯ä¸»è¦é—®é¢˜ï¼Œä½†éœ€é¢„ç•™æ‰©å±•æ€§ã€‚\",\n",
      "    \"**ç”¨æˆ·ä½“éªŒ**: æä¾›æ¸…æ™°çš„æ—¥å¿—è¾“å‡ºï¼Œå‘ŠçŸ¥ç”¨æˆ·å½“å‰æ­£åœ¨é‡è¯•ã€ä»ç¼“å­˜åŠ è½½æˆ–å·²ä¿å­˜è¿›åº¦ï¼Œä»¥å¢å¼ºç”¨æˆ·å¯¹å¼±ç½‘åœºæ™¯ä¸‹ç¨‹åºè¡Œä¸ºçš„ç†è§£ã€‚\",\n",
      "    \"**LLM APIé€Ÿç‡é™åˆ¶**: å³ä½¿æœ‰æŒ‡æ•°é€€é¿é‡è¯•ï¼Œå¦‚æœLLMæœåŠ¡æœ‰éå¸¸ä¸¥æ ¼çš„é€Ÿç‡é™åˆ¶ï¼Œé•¿æ—¶é—´çš„è¯·æ±‚ä»ç„¶å¯èƒ½å¯¼è‡´è¢«é™æµã€‚å¯èƒ½éœ€è¦æ›´é«˜çº§çš„ä»¤ç‰Œæ¡¶ç®—æ³•æˆ–é’ˆå¯¹ç‰¹å®šLLMæœåŠ¡çš„é™æµå¤„ç†ã€‚\"\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gemini-2.5-flash\",\n",
      "    \"temperature\": 0.3,\n",
      "    \"timestamp\": \"2025-12-19T13:14:45.377285\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ç¾åŒ–è¾“å‡ºç¬¬ä¸€ä¸ªé—®ç­”å¯¹\n",
    "if dataset['qa_pairs']:\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ“ å®Œæ•´é—®ç­”å¯¹ç¤ºä¾‹\")\n",
    "    print(\"=\"*70)\n",
    "    sample_qa = dataset['qa_pairs'][0]\n",
    "    print(json.dumps(sample_qa, ensure_ascii=False, indent=2))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# ç¾åŒ–è¾“å‡ºç¬¬ä¸€ä¸ªè®¾è®¡æ–¹æ¡ˆ\n",
    "if dataset['design_solutions']:\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ—ï¸  å®Œæ•´è®¾è®¡æ–¹æ¡ˆç¤ºä¾‹\")\n",
    "    print(\"=\"*70)\n",
    "    sample_design = dataset['design_solutions'][0]\n",
    "    print(json.dumps(sample_design, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16f3c4",
   "metadata": {},
   "source": [
    "## ğŸš€ å¿«æ·æ–¹å¼: ä¸€é”®ç”Ÿæˆ\n",
    "\n",
    "å¦‚æœä¸éœ€è¦è¯¦ç»†æ­¥éª¤ï¼Œå¯ä»¥ä½¿ç”¨å¿«æ·å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd2891d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸš€ å¼€å§‹ç”Ÿæˆè®­ç»ƒæ•°æ®\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ å‘ç°Pythonæ–‡ä»¶...\n",
      "   æ‰¾åˆ° 13 ä¸ªæ–‡ä»¶\n",
      "\n",
      "ğŸ“ ç”Ÿæˆ 5 ä¸ªé—®ç­”å¯¹...\n",
      "   [1/5] å¤„ç†æ–‡ä»¶: llm_om_few_shot.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [2/5] å¤„ç†æ–‡ä»¶: om_csv_to_database.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [3/5] å¤„ç†æ–‡ä»¶: om_ontology_to_csv.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [4/5] å¤„ç†æ–‡ä»¶: fix_multifarm_reference.py\n",
      "       âœ… æˆåŠŸ\n",
      "   [5/5] å¤„ç†æ–‡ä»¶: generate_anatomy_mse_benchmark.py\n",
      "       âœ… æˆåŠŸ\n",
      "\n",
      "ğŸ—ï¸  ç”Ÿæˆ 3 ä¸ªè®¾è®¡æ–¹æ¡ˆ...\n",
      "   [1/3] éœ€æ±‚: æ‰©å±•om_csv_to_databaseä»¥æ”¯æŒå¾®æœåŠ¡åœºæ™¯...\n",
      "       âœ… æˆåŠŸ\n",
      "   [2/3] éœ€æ±‚: æ‰©å±•om_ontology_to_csvä»¥æ”¯æŒè¾¹ç¼˜è®¡ç®—åœºæ™¯...\n",
      "       âœ… æˆåŠŸ\n",
      "   [3/3] éœ€æ±‚: ä¸ºgenerate_conference_benchmarkæ·»åŠ æ‰¹é‡å¤„ç†åŠŸèƒ½...\n",
      "       âœ… æˆåŠŸ\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ç”Ÿæˆå®Œæˆ\n",
      "======================================================================\n",
      "   é—®ç­”å¯¹: 5/5\n",
      "   è®¾è®¡æ–¹æ¡ˆ: 3/3\n",
      "   æˆåŠŸç‡: 100.0%\n",
      "\n",
      "ğŸ’¾ æ•°æ®å·²ä¿å­˜: outputs/ontology-llm/quick_output.json\n",
      "   æ–‡ä»¶å¤§å°: 96.8 KB\n",
      "\n",
      "ğŸ‰ å¿«æ·ç”Ÿæˆå®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "from src.simple_generator import quick_generate\n",
    "\n",
    "# ä¸€é”®ç”Ÿæˆï¼ˆæ‰€æœ‰æ­¥éª¤è‡ªåŠ¨å®Œæˆï¼‰\n",
    "dataset_quick = quick_generate(\n",
    "    project_path=str(project_path),\n",
    "    num_qa=5,\n",
    "    num_design=3,\n",
    "    output_path=f\"outputs/{project_name}/quick_output.json\",\n",
    "    use_context=True\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ‰ å¿«æ·ç”Ÿæˆå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e68a8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š é™„å½•: è¿›é˜¶é…ç½®\n",
    "\n",
    "\n",
    "### å¸¸è§é—®é¢˜\n",
    "\n",
    "**Q: APIè°ƒç”¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ**\n",
    "- æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®\n",
    "- éªŒè¯ç½‘ç»œè¿æ¥\n",
    "- æŸ¥çœ‹APIé…é¢é™åˆ¶\n",
    "\n",
    "**Q: å¦‚ä½•æé«˜ç”Ÿæˆè´¨é‡ï¼Ÿ**\n",
    "- å¯ç”¨ä¸Šä¸‹æ–‡å¢å¼º (`use_context=True`)\n",
    "- é€‰æ‹©åˆé€‚çš„ä¸Šä¸‹æ–‡çº§åˆ«\n",
    "- è°ƒæ•´temperatureå‚æ•°ï¼ˆ0.2-0.4æœ€ä½³ï¼‰\n",
    "\n",
    "**Q: å¦‚ä½•åŠ é€Ÿç”Ÿæˆï¼Ÿ**\n",
    "- ä½¿ç”¨`minimal`ä¸Šä¸‹æ–‡çº§åˆ«\n",
    "- å‡å°‘å•æ¬¡ç”Ÿæˆæ•°é‡\n",
    "- ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
